{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT5hODrZlTUz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# Constrained Portfolio Optimization via Quantum Approximate Optimization Algorithm (QAOA) with XY-Mixers and Trotterized Initialization\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2602.14827-b31b1b.svg)](https://arxiv.org/abs/2602.14827)\n",
        "[![Journal](https://img.shields.io/badge/Journal-ArXiv%20Preprint-003366)](https://arxiv.org/abs/2602.14827)\n",
        "[![Year](https://img.shields.io/badge/Year-2026-purple)](https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Financial%20Engineering%20%7C%20Quantum%20Computing-00529B)](https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-Yahoo%20Finance%20%7C%20Bloomberg-lightgrey)](https://finance.yahoo.com/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-QAOA%20%7C%20Combinatorial%20Optimization-orange)](https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Walk--Forward%20Backtesting-red)](https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm)\n",
        "[![Validation](https://img.shields.io/badge/Validation-Out--of--Sample%20Performance-green)](https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Trotterized%20Initialization-yellow)](https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![PennyLane](https://img.shields.io/badge/PennyLane-%23000000.svg?style=flat&logo=Xanadu&logoColor=white)](https://pennylane.ai/)\n",
        "[![D-Wave](https://img.shields.io/badge/D--Wave%20Ocean-%2300AEEF.svg?style=flat&logo=D-Wave&logoColor=white)](https://docs.ocean.dwavesys.com/)\n",
        "[![YAML](https://img.shields.io/badge/YAML-%23CB171E.svg?style=flat&logo=yaml&logoColor=white)](https://yaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![Open Source](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen)](https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm`\n",
        "\n",
        "**Owner:** 2026 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2026 paper entitled **\"Constrained Portfolio Optimization via Quantum Approximate Optimization Algorithm (QAOA) with XY-Mixers and Trotterized Initialization: A Hybrid Approach for Direct Indexing\"** by:\n",
        "\n",
        "*   **Javier Mancilla** (SquareOne Capital)\n",
        "*   **Theodoros D. Bouloumis** (Aristotle University of Thessaloniki)\n",
        "*   **Frederic Goguikian** (SquareOne Capital)\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire hybrid quantum-classical workflow: from the ingestion and rigorous validation of financial market data to the formulation and simulation of constraint-preserving quantum circuits, culminating in comprehensive out-of-sample evaluation against classical heuristics.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_quantum_hybrid_portfolio_optimization`](#key-callable-execute_quantum_hybrid_portfolio_optimization)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Mancilla et al. (2026). The core of this repository is the iPython Notebook `constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm_draft.ipynb`, which contains a comprehensive suite of 27+ functions to replicate the paper's findings.\n",
        "\n",
        "The pipeline addresses the critical challenge of **Cardinality Constrained Portfolio Optimization** in the context of \"Direct Indexing.\" Selecting exactly $K$ assets from a universe of $N$ transforms standard convex Markowitz optimization into an NP-hard combinatorial problem.\n",
        "\n",
        "The paper proposes a **Hard-Constraint QAOA** formulation. Unlike standard QAOA implementations that rely on soft penalty terms (which distort the energy landscape), this approach enforces constraints strictly via the quantum ansatz itself using Dicke states and XY-mixers. This codebase operationalizes the proposed solution:\n",
        "-   **Validates** data integrity using strict schema checks and temporal causality enforcement.\n",
        "-   **Engineers** the quantum state using Dicke state initialization to confine evolution to the feasible subspace.\n",
        "-   **Simulates** the quantum circuit using PennyLane, employing a Trotterized parameter initialization to mitigate Barren Plateaus.\n",
        "-   **Benchmarks** the quantum solver against Simulated Annealing (SA) and Hierarchical Risk Parity (HRP).\n",
        "-   **Evaluates** performance via rigorous out-of-sample walk-forward backtesting, computing Sharpe Ratios, Drawdowns, and Turnover net of transaction costs.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods combine techniques from Financial Econometrics, Quantum Information Science, and Convex Optimization.\n",
        "\n",
        "**1. The Combinatorial Objective:**\n",
        "The objective is to select exactly $K$ assets to minimize the risk-return trade-off:\n",
        "$$ \\min_{x \\in \\{0,1\\}^N} \\left( q x^\\top \\Sigma x - (1-q) \\mu^\\top x \\right) \\quad \\text{s.t.} \\quad \\sum_{i=1}^N x_i = K $$\n",
        "\n",
        "**2. Constraint-Preserving Quantum Ansatz:**\n",
        "*   **Dicke State Initialization:** The system begins in an equal superposition of all valid portfolios:\n",
        "    $$ |\\psi_0\\rangle = |D^K_N\\rangle = \\binom{N}{K}^{-1/2} \\sum_{|x|=K} |x\\rangle $$\n",
        "*   **XY-Mixer Hamiltonian:** The evolution operator performs partial SWAPs, commuting with the number operator to preserve the Hamming weight:\n",
        "    $$ H_{XY} = \\sum_{(i,j) \\in E} (X_i X_j + Y_i Y_j) $$\n",
        "\n",
        "**3. Trotterized Initialization:**\n",
        "To avoid vanishing gradients in deep circuits, parameters are initialized via an adiabatic linear ramp:\n",
        "$$ \\gamma_l = \\frac{l}{p}\\Delta t, \\quad \\beta_l = \\left(1 - \\frac{l}{p}\\right)\\Delta t $$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, 27-Task Architecture:** The pipeline is decomposed into highly specialized, mathematically rigorous functions.\n",
        "-   **Configuration-Driven Design:** All study parameters (risk aversion, circuit depths, transaction costs) are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks schema integrity, timezone consistency, and strict causality (zero look-ahead bias).\n",
        "-   **Advanced Quantum Simulation:** Uses `PennyLane` for statevector simulation, featuring a highly optimized single-pass `value_and_grad` Adam training loop.\n",
        "-   **Classical Baselines:** Integrates `dimod` and `neal` for Simulated Annealing via QUBO, and `PyPortfolioOpt` for Hierarchical Risk Parity.\n",
        "-   **Institutional Fidelity Assertions:** The pipeline concludes with mathematical proofs asserting that all generated portfolios strictly obeyed the $K$-hot and sum-to-one constraints.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Engineering (Tasks 1-4):** Validates the configuration, cleanses the raw price matrix (ffill/dropna), and freezes a deterministic, causal rebalance calendar.\n",
        "2.  **Temporal & Return Infrastructure (Tasks 5-6):** Extracts strict $[t-L, t)$ causal windows and computes daily logarithmic returns.\n",
        "3.  **Econometric Estimation (Tasks 7-9):** Computes annualized expected returns ($\\mu$) and Ledoit-Wolf shrinkage covariance matrices ($\\Sigma$), and manages the temporal continuity state.\n",
        "4.  **Simulated Annealing Baseline (Tasks 10-12):** Constructs the penalized QUBO matrix, executes the `neal` sampler, and filters for feasible candidates.\n",
        "5.  **QAOA Ansatz Construction (Tasks 13-15):** Prepares the Dicke statevector, builds the complete-graph XY-mixer, and maps the financial moments to the Ising Cost Hamiltonian.\n",
        "6.  **QAOA Training & Selection (Tasks 16-18):** Executes the Trotterized Adam optimization loop across depths $p \\in \\{1 \\dots 6\\}$, extracts exact statevector probabilities, filters via bitwise operations, and selects the global optimum.\n",
        "7.  **Continuous Allocation (Tasks 19-21):** Solves the Sharpe-max problem via SLSQP on the selected subsets, with a robust fallback to HRP.\n",
        "8.  **Performance Accounting (Tasks 22-24):** Computes holding-period returns, $L_1$ turnover, applies 5 bps transaction costs, and aggregates final financial metrics and depth-scaling diagnostics.\n",
        "9.  **Orchestration & Verification (Tasks 25-27):** Serializes all artifacts to disk and executes the final mathematical fidelity assertions.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The project is contained within a single, comprehensive Jupyter Notebook: `constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm_draft.ipynb`. The notebook is structured as a logical pipeline with modular orchestrator functions for each of the 27 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_quantum_hybrid_portfolio_optimization`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_quantum_hybrid_portfolio_optimization`:** This master orchestrator function runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, managing data flow between validation, econometric estimation, quantum simulation, classical allocation, and final fidelity verification.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `pennylane`, `dimod`, `dwave-neal`, `PyPortfolioOpt`, `pyyaml`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm.git\n",
        "    cd constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy pennylane dimod dwave-neal PyPortfolioOpt pyyaml\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a single primary DataFrame (`raw_price_df`):\n",
        "\n",
        "-   **Index:** `DatetimeIndex` (monotonically increasing trading days).\n",
        "-   **Columns:** Exactly 10 string identifiers matching the configured universe (e.g., `AAPL`, `MSFT`, `GOOGL`, `AMZN`, `JPM`, `V`, `TSLA`, `UNH`, `LLY`, `XOM`).\n",
        "-   **Values:** `float64` representing auto-adjusted closing prices (strictly positive).\n",
        "\n",
        "*Note: The usage example below includes a synthetic data generator using Geometric Brownian Motion for testing purposes if access to live Yahoo Finance data is unavailable.*\n",
        "\n",
        "## Usage\n",
        "\n",
        "The following snippet demonstrates how to generate synthetic data, load the configuration, and execute the top-level orchestrator.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "# Assuming all pipeline callables are loaded in the current namespace\n",
        "\n",
        "# 1. Generate Synthetic Price Data (Geometric Brownian Motion)\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range(start=\"2024-01-01\", end=\"2025-12-31\", freq='B')\n",
        "universe = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"JPM\", \"V\", \"TSLA\", \"UNH\", \"LLY\", \"XOM\"]\n",
        "prices = np.zeros((len(dates), len(universe)))\n",
        "prices[0] = np.random.uniform(100, 200, size=len(universe))\n",
        "dt = 1.0 / 252.0\n",
        "for t in range(1, len(dates)):\n",
        "    Z = np.random.standard_normal(len(universe))\n",
        "    prices[t] = prices[t-1] * np.exp((0.10 - 0.5 * 0.20**2) * dt + 0.20 * np.sqrt(dt) * Z)\n",
        "\n",
        "raw_price_df = pd.DataFrame(prices, index=dates, columns=universe).astype(np.float64)\n",
        "\n",
        "# 2. Load Configuration\n",
        "with open(\"config.yaml\", \"r\") as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "# 3. Execute the Pipeline\n",
        "output_directory = \"./quantum_portfolio_artifacts\"\n",
        "study_artifacts = execute_quantum_hybrid_portfolio_optimization(\n",
        "    raw_price_df=raw_price_df,\n",
        "    raw_config=config,\n",
        "    output_dir=output_directory\n",
        ")\n",
        "\n",
        "# 4. Inspect Results\n",
        "print(\"\\n[Fidelity Audit]\")\n",
        "print(f\"Mathematical Constraints Verified: {study_artifacts['fidelity_verified']}\")\n",
        "\n",
        "print(\"\\n[Financial Performance Metrics (2025)]\")\n",
        "metrics_df = pd.DataFrame.from_dict(study_artifacts[\"results_bundle\"][\"financial_metrics\"], orient=\"index\")\n",
        "print(metrics_df)\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a comprehensive dictionary containing:\n",
        "-   **`validated_config`**: The strictly typed configuration object.\n",
        "-   **`data_validation_report`**: Telemetry from the data cleansing phase.\n",
        "-   **`cleaned_price_df`**: The canonical price matrix used for the study.\n",
        "-   **`rebalance_dates`**: The frozen temporal schedule.\n",
        "-   **`results_bundle`**: A nested dictionary containing:\n",
        "    -   `financial_metrics`: Total Return, Volatility, Sharpe, MDD, Turnover for QAOA, SA, and HRP.\n",
        "    -   `depth_scaling_table`: Aggregated quantum telemetry (Cost, Iterations, Gradient Norms) across depths $p=1 \\dots 6$.\n",
        "    -   `persisted_files`: A list of all artifacts serialized to disk (Parquet, NPY, JSON).\n",
        "-   **`fidelity_verified`**: A boolean confirming all mathematical constraints were upheld.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm/\n",
        "│\n",
        "├── constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                                                                     # Master configuration file\n",
        "├── requirements.txt                                                                                # Python package dependencies\n",
        "│\n",
        "├── LICENSE                                                                                         # MIT Project License File\n",
        "└── README.md                                                                                       # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as:\n",
        "-   **Financial Setup:** Asset universe, lookback window ($L$), and risk aversion ($q$).\n",
        "-   **Quantum Architecture:** Circuit depths ($p$), Trotterization bounds, and Adam optimizer step sizes.\n",
        "-   **Classical Solvers:** SA reads/sweeps and SLSQP allocation bounds.\n",
        "-   **Friction Models:** Transaction cost basis points ($\\tau$) and continuity bonus ($\\kappa$).\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, strict type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Hardware Execution:** Migrating the `default.qubit` statevector simulator to noisy quantum hardware (e.g., IBM, IonQ) via Amazon Braket or Azure Quantum.\n",
        "-   **Multi-Period Regularization:** Incorporating explicit turnover penalties directly into the Ising Hamiltonian to optimize the net-of-fee objective natively on the QPU.\n",
        "-   **Alternative Mixers:** Exploring ring-graph XY-mixers to reduce circuit depth and CNOT gate counts for near-term hardware compatibility.\n",
        "-   **Expanded Universes:** Scaling the simulation beyond $N=10$ using tensor network simulators (e.g., `default.tensor`).\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{mancilla2026constrained,\n",
        "  title={Constrained Portfolio Optimization via Quantum Approximate Optimization Algorithm (QAOA) with XY-Mixers and Trotterized Initialization: A Hybrid Approach for Direct Indexing},\n",
        "  author={Mancilla, Javier and Bouloumis, Theodoros D. and Goguikian, Frederic},\n",
        "  journal={arXiv preprint arXiv:2602.14827},\n",
        "  year={2026}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2026). Constrained Portfolio Optimization via QAOA with XY-Mixers: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Javier Mancilla, Theodoros D. Bouloumis, and Frederic Goguikian** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source quantum and scientific Python communities. Sincere thanks to the developers of **PennyLane, D-Wave Ocean, PyPortfolioOpt, Pandas, NumPy, and SciPy**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `constrained_portfolio_optimization_via_quantum_approximate_optimization_algorithm_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "htQXka6WmcyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Constrained Portfolio Optimization via Quantum Approximate Optimization Algorithm (QAOA) with XY-Mixers and Trotterized Initialization: A Hybrid Approach for Direct Indexing*\"\n",
        "\n",
        "Authors: Javier Mancilla, Theodoros D. Bouloumis, Frederic Goguikian\n",
        "\n",
        "E-Journal Submission Date: 16 February 2026\n",
        "\n",
        "Link: https://arxiv.org/abs/2602.14827\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Portfolio optimization under strict cardinality constraints is a combinatorial challenge that defies classical convex optimization techniques, particularly in the context of \"Direct Indexing\" and ESG-constrained mandates. In the Noisy Intermediate-Scale Quantum (NISQ) era, the Quantum Approximate Optimization Algorithm (QAOA) offers a promising hybrid approach. However, standard QAOA implementations utilizing transverse field mixers often fail to strictly enforce hard constraints, necessitating soft penalties that distort the energy landscape. This paper presents a comprehensive analysis of a constraint-preserving QAOA formulation against Simulated Annealing (SA) and Hierarchical Risk Parity (HRP). We implement a specific QAOA ansatz utilizing a Dicke state initialization and an XY-mixer Hamiltonian that strictly preserves the Hamming weight of the solution, ensuring only valid portfolios of size K are explored. Furthermore, we introduce a Trotterized parameter initialization schedule inspired by adiabatic quantum computing to mitigate the \"Barren Plateau\" problem. Backtesting on a basket of 10 US equities over 2025 reveals that our QAOA approach achieves a Sharpe Ratio of 1.81, significantly outperforming Simulated Annealing (1.31) and HRP (0.98). We further analyze the operational implications of the algorithm's high turnover (76.8%), discussing the trade-offs between theoretical optimality and implementation costs in institutional settings."
      ],
      "metadata": {
        "id": "9FMIrXrc-yZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Executive Summary**\n",
        "This paper addresses the **Cardinality Constrained Portfolio Optimization (CCPO)** problem—specifically within the context of \"Direct Indexing\"—where an investor must select exactly $K$ assets out of a universe of $N$ to maximize risk-adjusted returns. Mathematically, this transforms the convex Markowitz Mean-Variance Optimization (MVO) into an **NP-hard combinatorial problem**.\n",
        "\n",
        "The authors propose a **Hard-Constraint QAOA (Quantum Approximate Optimization Algorithm)** formulation. Unlike standard QAOA implementations that rely on soft penalty terms (which distort the energy landscape), this approach enforces constraints strictly via the quantum ansatz itself. The method utilizes **Dicke State initialization** and **XY-Mixers** to confine the quantum evolution to the feasible subspace of Hamming weight $K$.\n",
        "\n",
        "Empirically, the model is backtested on a basket of 10 US equities over a 2025 walk-forward period. The QAOA approach achieves a **Sharpe Ratio of 1.81**, significantly outperforming Simulated Annealing (1.31) and Hierarchical Risk Parity (0.98), albeit with significantly higher portfolio turnover.\n",
        "\n",
        "\n",
        "### **Mathematical Formulation & The \"Combinatorial Cliff\"**\n",
        "The authors identify the core computational bottleneck in Direct Indexing: selecting a subset of assets is discrete, while weight allocation is continuous.\n",
        "\n",
        "*   **The Objective:** Minimize the Ising Hamiltonian representing the risk-return trade-off:\n",
        "    $$ \\min_{x \\in \\{0,1\\}^N} \\left( q x^\\top \\Sigma x - (1-q) \\mu^\\top x \\right) $$\n",
        "    Subject to the hard constraint:\n",
        "    $$ \\sum_{i=1}^N x_i = K $$\n",
        "    Where $\\mu$ is the expected return vector, $\\Sigma$ is the covariance matrix, and $q$ is the risk aversion parameter (set to $0.3$).\n",
        "\n",
        "*   **The Challenge:** As $N$ grows, the search space $\\binom{N}{K}$ expands factorially. Classical heuristics (Simulated Annealing) often trap in local minima, while standard QAOA with transverse field mixers requires penalty terms ($+ P(\\sum x_i - K)^2$) that complicate convergence.\n",
        "\n",
        "### **The Quantum Methodology: Constraint-Preserving QAOA**\n",
        "The paper’s primary contribution is a specific QAOA ansatz designed to strictly preserve the cardinality constraint $K$ without penalty terms.\n",
        "\n",
        "#### **A. Dicke State Initialization**\n",
        "Instead of the standard superposition of all states $|+\\rangle^{\\otimes N}$, the system is initialized in a **Dicke State** $|D^K_N\\rangle$:\n",
        "$$ |\\psi_0\\rangle = |D^K_N\\rangle = \\binom{N}{K}^{-1/2} \\sum_{|x|=K} |x\\rangle $$\n",
        "This ensures the starting state is an equal superposition of *only* valid portfolios with Hamming weight $K$.\n",
        "\n",
        "#### **B. The XY-Mixer Hamiltonian**\n",
        "To evolve the system without leaving the feasible subspace, the authors replace the standard transverse field mixer ($H_X = \\sum X_i$) with a complete-graph **XY-Mixer**:\n",
        "$$ H_{XY} = \\sum_{(i,j) \\in E} (X_i X_j + Y_i Y_j) $$\n",
        "*   **Mechanism:** This operator performs a partial SWAP ($|01\\rangle \\leftrightarrow |10\\rangle$), exchanging excitation between qubits $i$ and $j$.\n",
        "*   **Commutation:** Crucially, $[H_{XY}, \\sum Z_i] = 0$. The mixer commutes with the number operator, guaranteeing that the unitary evolution $U(\\beta) = e^{-i\\beta H_{XY}}$ never alters the number of selected assets.\n",
        "\n",
        "#### **C. Trotterized Parameter Initialization**\n",
        "To mitigate the **Barren Plateau** problem (vanishing gradients in deep circuits), the authors employ a **Trotterized schedule** inspired by Adiabatic Quantum Computing (AQC).\n",
        "*   Instead of random initialization, parameters $(\\gamma, \\beta)$ are initialized via a linear ramp:\n",
        "    $$ \\gamma_l = \\frac{l}{p}\\Delta t, \\quad \\beta_l = \\left(1 - \\frac{l}{p}\\right)\\Delta t $$\n",
        "*   **Result:** Diagnostics confirm that gradient norms do *not* vanish even at circuit depth $p=6$, proving the trainability of the circuit.\n",
        "\n",
        "### **The Hybrid Algorithmic Pipeline**\n",
        "The authors implement a two-stage hybrid process:\n",
        "1.  **Discrete Selection (Quantum):** The QAOA-XY circuit (simulated via PennyLane) identifies the optimal bitstring $x$ (the subset of 5 assets).\n",
        "2.  **Continuous Allocation (Classical):** Once the subset $S$ is fixed, a classical solver (SLSQP) optimizes the weights $w_i$ to maximize the Sharpe Ratio, subject to box constraints ($0.05 \\le w_i \\le 0.50$).\n",
        "\n",
        "**Benchmarks:**\n",
        "*   **Simulated Annealing (SA):** Uses a QUBO formulation with penalty terms.\n",
        "*   **Hierarchical Risk Parity (HRP):** A clustering-based approach that avoids matrix inversion (used as a baseline and fallback).\n",
        "\n",
        "### **Empirical Results (2025 Walk-Forward)**\n",
        "The experiment utilized a rolling window of 180 days to rebalance monthly throughout 2025.\n",
        "\n",
        "#### **A. Financial Performance**\n",
        "| Metric | QAOA (XY) | Simulated Annealing | HRP |\n",
        "| :--- | :---: | :---: | :---: |\n",
        "| **Total Return** | **30.09%** | 24.17% | 10.88% |\n",
        "| **Sharpe Ratio** | **1.81** | 1.31 | 0.98 |\n",
        "| **Max Drawdown** | -8.27% | -9.26% | -8.40% |\n",
        "| **Monthly Turnover**| **76.8%** | 21.0% | 21.6% |\n",
        "\n",
        "*   **Analysis:** QAOA consistently found lower-energy states (better risk-return portfolios) than SA. The quantum solver was more aggressive in reallocating capital to capture short-term alpha.\n",
        "\n",
        "#### **B. Quantum Diagnostics**\n",
        "*   **Depth Scaling:** Performance improved monotonically from $p=1$ to $p=6$. The final cost decreased from -0.1488 to -0.5355.\n",
        "*   **Barren Plateaus:** Gradient magnitudes remained healthy ($5.47 \\times 10^{-2}$ at $p=6$), validating the Trotterized initialization strategy.\n",
        "\n",
        "\n",
        "### **Critical Discussion: The Turnover Trade-off**\n",
        "While QAOA achieved the highest theoretical Sharpe Ratio, the paper highlights a critical operational constraint: **Turnover**.\n",
        "\n",
        "*   **The Observation:** QAOA exhibited 76.8% monthly turnover, compared to ~21% for classical methods.\n",
        "*   **Interpretation:** The quantum algorithm, unburdened by \"warm-start\" biases inherent in SA, aggressively re-optimized the portfolio every month to find the global minimum. While this generates higher gross returns, in a high-transaction-cost environment (e.g., illiquid markets), the net alpha could be eroded.\n",
        "*   **Conclusion:** The method is currently best suited for liquid, low-fee environments (like US Large Caps).\n",
        "\n",
        "### **Verdict**\n",
        "This paper successfully demonstrates that **constraint-preserving QAOA** is not merely a theoretical construct but a viable solver for NP-hard financial problems. By eliminating penalty terms via **XY-mixers** and **Dicke states**, the authors provide a robust framework for Direct Indexing. The superior Sharpe Ratio confirms the quantum algorithm's ability to navigate complex energy landscapes better than thermal heuristics. However, future iterations must incorporate **multi-period coupling** or explicit turnover penalties into the Hamiltonian to make the strategy deployable for institutional capital with strict transaction cost mandates."
      ],
      "metadata": {
        "id": "utxC8CWCCSfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "LGB95LeKbFCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Constrained Portfolio Optimization via QAOA with XY-Mixers and Trotterization\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Constrained Portfolio Optimization via\n",
        "#  Quantum Approximate Optimization Algorithm (QAOA) with XY-Mixers and\n",
        "#  Trotterized Initialization: A Hybrid Approach for Direct Indexing\" by\n",
        "#  Javier Mancilla, Theodoros D. Bouloumis, and Frederic Goguikian (2026).\n",
        "#  It delivers a computationally tractable, hybrid quantum-classical system\n",
        "#  for solving NP-hard cardinality-constrained portfolio selection problems,\n",
        "#  specifically tailored for Direct Indexing and ESG-constrained mandates.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Constraint-preserving QAOA formulation for exact K-of-N asset selection\n",
        "#  • Dicke-state initialization to confine evolution to the feasible subspace\n",
        "#  • Complete-graph XY-mixer Hamiltonian to strictly preserve Hamming weight\n",
        "#  • Trotterized parameter initialization schedule to mitigate Barren Plateaus\n",
        "#  • Simulated Annealing (SA) baseline via QUBO with adaptive penalty scaling\n",
        "#  • Hierarchical Risk Parity (HRP) baseline and robust fallback mechanism\n",
        "#  • Continuous Sharpe-max weight allocation via SLSQP with box constraints\n",
        "#  • Walk-forward backtesting framework with explicit transaction cost modeling\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • PennyLane state-vector simulation for high-fidelity quantum circuit execution\n",
        "#  • D-Wave Neal simulated annealing sampler for classical combinatorial optimization\n",
        "#  • PyPortfolioOpt for Ledoit-Wolf covariance shrinkage and HRP clustering\n",
        "#  • SciPy SLSQP for constrained convex optimization and weight allocation\n",
        "#  • Strict causal slicing and timezone-aware temporal alignment\n",
        "#  • Single-pass value_and_grad optimization to halve quantum simulation overhead\n",
        "#  • Comprehensive artifact persistence and institutional-grade fidelity assertions\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Mancilla, J., Bouloumis, T. D., & Goguikian, F. (2026). Constrained Portfolio\n",
        "#  Optimization via Quantum Approximate Optimization Algorithm (QAOA) with\n",
        "#  XY-Mixers and Trotterized Initialization: A Hybrid Approach for Direct Indexing.\n",
        "#  arXiv preprint arXiv:2602.14827. https://arxiv.org/abs/2602.14827\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# Standard Library Imports\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "import datetime\n",
        "import platform\n",
        "import importlib.metadata\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple, Optional, Union, Set\n",
        "\n",
        "# Third-Party Library Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pennylane as qml\n",
        "import dimod\n",
        "import neal\n",
        "from scipy.optimize import minimize, OptimizeResult\n",
        "from pypfopt import risk_models\n",
        "from pypfopt.hierarchical_risk_parity import HRPOpt\n",
        "\n",
        "# Configure module-level logger\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "mb2jNG6xbI9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "Y5nruCenbKWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft 1\n",
        "\n",
        "## **Discussion of Inputs-Processes-Outputs (IPO) of Key Callables**\n",
        "\n",
        "This section defines the Inputs, Processes, Outputs, Data Transformations, and the exact mathematical and methodological roles each key callable plays in implementing the \"*Constrained Portfolio Optimization via Quantum Approximate Optimization Algorithm (QAOA) with XY-Mixers and Trotterized Initialization: A Hybrid Approach for Direct Indexing*\" research methodology:\n",
        "\n",
        "### **Phase 1: Data Engineering & Validation**\n",
        "\n",
        "**1. `validate_master_config`**\n",
        "*   **Inputs:** Raw, unvalidated configuration dictionary.\n",
        "*   **Processes:** Executes a recursive Depth-First Search to enforce a closed-world schema, mathematically asserts cross-parameter invariants (e.g., $1 \\le K \\le N$), and pins categorical conventions.\n",
        "*   **Outputs:** A strictly typed, mathematically feasible configuration dictionary.\n",
        "*   **Data Transformation:** Transforms an untrusted user payload into a mathematically verified state object.\n",
        "*   **Role in Pipeline:** Ensures the foundational parameters of the study are exact. It implements the constraints: *\"We set $K = 5$ and $q = 0.3$ in all experiments.\"*\n",
        "\n",
        "**2. `validate_raw_price_df`**\n",
        "*   **Inputs:** Raw price DataFrame and validated configuration.\n",
        "*   **Processes:** Asserts `DatetimeIndex` monotonicity, timezone consistency, exact universe alignment, and quantifies missingness ratios.\n",
        "*   **Outputs:** A diagnostic validation report dictionary.\n",
        "*   **Data Transformation:** Transforms raw tabular data into structural metadata.\n",
        "*   **Role in Pipeline:** Validates the integrity of the empirical inputs, fulfilling: *\"Data source & preprocessing. We use Yahoo Finance auto-adjusted Close prices...\"*\n",
        "\n",
        "**3. `cleanse_price_data`**\n",
        "*   **Inputs:** Raw price DataFrame and validated configuration.\n",
        "*   **Processes:** Dynamically flattens MultiIndex structures, reorders columns to the canonical universe, applies forward-filling, and drops remaining NaNs.\n",
        "*   **Outputs:** A pristine, `float64` canonical price matrix.\n",
        "*   **Data Transformation:** Transforms a potentially noisy, multi-dimensional API response into a strictly 2D, dense, positive-definite matrix.\n",
        "*   **Role in Pipeline:** Implements the exact data hygiene protocol: *\"Missing values are forward-filled, and non-trading rows dropped.\"*\n",
        "\n",
        "**4. `build_rebalance_calendar`**\n",
        "*   **Inputs:** Cleansed price matrix and validated configuration.\n",
        "*   **Processes:** Generates theoretical monthly anchors, maps them to realized trading days, and mathematically asserts the availability of $L=180$ prior days.\n",
        "*   **Outputs:** An immutable tuple of 12 `pd.Timestamp` objects.\n",
        "*   **Data Transformation:** Transforms configuration rules into a concrete, causal chronological schedule.\n",
        "*   **Role in Pipeline:** Establishes the temporal backbone of the study, implementing the *\"monthly walk-forward protocol... across calendar year 2025.\"*\n",
        "\n",
        "\n",
        "### **Phase 2: Temporal & Return Infrastructure**\n",
        "\n",
        "**5. `extract_lookback_window`**\n",
        "*   **Inputs:** Cleansed price matrix, rebalance timestamp $t$, lookback $L$, and universe list.\n",
        "*   **Processes:** Applies a strict inequality mask ($< t$) to slice the history, extracting exactly the last $L$ rows.\n",
        "*   **Outputs:** A causal window DataFrame of shape $(L, N)$.\n",
        "*   **Data Transformation:** Transforms the full historical matrix into a localized, causal cross-section.\n",
        "*   **Role in Pipeline:** Prevents look-ahead bias by implementing: *\"estimated from the preceding $L = 180$ trading days of returns, i.e., a rolling window $[t - L, t)$.\"*\n",
        "\n",
        "**6. `compute_daily_log_returns`**\n",
        "*   **Inputs:** Causal window DataFrame and validated configuration.\n",
        "*   **Processes:** Computes element-wise logarithmic returns and truncates the shift-induced NaN row.\n",
        "*   **Outputs:** A daily log-returns DataFrame of shape $(L-1, N)$.\n",
        "*   **Data Transformation:** Transforms non-stationary prices $P_{\\tau,i}$ into stationary returns $R_{\\tau,i} = \\ln(P_{\\tau,i}/P_{\\tau-1,i})$.\n",
        "*   **Role in Pipeline:** Prepares the data for econometric estimation, fulfilling: *\"compute daily returns; drop NA\"*.\n",
        "\n",
        "\n",
        "### **Phase 3: Econometric Estimation & State Management**\n",
        "\n",
        "**7. `estimate_mu_annualized`**\n",
        "*   **Inputs:** Daily log-returns DataFrame and validated configuration.\n",
        "*   **Processes:** Computes the arithmetic sample mean along the time axis and applies the linear annualization scalar (252).\n",
        "*   **Outputs:** The annualized expected return vector $\\mu^{(\\text{ann})} \\in \\mathbb{R}^N$.\n",
        "*   **Data Transformation:** Collapses a 2D returns matrix into a 1D expected return vector.\n",
        "*   **Role in Pipeline:** Generates the linear parameters for the objective functions, implementing: *\"Estimate $\\mu$ (mean)... and annualize both\"*.\n",
        "\n",
        "**8. `estimate_sigma_annualized`**\n",
        "*   **Inputs:** Daily log-returns DataFrame and validated configuration.\n",
        "*   **Processes:** Computes the Ledoit-Wolf shrinkage covariance, annualizes it, and asserts symmetry and positive-definiteness.\n",
        "*   **Outputs:** The annualized covariance matrix $\\Sigma^{(\\text{ann})} \\in \\mathbb{R}^{N \\times N}$.\n",
        "*   **Data Transformation:** Transforms the returns matrix into a well-conditioned, symmetric positive-definite risk matrix.\n",
        "*   **Role in Pipeline:** Generates the quadratic parameters, implementing: *\"Covariance is estimated with Ledoit–Wolf shrinkage... and annualized\"*.\n",
        "\n",
        "**9. `update_continuity_state`**\n",
        "*   **Inputs:** Previous month's binary selection vector, configuration, and solver identifier.\n",
        "*   **Processes:** Initializes a zero vector for the first month or casts the previous selection to a strict integer indicator vector.\n",
        "*   **Outputs:** The continuity indicator vector $s_{\\text{prev}} \\in \\{0,1\\}^N$.\n",
        "*   **Data Transformation:** Transforms historical algorithmic state into a binary regularization mask.\n",
        "*   **Role in Pipeline:** Implements the temporal regularization mechanism: *\"If asset $i$ was held last month, we apply a continuity discount... to reduce churn.\"*\n",
        "\n",
        "\n",
        "### **Phase 4: Simulated Annealing Discrete Selection**\n",
        "\n",
        "**10. `build_qubo_matrix`**\n",
        "*   **Inputs:** $\\mu^{(\\text{ann})}, \\Sigma^{(\\text{ann})}$, configuration, and $s_{\\text{prev}}$.\n",
        "*   **Processes:** Computes the adaptive penalty $P$, constructs the upper-triangular QUBO dictionary, and subtracts $\\kappa$ from the diagonals of previously held assets.\n",
        "*   **Outputs:** A sparse dictionary representing the QUBO matrix.\n",
        "*   **Data Transformation:** Transforms financial moments into a classical quadratic energy landscape.\n",
        "*   **Role in Pipeline:** Implements the classical baseline formulation (Eq. 7):\n",
        "    $$ Q_{ii} = q \\Sigma_{ii}^{(\\text{ann})} - (1 - q) \\mu_i^{(\\text{ann})} + P (1 - 2K) $$\n",
        "    $$ Q_{ij} = 2q \\Sigma_{ij}^{(\\text{ann})} + 2P \\quad (i < j) $$\n",
        "\n",
        "**11. `run_sa_sampling`**\n",
        "*   **Inputs:** QUBO dictionary, $N$, and configuration.\n",
        "*   **Processes:** Converts the QUBO to a `dimod.BinaryQuadraticModel`, instantiates the `neal` sampler, and executes the stochastic heuristic using pinned seeds.\n",
        "*   **Outputs:** A raw `dimod.SampleSet`.\n",
        "*   **Data Transformation:** Transforms the deterministic energy landscape into a stochastic distribution of binary samples.\n",
        "*   **Role in Pipeline:** Executes the classical heuristic: *\"Sample with neal: num\\_reads=5000, num\\_sweeps=1000\"*.\n",
        "\n",
        "**12. `filter_sa_samples`**\n",
        "*   **Inputs:** Raw `SampleSet` and configuration.\n",
        "*   **Processes:** Iterates over aggregated records, filters for exact Hamming weight $K$, and selects the minimum energy candidate via lexicographic tie-breaking.\n",
        "*   **Outputs:** The optimal classical binary vector $x^{SA} \\in \\{0,1\\}^N$.\n",
        "*   **Data Transformation:** Collapses 5000 stochastic samples into a single, deterministic, feasible selection.\n",
        "*   **Role in Pipeline:** Enforces the hard constraint classically: *\"keep best feasible ($\\sum_i x_i = K$)\"*.\n",
        "\n",
        "\n",
        "### **Phase 5: QAOA Ansatz Construction**\n",
        "\n",
        "**13. `prepare_dicke_state_vector`**\n",
        "*   **Inputs:** Universe size $N$ and cardinality $K$.\n",
        "*   **Processes:** Enumerates the $2^N$ Hilbert space, identifies indices with Hamming weight $K$, and assigns a uniform real amplitude.\n",
        "*   **Outputs:** A dense statevector array of length $2^N$.\n",
        "*   **Data Transformation:** Transforms scalar constraints into a quantum probability amplitude vector.\n",
        "*   **Role in Pipeline:** Implements the constraint-preserving initialization (Eq. 2):\n",
        "    $$ |\\psi_0\\rangle = |D^K_N\\rangle = \\binom{N}{K}^{-1/2} \\sum_{|x|=K} |x\\rangle $$\n",
        "\n",
        "**14. `build_xy_mixer_hamiltonian`**\n",
        "*   **Inputs:** Universe size $N$ and configuration.\n",
        "*   **Processes:** Generates complete graph edges and constructs the $X_i X_j + Y_i Y_j$ Pauli tensor products.\n",
        "*   **Outputs:** A `qml.Hamiltonian` object.\n",
        "*   **Data Transformation:** Transforms graph topology into a quantum mixing operator.\n",
        "*   **Role in Pipeline:** Implements the particle-preserving mixer (Eq. 3):\n",
        "    $$ H_{XY} = \\sum_{(i,j) \\in E} (X_i X_j + Y_i Y_j) $$\n",
        "\n",
        "**15. `build_cost_hamiltonian`**\n",
        "*   **Inputs:** $\\mu^{(\\text{ann})}, \\Sigma^{(\\text{ann})}$, configuration, and $s_{\\text{prev}}$.\n",
        "*   **Processes:** Computes $\\alpha_i$ (applying continuity) and $\\beta_{ij}$, mapping them to Pauli-Z observables.\n",
        "*   **Outputs:** A `qml.Hamiltonian` object.\n",
        "*   **Data Transformation:** Transforms financial moments into a quantum phase separator operator.\n",
        "*   **Role in Pipeline:** Implements the Ising mapping (Eq. 5):\n",
        "    $$ H_C = \\sum_i \\alpha_i Z_i + \\sum_{i<j} \\beta_{ij} Z_i Z_j $$\n",
        "\n",
        "\n",
        "### **Phase 6: QAOA Training & Selection**\n",
        "\n",
        "**16. `train_qaoa_single_depth`**\n",
        "*   **Inputs:** Depth $p$, Dicke vector, $H_C, H_{XY}$, and configuration.\n",
        "*   **Processes:** Generates Trotterized parameters, constructs the QNode, and executes a single-pass `value_and_grad` Adam optimization with early stopping.\n",
        "*   **Outputs:** Optimized parameters $\\gamma^*, \\beta^*$ and a telemetry log.\n",
        "*   **Data Transformation:** Transforms heuristic initial parameters into optimized variational parameters via gradient descent.\n",
        "*   **Role in Pipeline:** Implements the Barren Plateau mitigation (Eq. 6) and training:\n",
        "    $$ \\gamma_l = \\frac{l}{p}\\Delta t, \\quad \\beta_l = \\left(1 - \\frac{l}{p}\\right)\\Delta t $$\n",
        "\n",
        "**17. `qaoa_readout_and_rescore`**\n",
        "*   **Inputs:** Trained parameters, quantum invariants, $\\mu, \\Sigma$, and configuration.\n",
        "*   **Processes:** Computes exact statevector probabilities, filters via bitwise operations ($|x|=K, \\Pr \\ge 0.01$), and rescores using the true Markowitz objective.\n",
        "*   **Outputs:** The optimal quantum binary vector $x^*$, its classical cost, and the diagnostic table.\n",
        "*   **Data Transformation:** Collapses a continuous quantum probability distribution into a discrete, deterministic selection.\n",
        "*   **Role in Pipeline:** Bridges the quantum and classical domains: *\"filter weight-$K$, prob $\\ge 1\\%$... Score candidates by classical cost Eq. (1); keep best\"*.\n",
        "\n",
        "**18. `qaoa_select_best_across_depths`**\n",
        "*   **Inputs:** Quantum invariants, $\\mu, \\Sigma$, and configuration.\n",
        "*   **Processes:** Iterates through depths $p \\in \\{1 \\dots 6\\}$, aggregates results, and selects the global minimum classical cost.\n",
        "*   **Outputs:** The final $x^{QAOA}$ vector and the depth-scaling table.\n",
        "*   **Data Transformation:** Reduces multiple depth evaluations into a single optimal portfolio.\n",
        "*   **Role in Pipeline:** Implements the depth-scaling logic: *\"Choose final QAOA selection as best across $p = 1, \\dots, 6$\"*.\n",
        "\n",
        "\n",
        "### **Phase 7: Continuous Allocation & Baseline**\n",
        "\n",
        "**19. `allocate_sharpe_max`**\n",
        "*   **Inputs:** Selection vector $x_t, \\mu, \\Sigma$, and configuration.\n",
        "*   **Processes:** Extracts the $K$-dimensional subset, executes SLSQP to minimize negative Sharpe, validates bounds, and expands back to $N$ dimensions.\n",
        "*   **Outputs:** The continuous weight vector $w_t \\in \\mathbb{R}^N$.\n",
        "*   **Data Transformation:** Transforms a discrete binary selection into an optimal continuous capital allocation.\n",
        "*   **Role in Pipeline:** Implements the classical convex allocation: *\"Solve Sharpe-max (SLSQP) on $S$ with bounds $0.05 \\le w_i \\le 0.50$ and $\\sum_i w_i = 1$\"*.\n",
        "\n",
        "**20. `compute_hrp_weights`**\n",
        "*   **Inputs:** Daily log-returns DataFrame and configuration.\n",
        "*   **Processes:** Instantiates `HRPOpt`, executes recursive bisection clustering, and aligns the output to the canonical universe.\n",
        "*   **Outputs:** The HRP weight vector $w_t \\in \\mathbb{R}^N$.\n",
        "*   **Data Transformation:** Transforms a returns matrix into a hierarchical, risk-parity weighted vector.\n",
        "*   **Role in Pipeline:** Implements the robust baseline: *\"Compute HRP baseline weights (PyPortfolioOpt)\"*.\n",
        "\n",
        "**21. `dispatch_allocation`**\n",
        "*   **Inputs:** Selection $x_t, \\mu, \\Sigma$, returns DataFrame, configuration, and metadata.\n",
        "*   **Processes:** Attempts primary SLSQP allocation, triggers HRP fallback on failure, and mathematically asserts final invariants (sum-to-one, non-negativity).\n",
        "*   **Outputs:** The final, validated weight vector $w_t$ and a fallback flag.\n",
        "*   **Data Transformation:** Routes and resolves intermediate allocations into a guaranteed valid portfolio state.\n",
        "*   **Role in Pipeline:** Implements the fault-tolerance mechanism: *\"If the selector fails, we fall back to HRP weights\"*.\n",
        "\n",
        "\n",
        "### **Phase 8: Performance Accounting & Evaluation**\n",
        "\n",
        "**22. `compute_holding_returns_and_turnover`**\n",
        "*   **Inputs:** Price DataFrame, timestamps $t, t^+$, weights $w_t, w_{t-1}$, and configuration.\n",
        "*   **Processes:** Computes price-relative asset returns, aggregates gross portfolio return via dot product, and calculates $L_1$ turnover.\n",
        "*   **Outputs:** Gross return, asset returns vector, and scalar turnover.\n",
        "*   **Data Transformation:** Transforms prices and weights into realized performance scalars.\n",
        "*   **Role in Pipeline:** Implements the operational accounting: *\"turnover = $\\sum_i |w_{i,t} - w_{i,t-1}|$\"*.\n",
        "\n",
        "**23. `compute_net_return_and_value`**\n",
        "*   **Inputs:** Gross return, turnover, previous value $V_{t-1}$, and configuration.\n",
        "*   **Processes:** Applies the transaction cost penalty, geometrically compounds the portfolio value, and asserts $V_t > 0$.\n",
        "*   **Outputs:** Net return and updated value $V_t$.\n",
        "*   **Data Transformation:** Transforms gross metrics into a net wealth path.\n",
        "*   **Role in Pipeline:** Implements the friction model: *\"Net return = gross - $\\tau \\times$ turnover; $V_t \\leftarrow V_{t-1} (1 + \\text{net return})$\"*.\n",
        "\n",
        "**24. `compute_all_metrics`**\n",
        "*   **Inputs:** Strategy time-series dictionaries, QAOA diagnostics, and configuration.\n",
        "*   **Processes:** Computes Total Return, unbiased Annualized Volatility, Sharpe Ratio, Maximum Drawdown, and aggregates the depth-scaling table.\n",
        "*   **Outputs:** The final comprehensive results dictionary.\n",
        "*   **Data Transformation:** Transforms chronological arrays into summary statistical tables.\n",
        "*   **Role in Pipeline:** Generates the data for Table II and Table III, fulfilling: *\"report net performance\"*.\n",
        "\n",
        "### **Phase 9: Orchestration, Persistence & Verification**\n",
        "\n",
        "**25. `persist_artifacts`**\n",
        "*   **Inputs:** All generated data, decisions, metrics, configuration, and output directory.\n",
        "*   **Processes:** Serializes DataFrames to Parquet, arrays to NPY, dicts to JSON/CSV, and generates a software version manifest.\n",
        "*   **Outputs:** A list of written absolute file paths.\n",
        "*   **Data Transformation:** Freezes in-memory state into immutable disk artifacts.\n",
        "*   **Role in Pipeline:** Ensures institutional reproducibility and auditability of the research pipeline.\n",
        "\n",
        "**26. `run_full_backtest`**\n",
        "*   **Inputs:** Cleansed price DataFrame, rebalance dates, and configuration.\n",
        "*   **Processes:** Initializes state, executes the Algorithm 1 walk-forward loop, finalizes metrics, and triggers persistence.\n",
        "*   **Outputs:** The comprehensive results bundle.\n",
        "*   **Data Transformation:** Orchestrates the macro-transformation of clean data into a complete backtest execution state.\n",
        "*   **Role in Pipeline:** Implements the entirety of *\"Algorithm 1 QAOA-XY / SA Hybrid Portfolio Construction\"*.\n",
        "\n",
        "**27. `run_fidelity_assertions`**\n",
        "*   **Inputs:** Results bundle, price DataFrame, rebalance dates, and configuration.\n",
        "*   **Processes:** Mathematically asserts $K$-hot feasibility, sum-to-one constraints, box bounds, strict causality ($< t$), and structural completeness.\n",
        "*   **Outputs:** None (raises exceptions on failure).\n",
        "*   **Data Transformation:** Transforms the results bundle into a mathematical proof of validity.\n",
        "*   **Role in Pipeline:** Guarantees the pipeline strictly adhered to the *\"hard constraints\"* and causality requirements of the study.\n",
        "\n",
        "**28. `execute_quantum_hybrid_portfolio_optimization` (Top-Level)**\n",
        "*   **Inputs:** Raw price DataFrame, raw configuration, and output directory.\n",
        "*   **Processes:** Executes Phase 1 (Data Engineering) followed by Phase 2 (Optimization & Verification).\n",
        "*   **Outputs:** The final, verified, comprehensive dictionary of all artifacts.\n",
        "*   **Data Transformation:** The ultimate Facade; transforms raw, unstructured inputs into verified, publication-grade artifacts.\n",
        "*   **Role in Pipeline:** Serves as the master entry point for the *\"end-to-end evaluation pipeline\"*.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Usage Example**\n",
        "\n",
        "Here is the granular, step-by-step guide to executing the end-to-end pipeline for **\"Constrained Portfolio Optimization via Quantum Approximate Optimization Algorithm (QAOA) with XY-Mixers and Trotterized Initialization.\"**\n",
        "\n",
        "*Note: This example assumes that all the callables defined in this conversation (from Task 1 through Task 27-plus, including the top-level orchestrator `execute_quantum_hybrid_portfolio_optimization`) are loaded into the current namespace of a single Jupyter Notebook. No external `.py` module imports are required for the pipeline functions.*\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Step 1: Synthetic Data Generation (`raw_price_df`)**\n",
        "\n",
        "The first requirement is a high-fidelity synthetic representation of the raw time-series price matrix. This dataset must adhere strictly to the schema defined in the study: a monotonically increasing `DatetimeIndex` and 10 specific asset columns containing strictly positive, auto-adjusted `float64` prices.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Date Generation:** We generate a sequence of business days (Monday-Friday) spanning from January 2024 through December 2025. This guarantees sufficient pre-history to satisfy the $L=180$ lookback window requirement prior to the first rebalance in January 2025.\n",
        "2.  **Price Simulation:** We simulate asset paths using Geometric Brownian Motion (GBM). GBM is the industry standard for modeling equity prices because it mathematically guarantees strict positivity ($P_t > 0$) and log-normally distributed returns, perfectly aligning with the study's econometric assumptions.\n",
        "3.  **Schema Enforcement:** We explicitly cast the index to `datetime64[ns]` and the matrix to `float64` to satisfy the strict validation gates of Task 2.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Set global seed for reproducible synthetic data generation\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_synthetic_price_matrix(\n",
        "    start_date: str = \"2024-01-01\",\n",
        "    end_date: str = \"2025-12-31\",\n",
        "    universe: list = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"JPM\", \"V\", \"TSLA\", \"UNH\", \"LLY\", \"XOM\"]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a synthetic, high-fidelity price matrix using Geometric Brownian Motion.\n",
        "\n",
        "    Purpose:\n",
        "        To create a mathematically plausible dataset that mimics the schema and statistical\n",
        "        properties of the yfinance auto-adjusted close prices. This allows for the execution\n",
        "        of the quantum-hybrid pipeline without requiring live API access.\n",
        "\n",
        "    Inputs:\n",
        "        start_date (str): The start date of the simulation. Default is \"2024-01-01\".\n",
        "        end_date (str): The end date of the simulation. Default is \"2025-12-31\".\n",
        "        universe (list): The canonical list of 10 asset tickers.\n",
        "\n",
        "    Processes:\n",
        "        1. Date Generation: Creates a business-day frequency DatetimeIndex.\n",
        "        2. GBM Simulation: Simulates strictly positive price paths using drift (mu)\n",
        "           and volatility (sigma) parameters typical of US Large Cap equities.\n",
        "        3. Schema Enforcement: Constructs the DataFrame and enforces float64 types.\n",
        "\n",
        "    Outputs:\n",
        "        pd.DataFrame: A DataFrame of shape (T, 10) with a DatetimeIndex.\n",
        "    \"\"\"\n",
        "    # 1. Generate Trading Days (Business Days)\n",
        "    dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
        "    num_days = len(dates)\n",
        "    num_assets = len(universe)\n",
        "\n",
        "    # 2. Simulate Prices via Geometric Brownian Motion (GBM)\n",
        "    # Assume an average annualized return of 10% and annualized volatility of 20%\n",
        "    mu_annual = 0.10\n",
        "    sigma_annual = 0.20\n",
        "    dt = 1.0 / 252.0  # Daily time step\n",
        "\n",
        "    # Initialize price matrix with starting prices around $150\n",
        "    prices = np.zeros((num_days, num_assets))\n",
        "    prices[0] = np.random.uniform(100, 200, size=num_assets)\n",
        "\n",
        "    # Generate daily shocks\n",
        "    for t in range(1, num_days):\n",
        "        # Standard normal shocks\n",
        "        Z = np.random.standard_normal(num_assets)\n",
        "        # GBM discrete exact solution: P_t = P_{t-1} * exp((mu - sigma^2/2)*dt + sigma*sqrt(dt)*Z)\n",
        "        drift = (mu_annual - 0.5 * sigma_annual**2) * dt\n",
        "        diffusion = sigma_annual * np.sqrt(dt) * Z\n",
        "        prices[t] = prices[t-1] * np.exp(drift + diffusion)\n",
        "\n",
        "    # 3. Construct DataFrame and Enforce Schema\n",
        "    raw_price_df = pd.DataFrame(prices, index=dates, columns=universe)\n",
        "    raw_price_df.index.name = \"Date\"\n",
        "    raw_price_df = raw_price_df.astype(np.float64)\n",
        "\n",
        "    return raw_price_df\n",
        "\n",
        "# Generate the synthetic dataset\n",
        "raw_price_df = generate_synthetic_price_matrix()\n",
        "\n",
        "# Preview the data to verify schema compliance\n",
        "print(\"Synthetic Price Matrix Preview:\")\n",
        "print(raw_price_df.head())\n",
        "print(f\"\\nShape: {raw_price_df.shape}\")\n",
        "print(f\"Index Type: {type(raw_price_df.index)}\")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Step 2: Loading the Configuration (`config.yaml`)**\n",
        "\n",
        "The study relies on a deterministic configuration file (`config.yaml`) that defines all hyperparameters, quantum circuit architectures, and evaluation metrics. We assume this file has been saved in the current working directory.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **File I/O:** Open `config.yaml` in read mode.\n",
        "2.  **Parsing:** Use `yaml.safe_load` to convert the YAML structure into a nested Python dictionary.\n",
        "3.  **Validation:** The resulting dictionary will be passed to the orchestrator, which will subject it to the rigorous recursive type-checking defined in Task 1.\n",
        "\n",
        "```python\n",
        "def load_study_configuration(filepath: str = \"config.yaml\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Loads the study configuration parameters from a YAML file into a Python dictionary.\n",
        "\n",
        "    Purpose:\n",
        "        To ingest the deterministic hyperparameters, quantum circuit definitions, and\n",
        "        evaluation metrics defined in the external configuration file. This ensures\n",
        "        institutional reproducibility by separating code from configuration state.\n",
        "\n",
        "    Inputs:\n",
        "        filepath (str): The path to the YAML configuration file. Default is \"config.yaml\".\n",
        "\n",
        "    Outputs:\n",
        "        Dict[str, Any]: A nested dictionary containing the study configuration.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the configuration file is missing.\n",
        "        yaml.YAMLError: If the file contains invalid YAML syntax.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"CRITICAL: Configuration file '{filepath}' not found in the working directory.\")\n",
        "\n",
        "    try:\n",
        "        with open(filepath, \"r\") as file:\n",
        "            config = yaml.safe_load(file)\n",
        "        print(f\"\\nSuccessfully loaded configuration from {filepath}\")\n",
        "        return config\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"\\nError parsing YAML file {filepath}: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load the configuration\n",
        "# Note: Ensure 'config.yaml' is in your working directory with the content provided previously.\n",
        "config = load_study_configuration(\"config.yaml\")\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Step 3: Executing the Pipeline (`execute_quantum_hybrid_portfolio_optimization`)**\n",
        "\n",
        "With the data (`raw_price_df`) and configuration (`config`) in memory, we invoke the top-level orchestrator. This function manages the entire lifecycle: data cleansing, causal calendar generation, quantum circuit simulation (QAOA-XY), classical simulated annealing (SA), continuous weight allocation (SLSQP), performance accounting, and the final mathematical fidelity audit.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Function Call:** Pass the DataFrame, Dictionary, and an output directory path to the orchestrator.\n",
        "2.  **Output Handling:** The function returns a comprehensive dictionary containing the validated artifacts, financial metrics, depth-scaling tables, and a boolean flag confirming that all mathematical constraints (e.g., $K$-hot feasibility, zero look-ahead bias) were strictly upheld.\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# Execution of the End-to-End Study Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure we have valid inputs before running\n",
        "    if not raw_price_df.empty and config:\n",
        "        \n",
        "        print(\"\\nInitiating Quantum-Hybrid Portfolio Optimization Pipeline...\")\n",
        "        \n",
        "        # Define the output directory for serialized artifacts (Parquet, NPY, JSON)\n",
        "        output_directory = \"./quantum_portfolio_artifacts\"\n",
        "        \n",
        "        # Execute the pipeline\n",
        "        # This will trigger the extensive logging defined throughout the 27 tasks\n",
        "        study_artifacts = execute_quantum_hybrid_portfolio_optimization(\n",
        "            raw_price_df=raw_price_df,\n",
        "            raw_config=config,\n",
        "            output_dir=output_directory\n",
        "        )\n",
        "        \n",
        "        # ==============================================================================\n",
        "        # Inspecting the Outputs\n",
        "        # ==============================================================================\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STUDY EXECUTION COMPLETE\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # 1. Verify Institutional Fidelity\n",
        "        fidelity_status = study_artifacts.get(\"fidelity_verified\", False)\n",
        "        print(f\"\\n[Fidelity Audit]\")\n",
        "        print(f\"Mathematical Constraints & Causality Verified: {fidelity_status}\")\n",
        "        \n",
        "        # 2. Accessing Financial Metrics (Table III equivalent)\n",
        "        print(\"\\n[Financial Performance Metrics (2025)]\")\n",
        "        metrics_dict = study_artifacts[\"results_bundle\"][\"financial_metrics\"]\n",
        "        metrics_df = pd.DataFrame.from_dict(metrics_dict, orient=\"index\")\n",
        "        # Format for readability\n",
        "        metrics_df[\"Total Return\"] = metrics_df[\"Total Return\"].apply(lambda x: f\"{x:.2%}\")\n",
        "        metrics_df[\"Annualized Volatility\"] = metrics_df[\"Annualized Volatility\"].apply(lambda x: f\"{x:.2%}\")\n",
        "        metrics_df[\"Max Drawdown\"] = metrics_df[\"Max Drawdown\"].apply(lambda x: f\"{x:.2%}\")\n",
        "        metrics_df[\"Average Monthly Turnover\"] = metrics_df[\"Average Monthly Turnover\"].apply(lambda x: f\"{x:.2%}\")\n",
        "        metrics_df[\"Sharpe Ratio\"] = metrics_df[\"Sharpe Ratio\"].apply(lambda x: f\"{x:.2f}\")\n",
        "        print(metrics_df)\n",
        "        \n",
        "        # 3. Accessing QAOA Depth-Scaling Diagnostics (Table II equivalent)\n",
        "        print(\"\\n[QAOA-XY Depth Scaling Diagnostics]\")\n",
        "        depth_table = study_artifacts[\"results_bundle\"].get(\"depth_scaling_table\", [])\n",
        "        if depth_table:\n",
        "            depth_df = pd.DataFrame(depth_table)\n",
        "            print(depth_df.to_string(index=False))\n",
        "            \n",
        "        # 4. Confirm Artifact Persistence\n",
        "        persisted_files = study_artifacts[\"results_bundle\"].get(\"persisted_files\", [])\n",
        "        print(f\"\\n[Artifact Persistence]\")\n",
        "        print(f\"Successfully serialized {len(persisted_files)} artifacts to '{output_directory}'.\")\n",
        "        \n",
        "    else:\n",
        "        print(\"Error: Missing data or configuration. Cannot proceed.\")\n",
        "```\n",
        "\n",
        "### **Summary of the Execution Flow**\n",
        "\n",
        "1.  **Data Ingestion:** The synthetic `raw_price_df` and parsed `config` are passed to the top-level orchestrator.\n",
        "2.  **Phase 1 (Data Engineering):** The pipeline recursively validates the schema, cleanses the price matrix (handling NaNs), and freezes a strictly causal 12-month rebalance calendar for 2025.\n",
        "3.  **Phase 2 (Optimization Loop):** For each month, it estimates $\\mu$ and $\\Sigma$, constructs the QUBO and QAOA Hamiltonians, executes the quantum simulation across depths $p \\in \\{1 \\dots 6\\}$, and selects the optimal $K=5$ assets.\n",
        "4.  **Allocation & Accounting:** It solves the continuous Sharpe-max problem via SLSQP, applies the 5 bps transaction cost to the realized turnover, and compounds the portfolio value.\n",
        "5.  **Verification:** It mathematically proves that no look-ahead bias occurred and that all portfolios strictly adhered to the $K$-hot and sum-to-one constraints before returning the final publication-grade metrics.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## **Implemented Callables**"
      ],
      "metadata": {
        "id": "EhHPCG2FbOwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Validate `MASTER_STUDY_CONFIGURATION` integrity and completeness\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate MASTER_STUDY_CONFIGURATION integrity and completeness\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Verify required top-level and nested keys exist with correct types (closed schema).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Define the function to verify the schema types and completeness\n",
        "def _verify_schema_types_and_completeness(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Recursively verifies that the configuration dictionary strictly adheres to the exhaustive schema.\n",
        "\n",
        "    This function enforces a \"closed-world assumption\" for the configuration payload. It\n",
        "    traverses every node of the dictionary, mathematically asserting that no keys are missing,\n",
        "    no unauthorized keys are present, and every value strictly matches its expected Python type.\n",
        "    This prevents silent numerical drift caused by misconfigured solver parameters.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any keys are missing or unexpected at any level of the hierarchy.\n",
        "        TypeError: If any configuration value does not match the expected type.\n",
        "    \"\"\"\n",
        "    # Define the absolute ground truth schema for the entire configuration payload.\n",
        "    # Note: Free-form dictionaries (like plotting_parameters) are mapped to the `dict` type class,\n",
        "    # while structured dictionaries are mapped to nested dictionary instances.\n",
        "    exhaustive_schema = {\n",
        "        \"global_setup\": {\n",
        "            \"universe\": list, \"lookback_window_L\": int, \"annualization_factor\": int,\n",
        "            \"rebalance_frequency\": str, \"backtest_year\": int\n",
        "        },\n",
        "        \"rebalance_calendar\": {\n",
        "            \"rebalance_rule\": str, \"rebalance_dates_override\": (list, type(None))\n",
        "        },\n",
        "        \"data_conventions\": {\n",
        "            \"price_field\": str, \"yfinance_auto_adjust\": bool, \"missing_data_handling\": str,\n",
        "            \"window_construction_policy\": str, \"min_observations_per_window\": int\n",
        "        },\n",
        "        \"return_conventions\": {\n",
        "            \"return_type\": str, \"holding_period_return_method\": str\n",
        "        },\n",
        "        \"capital_and_bounds\": {\n",
        "            \"initial_capital\": float, \"allocation_bounds\": tuple\n",
        "        },\n",
        "        \"objective_function\": {\n",
        "            \"cardinality_K\": int, \"risk_aversion_q\": float, \"continuity_bonus_kappa\": float,\n",
        "            \"continuity_bonus_application_rule\": str, \"classical_objective_rescoring_policy\": str\n",
        "        },\n",
        "        \"estimation\": {\n",
        "            \"covariance_estimator\": str, \"mean_estimator\": str, \"annualize_mu_and_sigma\": bool\n",
        "        },\n",
        "        \"qaoa_architecture\": {\n",
        "            \"qaoa_depths_p\": list, \"qaoa_device_name\": str, \"qaoa_measurement_mode\": str,\n",
        "            \"qaoa_shots\": (int, type(None)), \"xy_mixer_graph_type\": str,\n",
        "            \"qaoa_gamma_bounds\": tuple, \"qaoa_beta_bounds\": tuple, \"trotter_initialization_rule\": str\n",
        "        },\n",
        "        \"qaoa_optimization\": {\n",
        "            \"qaoa_stepsize\": float, \"qaoa_epsilon\": float, \"qaoa_max_iterations\": int,\n",
        "            \"qaoa_use_early_stopping\": bool,\n",
        "            \"qaoa_early_stopping\": {  # Explicitly defined nested schema\n",
        "                \"monitor\": str,\n",
        "                \"patience\": int,\n",
        "                \"min_delta\": float\n",
        "            },\n",
        "            \"qaoa_readout_filter_prob\": float, \"qaoa_readout_feasibility_filter\": str\n",
        "        },\n",
        "        \"sa_solver\": {\n",
        "            \"sa_penalty_scalar\": float, \"sa_num_reads\": int, \"sa_num_sweeps\": int,\n",
        "            \"sa_feasibility_filter\": str\n",
        "        },\n",
        "        \"allocation\": {\n",
        "            \"allocation_objective\": str, \"risk_free_rate_annual\": float, \"optimizer\": str,\n",
        "            \"full_investment_constraint\": str, \"bounds\": tuple\n",
        "        },\n",
        "        \"hrp\": {\n",
        "            \"compute_hrp_baseline\": bool, \"use_hrp_as_fallback\": bool\n",
        "        },\n",
        "        \"evaluation_metrics\": {\n",
        "            \"transaction_cost_bps\": float, \"turnover_definition\": str,\n",
        "            \"periods_per_year\": int, \"sharpe_definition_policy\": str\n",
        "        },\n",
        "        \"randomness\": {\n",
        "            \"global_seed\": int, \"sa_seed\": int, \"qaoa_seed\": int\n",
        "        },\n",
        "        \"plotting_parameters\": dict,      # Leaf node: free-form dictionary\n",
        "        \"raw_data_schema_example\": dict   # Leaf node: free-form dictionary\n",
        "    }\n",
        "\n",
        "    def _recursive_type_check(config_node: Dict[str, Any], schema_node: Dict[str, Any], path_string: str) -> None:\n",
        "        \"\"\"Helper function to recursively traverse and validate the configuration tree.\"\"\"\n",
        "        config_keys = set(config_node.keys())\n",
        "        schema_keys = set(schema_node.keys())\n",
        "\n",
        "        # Assert exact set equality of keys at the current node\n",
        "        if config_keys != schema_keys:\n",
        "            missing = schema_keys - config_keys\n",
        "            unexpected = config_keys - schema_keys\n",
        "            error_msg = f\"Schema violation at '{path_string}'.\"\n",
        "            if missing:\n",
        "                error_msg += f\" Missing keys: {missing}.\"\n",
        "            if unexpected:\n",
        "                error_msg += f\" Unexpected keys: {unexpected}.\"\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        # Iterate through keys and assert types\n",
        "        for key, expected_type in schema_node.items():\n",
        "            current_path = f\"{path_string}.{key}\" if path_string else key\n",
        "            value = config_node[key]\n",
        "\n",
        "            # Bifurcate logic to handle nested dictionaries vs primitive types\n",
        "            if isinstance(expected_type, dict):\n",
        "                # If the schema defines a nested dictionary, assert the value is a dict\n",
        "                if not isinstance(value, dict):\n",
        "                    raise TypeError(f\"Type violation at '{current_path}'. Expected dict, got {type(value).__name__}.\")\n",
        "\n",
        "                # Recurse deeper into the tree\n",
        "                _recursive_type_check(value, expected_type, current_path)\n",
        "            else:\n",
        "                # If the schema defines a primitive type (or tuple of types), assert it directly\n",
        "                if not isinstance(value, expected_type):\n",
        "                    raise TypeError(f\"Type violation at '{current_path}'. Expected {expected_type}, got {type(value).__name__}.\")\n",
        "\n",
        "    # Initiate the recursive validation from the root of the configuration dictionary\n",
        "    _recursive_type_check(config, exhaustive_schema, \"\")\n",
        "    logger.debug(\"Exhaustive recursive schema validation passed.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate cross-parameter invariants that determine mathematical feasibility.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Define the function to validate mathematical invariants\n",
        "def _validate_cross_parameter_invariants(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the mathematical feasibility of the configuration parameters.\n",
        "\n",
        "    This function ensures that the cardinality constraint is valid given the universe size,\n",
        "    that the risk aversion parameter is within the unit interval, that the continuous\n",
        "    allocation bounds permit a feasible sum-to-one solution, and that solver parameters\n",
        "    are strictly positive.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any mathematical invariant is violated.\n",
        "    \"\"\"\n",
        "    # Extract the universe size N\n",
        "    n_assets = len(config[\"global_setup\"][\"universe\"])\n",
        "    # Extract the cardinality constraint K\n",
        "    k_cardinality = config[\"objective_function\"][\"cardinality_K\"]\n",
        "\n",
        "    # Assert that K is between 1 and N inclusive\n",
        "    if not (1 <= k_cardinality <= n_assets):\n",
        "        # Raise an error if the cardinality constraint is mathematically invalid\n",
        "        raise ValueError(f\"Cardinality K ({k_cardinality}) must satisfy 1 <= K <= N ({n_assets})\")\n",
        "\n",
        "    # Extract the risk aversion parameter q\n",
        "    risk_aversion_q = config[\"objective_function\"][\"risk_aversion_q\"]\n",
        "\n",
        "    # Assert that q is within the closed interval [0, 1]\n",
        "    if not (0.0 <= risk_aversion_q <= 1.0):\n",
        "        # Raise an error if the risk aversion parameter is out of bounds\n",
        "        raise ValueError(f\"Risk aversion q ({risk_aversion_q}) must satisfy 0 <= q <= 1\")\n",
        "\n",
        "    # Extract the minimum and maximum allocation bounds\n",
        "    w_min, w_max = config[\"capital_and_bounds\"][\"allocation_bounds\"]\n",
        "\n",
        "    # Calculate the minimum possible sum of weights for K assets\n",
        "    min_possible_sum = k_cardinality * w_min\n",
        "    # Calculate the maximum possible sum of weights for K assets\n",
        "    max_possible_sum = k_cardinality * w_max\n",
        "\n",
        "    # Assert that the sum-to-one constraint is feasible given the bounds and cardinality\n",
        "    if not (min_possible_sum <= 1.0 <= max_possible_sum):\n",
        "        # Raise an error if the allocation bounds make the equality constraint infeasible\n",
        "        raise ValueError(f\"Allocation bounds ({w_min}, {w_max}) with K={k_cardinality} make sum=1 constraint infeasible. \"\n",
        "                         f\"Min sum: {min_possible_sum}, Max sum: {max_possible_sum}\")\n",
        "\n",
        "    # Extract the list of QAOA depths to scan\n",
        "    qaoa_depths = config[\"qaoa_architecture\"][\"qaoa_depths_p\"]\n",
        "\n",
        "    # Assert that the depths list is not empty\n",
        "    if not qaoa_depths:\n",
        "        # Raise an error if no QAOA depths are provided\n",
        "        raise ValueError(\"QAOA depths list cannot be empty\")\n",
        "\n",
        "    # Assert that the maximum depth does not exceed the practical limit of 10\n",
        "    if max(qaoa_depths) > 10:\n",
        "        # Raise an error if the circuit depth is pathologically high\n",
        "        raise ValueError(f\"Maximum QAOA depth ({max(qaoa_depths)}) exceeds practical limit of 10\")\n",
        "\n",
        "    # Iterate through the provided QAOA depths\n",
        "    for depth in qaoa_depths:\n",
        "        # Assert that each depth is a strictly positive integer\n",
        "        if not isinstance(depth, int) or depth <= 0:\n",
        "            # Raise an error if an invalid depth is found\n",
        "            raise ValueError(f\"QAOA depths must be positive integers, got {depth}\")\n",
        "\n",
        "    # Extract the number of reads for the Simulated Annealing solver\n",
        "    sa_reads = config[\"sa_solver\"][\"sa_num_reads\"]\n",
        "    # Extract the number of sweeps for the Simulated Annealing solver\n",
        "    sa_sweeps = config[\"sa_solver\"][\"sa_num_sweeps\"]\n",
        "\n",
        "    # Assert that the number of SA reads is strictly positive\n",
        "    if sa_reads <= 0:\n",
        "        # Raise an error if SA reads are invalid\n",
        "        raise ValueError(f\"SA num_reads must be > 0, got {sa_reads}\")\n",
        "    # Assert that the number of SA sweeps is strictly positive\n",
        "    if sa_sweeps <= 0:\n",
        "        # Raise an error if SA sweeps are invalid\n",
        "        raise ValueError(f\"SA num_sweeps must be > 0, got {sa_sweeps}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate that every drift-inducing convention is explicitly pinned.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Define the function to validate categorical conventions\n",
        "def _validate_drift_inducing_conventions(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates that all categorical and drift-inducing conventions are explicitly pinned.\n",
        "\n",
        "    This function ensures that string-based configurations match the exact permitted\n",
        "    literals, preventing silent methodological drift. It also enforces the determinism\n",
        "    contract for random seeds and measurement modes.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a convention string is unrecognized or a seed is invalid.\n",
        "    \"\"\"\n",
        "    # Extract the rebalance rule string\n",
        "    rebalance_rule = config[\"rebalance_calendar\"].get(\"rebalance_rule\")\n",
        "    # Extract the explicit rebalance dates override list\n",
        "    rebalance_override = config[\"rebalance_calendar\"].get(\"rebalance_dates_override\")\n",
        "\n",
        "    # Assert that at least one deterministic rebalance schedule method is provided\n",
        "    if not rebalance_rule and not rebalance_override:\n",
        "        # Raise an error if the rebalance schedule is completely ambiguous\n",
        "        raise ValueError(\"Must provide either a rebalance_rule or rebalance_dates_override\")\n",
        "\n",
        "    # Extract the return type convention\n",
        "    return_type = config[\"return_conventions\"][\"return_type\"]\n",
        "    # Assert that the return type is exactly 'log' or 'simple'\n",
        "    if return_type not in {\"log\", \"simple\"}:\n",
        "        # Raise an error if the return type is unrecognized\n",
        "        raise ValueError(f\"return_type must be 'log' or 'simple', got '{return_type}'\")\n",
        "\n",
        "    # Extract the holding period return method convention\n",
        "    hp_method = config[\"return_conventions\"][\"holding_period_return_method\"]\n",
        "    # Assert that the holding period method is exactly 'price_relative' or 'compound_daily_returns'\n",
        "    if hp_method not in {\"price_relative\", \"compound_daily_returns\"}:\n",
        "        # Raise an error if the holding period method is unrecognized\n",
        "        raise ValueError(f\"holding_period_return_method must be 'price_relative' or 'compound_daily_returns', got '{hp_method}'\")\n",
        "\n",
        "    # Extract the XY mixer graph topology convention\n",
        "    mixer_type = config[\"qaoa_architecture\"][\"xy_mixer_graph_type\"]\n",
        "    # Assert that the mixer topology is exactly 'complete' or 'ring'\n",
        "    if mixer_type not in {\"complete\", \"ring\"}:\n",
        "        # Raise an error if the mixer topology is unrecognized\n",
        "        raise ValueError(f\"xy_mixer_graph_type must be 'complete' or 'ring', got '{mixer_type}'\")\n",
        "\n",
        "    # Extract the Trotterized initialization rule convention\n",
        "    trotter_rule = config[\"qaoa_architecture\"][\"trotter_initialization_rule\"]\n",
        "    # Assert that the Trotter rule matches the pinned literal exactly\n",
        "    if trotter_rule != \"linear_ramp_between_bounds_per_depth\":\n",
        "        # Raise an error if the Trotter rule is unrecognized\n",
        "        raise ValueError(f\"trotter_initialization_rule must be 'linear_ramp_between_bounds_per_depth', got '{trotter_rule}'\")\n",
        "\n",
        "    # Extract the global random seed\n",
        "    global_seed = config[\"randomness\"][\"global_seed\"]\n",
        "    # Extract the Simulated Annealing random seed\n",
        "    sa_seed = config[\"randomness\"][\"sa_seed\"]\n",
        "    # Extract the QAOA random seed\n",
        "    qaoa_seed = config[\"randomness\"][\"qaoa_seed\"]\n",
        "\n",
        "    # Iterate through the extracted seeds to validate them\n",
        "    for seed_name, seed_val in [(\"global_seed\", global_seed), (\"sa_seed\", sa_seed), (\"qaoa_seed\", qaoa_seed)]:\n",
        "        # Assert that each seed is a non-negative integer\n",
        "        if not isinstance(seed_val, int) or seed_val < 0:\n",
        "            # Raise an error if a seed is invalid, breaking the determinism contract\n",
        "            raise ValueError(f\"{seed_name} must be a non-negative integer, got {seed_val}\")\n",
        "\n",
        "    # Extract the annual risk-free rate\n",
        "    rf_rate = config[\"allocation\"][\"risk_free_rate_annual\"]\n",
        "    # Assert that the risk-free rate is explicitly provided as a float\n",
        "    if not isinstance(rf_rate, float):\n",
        "        # Raise an error if the risk-free rate is not a float\n",
        "        raise TypeError(f\"risk_free_rate_annual must be a float, got {type(rf_rate).__name__}\")\n",
        "\n",
        "    # Extract the QAOA measurement mode\n",
        "    measurement_mode = config[\"qaoa_architecture\"][\"qaoa_measurement_mode\"]\n",
        "    # Extract the number of QAOA shots\n",
        "    qaoa_shots = config[\"qaoa_architecture\"][\"qaoa_shots\"]\n",
        "\n",
        "    # Check if the measurement mode is set to full statevector probabilities\n",
        "    if measurement_mode == \"statevector_full_probs\":\n",
        "        # Assert that shots are explicitly set to None to prevent shot noise drift\n",
        "        if qaoa_shots is not None:\n",
        "            # Raise an error if shots are provided during statevector simulation\n",
        "            raise ValueError(\"qaoa_shots must be None when qaoa_measurement_mode is 'statevector_full_probs'\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Define the main orchestrator function for configuration validation\n",
        "def validate_master_config(config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the comprehensive validation of the master study configuration.\n",
        "\n",
        "    This function acts as a strict gatekeeper, ensuring that the provided configuration\n",
        "    dictionary meets all structural, mathematical, and categorical requirements before\n",
        "    allowing the research pipeline to proceed. It executes the validation steps in a\n",
        "    deterministic sequence and returns the unmutated configuration upon success.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The raw master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The validated, unmutated configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any structural, mathematical, or categorical invariant is violated.\n",
        "        TypeError: If any configuration value possesses an incorrect data type.\n",
        "    \"\"\"\n",
        "    # Execute Step 1: Verify structural schema and types\n",
        "    _verify_schema_types_and_completeness(config)\n",
        "\n",
        "    # Execute Step 2: Verify mathematical feasibility invariants\n",
        "    _validate_cross_parameter_invariants(config)\n",
        "\n",
        "    # Execute Step 3: Verify categorical conventions and determinism contracts\n",
        "    _validate_drift_inducing_conventions(config)\n",
        "\n",
        "    # Return the validated configuration dictionary, confirming it is safe for downstream use\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "qxmA8hH4bQ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate raw_price_df schema, index, and value integrity (validate_raw_price_df)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate raw_price_df schema, index, and value integrity\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate the index: type, monotonicity, duplicates, and horizon sufficiency.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_index_integrity(raw_price_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the temporal integrity and timezone consistency of the raw price DataFrame's index.\n",
        "\n",
        "    This function ensures the index is a DatetimeIndex, is strictly monotonically\n",
        "    increasing, contains no duplicate timestamps, possesses a consistent timezone state,\n",
        "    and spans a sufficient chronological horizon to support the 2025 backtest.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw time-series price matrix.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing index validation metrics.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the index is not a pandas DatetimeIndex.\n",
        "        ValueError: If the index is not monotonic, has duplicates, or lacks sufficient history.\n",
        "    \"\"\"\n",
        "    idx = raw_price_df.index\n",
        "\n",
        "    # Assert the index is of type pandas.DatetimeIndex\n",
        "    if not isinstance(idx, pd.DatetimeIndex):\n",
        "        raise TypeError(f\"raw_price_df index must be a pd.DatetimeIndex, got {type(idx).__name__}\")\n",
        "\n",
        "    # Assert the index is strictly monotonically increasing\n",
        "    if not idx.is_monotonic_increasing:\n",
        "        raise ValueError(\"raw_price_df index is not monotonically increasing. Chronological ordering is required.\")\n",
        "\n",
        "    # Assert there are no duplicate timestamps\n",
        "    if idx.has_duplicates:\n",
        "        duplicates = idx[idx.duplicated()].unique()\n",
        "        raise ValueError(f\"raw_price_df index contains duplicate timestamps: {duplicates}\")\n",
        "\n",
        "    # Assert timezone consistency\n",
        "    # A valid DatetimeIndex in pandas will have a single tz attribute. If it were mixed,\n",
        "    # pandas would have cast the index to an Object dtype, which we caught above.\n",
        "    # We log the timezone state for downstream alignment awareness.\n",
        "    index_tz = idx.tz\n",
        "    if index_tz is None:\n",
        "        logger.debug(\"Price index is timezone-naive.\")\n",
        "    else:\n",
        "        logger.debug(f\"Price index is timezone-aware: {index_tz}\")\n",
        "\n",
        "    earliest_date = idx.min()\n",
        "    latest_date = idx.max()\n",
        "\n",
        "    # Assert the latest date is in or after the backtest year (2025)\n",
        "    if latest_date < pd.Timestamp('2025-01-01', tz=index_tz):\n",
        "        raise ValueError(f\"Data horizon insufficient. Latest date {latest_date.date()} is before the 2025 backtest period.\")\n",
        "\n",
        "    # Enforce the heuristic: count rows strictly before 2025-02-01 to ensure L=180 is feasible\n",
        "    pre_rebalance_cutoff = pd.Timestamp('2025-02-01', tz=index_tz)\n",
        "    pre_history_mask = idx < pre_rebalance_cutoff\n",
        "    pre_history_count = pre_history_mask.sum()\n",
        "\n",
        "    # Assert the pre-history count meets the L=180 requirement\n",
        "    if pre_history_count < 180:\n",
        "        raise ValueError(f\"Insufficient historical data. Found {pre_history_count} trading days before \"\n",
        "                         f\"{pre_rebalance_cutoff.date()}, but at least L=180 are required.\")\n",
        "\n",
        "    return {\n",
        "        \"total_rows\": len(idx),\n",
        "        \"earliest_date\": earliest_date.isoformat(),\n",
        "        \"latest_date\": latest_date.isoformat(),\n",
        "        \"timezone\": str(index_tz) if index_tz else \"naive\",\n",
        "        \"pre_2025_02_01_row_count\": int(pre_history_count)\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate columns: single-level index, exact universe membership, numeric types, non-empty series.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_column_integrity(raw_price_df: pd.DataFrame, universe: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the structural and schema integrity of the raw price DataFrame's columns.\n",
        "\n",
        "    This function ensures the columns form a single-level index, exactly match the\n",
        "    canonical universe of tickers without omissions or additions, contain only numeric\n",
        "    data types, and are not entirely composed of missing values.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw time-series price matrix.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing column validation metrics.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the columns are a MultiIndex or contain non-numeric data.\n",
        "        ValueError: If the column labels do not exactly match the universe, or if a column is entirely NaN.\n",
        "    \"\"\"\n",
        "    # Extract the columns object\n",
        "    cols = raw_price_df.columns\n",
        "\n",
        "    # Assert the column index is a single-level index (not a MultiIndex)\n",
        "    if isinstance(cols, pd.MultiIndex):\n",
        "        raise TypeError(\"raw_price_df columns must be a single-level index. MultiIndex detected. \"\n",
        "                        \"Data must be normalized prior to this validation step.\")\n",
        "\n",
        "    # Convert columns and universe to sets for exact membership comparison\n",
        "    col_set = set(cols)\n",
        "    universe_set = set(universe)\n",
        "\n",
        "    # Assert exact set equality between DataFrame columns and the canonical universe\n",
        "    if col_set != universe_set:\n",
        "        missing = universe_set - col_set\n",
        "        unexpected = col_set - universe_set\n",
        "        error_msg = \"Column labels do not exactly match the canonical universe.\"\n",
        "        if missing:\n",
        "            error_msg += f\" Missing tickers: {missing}.\"\n",
        "        if unexpected:\n",
        "            error_msg += f\" Unexpected tickers: {unexpected}.\"\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Iterate through each column to validate data types and non-empty status\n",
        "    for ticker in universe:\n",
        "        series = raw_price_df[ticker]\n",
        "\n",
        "        # Assert the column data type is numeric (float64 or coercible)\n",
        "        if not pd.api.types.is_numeric_dtype(series):\n",
        "            raise TypeError(f\"Column '{ticker}' must be numeric, got dtype {series.dtype}\")\n",
        "\n",
        "        # Assert the column is not entirely composed of NaN values\n",
        "        if series.isna().all():\n",
        "            raise ValueError(f\"Column '{ticker}' is entirely NaN. Asset cannot be optimized.\")\n",
        "\n",
        "    # Return the column validation metrics for the audit report\n",
        "    return {\n",
        "        \"column_count\": len(cols),\n",
        "        \"universe_match\": True,\n",
        "        \"all_numeric\": True\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate value integrity: missingness quantification, temporary forward-fill sanity, strict positivity.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_value_integrity(raw_price_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the numerical integrity of the raw price DataFrame's values.\n",
        "\n",
        "    This function quantifies missing data (NaNs) per asset, logs warnings if the missingness\n",
        "    ratio exceeds 5%, and performs a strict positivity check on a temporary forward-filled\n",
        "    view of the data to ensure no zero or negative prices exist.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw time-series price matrix.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing value integrity metrics and NaN statistics.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any non-positive prices are detected after forward-filling.\n",
        "    \"\"\"\n",
        "    total_rows = len(raw_price_df)\n",
        "    nan_counts = {}\n",
        "    nan_ratios = {}\n",
        "    warnings_logged = []\n",
        "\n",
        "    # Iterate through each column to quantify missingness\n",
        "    for col in raw_price_df.columns:\n",
        "        # Calculate the absolute number of NaN values\n",
        "        nan_count = int(raw_price_df[col].isna().sum())\n",
        "        # Calculate the ratio of NaN values to total rows\n",
        "        nan_ratio = float(nan_count / total_rows)\n",
        "\n",
        "        nan_counts[col] = nan_count\n",
        "        nan_ratios[col] = nan_ratio\n",
        "\n",
        "        # Implement the prescribed heuristic: warn if NaN ratio > 5%\n",
        "        if nan_ratio > 0.05:\n",
        "            warning_msg = f\"Asset '{col}' has a high missing data ratio: {nan_ratio:.2%} ({nan_count} rows).\"\n",
        "            logger.warning(warning_msg)\n",
        "            warnings_logged.append(warning_msg)\n",
        "\n",
        "    # Create a temporary forward-filled view of the DataFrame for the positivity check\n",
        "    # This propagates the last known price forward, simulating the cleansing policy\n",
        "    temp_ffilled_df = raw_price_df.ffill()\n",
        "\n",
        "    # Evaluate strict positivity: P_{t,i} > 0\n",
        "    # We only check non-NaN entries, as leading NaNs will remain NaN after ffill\n",
        "    # The condition (temp_ffilled_df <= 0) creates a boolean mask of violations\n",
        "    if (temp_ffilled_df <= 0).any().any():\n",
        "        # Identify the specific columns containing non-positive values for the error message\n",
        "        violating_cols = temp_ffilled_df.columns[(temp_ffilled_df <= 0).any()].tolist()\n",
        "        raise ValueError(f\"Data integrity failure: Non-positive prices (<= 0) detected in assets {violating_cols} \"\n",
        "                         f\"after temporary forward-fill.\")\n",
        "\n",
        "    # Return the value validation metrics for the audit report\n",
        "    return {\n",
        "        \"nan_counts_per_asset\": nan_counts,\n",
        "        \"nan_ratios_per_asset\": nan_ratios,\n",
        "        \"positivity_check_passed\": True,\n",
        "        \"warnings\": warnings_logged\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_raw_price_df(raw_price_df: pd.DataFrame, config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the comprehensive validation of the raw time-series price matrix.\n",
        "\n",
        "    This function acts as a strict gatekeeper for the data engineering phase. It sequentially\n",
        "    validates the temporal index integrity, the column schema and universe alignment, and the\n",
        "    numerical value integrity of the ingested price data. It returns a consolidated audit report\n",
        "    if all checks pass, or raises a fatal exception if any structural or numerical invariant is violated.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw time-series price matrix ingested from the data provider.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive validation report aggregating metrics from all checks.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the index or columns possess incorrect data types or structures.\n",
        "        ValueError: If chronological, schema, or numerical invariants are violated.\n",
        "    \"\"\"\n",
        "    logger.info(\"Commencing raw price DataFrame validation suite.\")\n",
        "\n",
        "    # Initialize the consolidated validation report\n",
        "    validation_report: Dict[str, Any] = {}\n",
        "\n",
        "    # Execute Step 1: Validate temporal index integrity\n",
        "    index_report = _validate_index_integrity(raw_price_df)\n",
        "    validation_report[\"index_metrics\"] = index_report\n",
        "    logger.info(\"Index integrity validation passed.\")\n",
        "\n",
        "    # Extract the canonical universe from the configuration for Step 2\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "\n",
        "    # Execute Step 2: Validate column schema and universe alignment\n",
        "    column_report = _validate_column_integrity(raw_price_df, universe)\n",
        "    validation_report[\"column_metrics\"] = column_report\n",
        "    logger.info(\"Column schema and universe alignment validation passed.\")\n",
        "\n",
        "    # Execute Step 3: Validate numerical value integrity and quantify missingness\n",
        "    value_report = _validate_value_integrity(raw_price_df)\n",
        "    validation_report[\"value_metrics\"] = value_report\n",
        "    logger.info(\"Numerical value integrity validation passed.\")\n",
        "\n",
        "    logger.info(\"Raw price DataFrame validation suite completed successfully.\")\n",
        "\n",
        "    # Return the consolidated audit artifact\n",
        "    return validation_report\n"
      ],
      "metadata": {
        "id": "yfvGAegNb59k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Cleanse and normalize raw_price_df (cleanse_price_data)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Cleanse and normalize raw_price_df\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Normalize column structure: canonical ticker order, deterministic MultiIndex flattening, and float64 casting.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _normalize_column_structure(raw_price_df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalizes the column structure of the raw price DataFrame using dynamic resolution.\n",
        "\n",
        "    This function creates a copy of the input data, dynamically detects and flattens a\n",
        "    MultiIndex by searching all levels for the configured price field, reorders the\n",
        "    columns to exactly match the canonical universe list, and casts the entire matrix\n",
        "    to numpy.float64.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw time-series price matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A normalized DataFrame with a single-level column index.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the MultiIndex cannot be resolved or if canonical tickers are missing.\n",
        "        TypeError: If the data cannot be cast to float64.\n",
        "    \"\"\"\n",
        "    # Create a deep copy to prevent mutating the original raw data artifact\n",
        "    normalized_df = raw_price_df.copy(deep=True)\n",
        "\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    price_field = config[\"data_conventions\"][\"price_field\"]\n",
        "\n",
        "    # Dynamically resolve and flatten a MultiIndex if present\n",
        "    if isinstance(normalized_df.columns, pd.MultiIndex):\n",
        "        logger.info(\"MultiIndex detected in columns. Attempting dynamic flattening.\")\n",
        "        field_found = False\n",
        "\n",
        "        # Iterate through all available levels to locate the target price field\n",
        "        for level_idx in range(normalized_df.columns.nlevels):\n",
        "            if price_field in normalized_df.columns.get_level_values(level_idx):\n",
        "                # Extract the cross-section corresponding to the target field\n",
        "                normalized_df = normalized_df.xs(price_field, axis=1, level=level_idx)\n",
        "                field_found = True\n",
        "                logger.debug(f\"Successfully flattened MultiIndex using level {level_idx}.\")\n",
        "                break\n",
        "\n",
        "        # Raise a fatal error if the dynamic search fails\n",
        "        if not field_found:\n",
        "            raise ValueError(f\"Cannot flatten MultiIndex: price_field '{price_field}' not found in any level.\")\n",
        "\n",
        "    try:\n",
        "        # Reorder the columns to exactly match the canonical universe list\n",
        "        normalized_df = normalized_df[universe]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Failed to reorder columns to match canonical universe. Missing tickers: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Cast the entire DataFrame to 64-bit floating point numbers for high precision\n",
        "        normalized_df = normalized_df.astype(np.float64)\n",
        "    except ValueError as e:\n",
        "        raise TypeError(f\"Failed to cast normalized DataFrame to float64. Non-numeric data present: {e}\")\n",
        "\n",
        "    return normalized_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Apply the configured missing-data policy: forward-fill then dropna; log row impacts.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_missing_data_policy(normalized_df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the deterministic missing-data handling policy to the normalized DataFrame.\n",
        "\n",
        "    This function enforces the 'ffill_then_dropna' policy, propagating the last known\n",
        "    prices forward along the time axis to preserve causality, and subsequently dropping\n",
        "    any rows that still contain missing values (typically leading NaNs).\n",
        "\n",
        "    Args:\n",
        "        normalized_df (pd.DataFrame): The structurally normalized price matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleansed DataFrame with no missing values.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unrecognized missing-data handling policy is configured.\n",
        "    \"\"\"\n",
        "    # Extract the pinned missing data handling policy from the configuration\n",
        "    policy = config[\"data_conventions\"][\"missing_data_handling\"]\n",
        "\n",
        "    # Assert that the policy exactly matches the required manuscript convention\n",
        "    if policy != \"ffill_then_dropna\":\n",
        "        raise ValueError(f\"Unrecognized missing_data_handling policy: '{policy}'. Expected 'ffill_then_dropna'.\")\n",
        "\n",
        "    # Record the initial number of rows for the audit trail\n",
        "    initial_row_count = len(normalized_df)\n",
        "\n",
        "    # Apply forward-fill along the time axis (axis=0) to propagate last known prices\n",
        "    # This is a causal operation that does not introduce look-ahead bias\n",
        "    cleansed_df = normalized_df.ffill(axis=0)\n",
        "\n",
        "    # Record the number of rows after forward-filling (should be identical to initial)\n",
        "    post_ffill_row_count = len(cleansed_df)\n",
        "\n",
        "    # Apply dropna along the time axis (axis=0) to remove rows with any remaining NaNs\n",
        "    # This typically removes leading rows before an asset's first trading day\n",
        "    cleansed_df = cleansed_df.dropna(axis=0, how='any')\n",
        "\n",
        "    # Record the final number of rows after dropping NaNs\n",
        "    final_row_count = len(cleansed_df)\n",
        "    # Calculate the exact number of rows removed during the dropna phase\n",
        "    rows_removed = post_ffill_row_count - final_row_count\n",
        "\n",
        "    # Log the row impacts for institutional auditability and reproducibility\n",
        "    logger.info(f\"Missing data policy '{policy}' applied.\")\n",
        "    logger.info(f\"Initial rows: {initial_row_count} | Post-ffill rows: {post_ffill_row_count} | \"\n",
        "                f\"Final rows: {final_row_count} | Rows removed: {rows_removed}\")\n",
        "\n",
        "    # Return the cleansed DataFrame\n",
        "    return cleansed_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Post-clean integrity re-validation: no NaN, strictly positive, sufficient rows for backtest.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _post_clean_integrity_validation(cleansed_df: pd.DataFrame, config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Performs a final, rigorous integrity validation on the cleansed DataFrame.\n",
        "\n",
        "    This function acts as a belt-and-suspenders check, mathematically asserting that\n",
        "    the matrix contains zero NaNs, strictly positive prices, a monotonically increasing\n",
        "    index, and a sufficient chronological horizon to execute the full backtest.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame): The cleansed and normalized price matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any post-cleansing integrity invariant is violated.\n",
        "    \"\"\"\n",
        "    # Assert that the sum of all NaN values across the entire matrix is exactly zero\n",
        "    if cleansed_df.isna().sum().sum() != 0:\n",
        "        raise ValueError(\"Post-clean validation failed: NaN values remain in the DataFrame.\")\n",
        "\n",
        "    # Assert that every single price in the matrix is strictly greater than zero\n",
        "    # This prevents undefined logarithms during return computations\n",
        "    if not (cleansed_df > 0).all().all():\n",
        "        raise ValueError(\"Post-clean validation failed: Non-positive prices (<= 0) detected in the DataFrame.\")\n",
        "\n",
        "    # Assert that the chronological index remains strictly monotonically increasing\n",
        "    if not cleansed_df.index.is_monotonic_increasing:\n",
        "        raise ValueError(\"Post-clean validation failed: Index is no longer monotonically increasing.\")\n",
        "\n",
        "    # Assert that the index contains no duplicate timestamps\n",
        "    if cleansed_df.index.has_duplicates:\n",
        "        raise ValueError(\"Post-clean validation failed: Index contains duplicate timestamps.\")\n",
        "\n",
        "    # Extract the lookback window length from the configuration\n",
        "    lookback_l = config[\"global_setup\"][\"lookback_window_L\"]\n",
        "    # Calculate the absolute minimum required rows: lookback window + 12 months of backtesting\n",
        "    min_required_rows = lookback_l + 12\n",
        "\n",
        "    # Assert that the cleansed DataFrame contains sufficient rows to complete the study\n",
        "    if len(cleansed_df) < min_required_rows:\n",
        "        raise ValueError(f\"Post-clean validation failed: Insufficient rows. \"\n",
        "                         f\"Have {len(cleansed_df)}, require at least {min_required_rows} (L={lookback_l} + 12).\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_price_data(raw_price_df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing and normalization of the raw time-series price matrix.\n",
        "\n",
        "    This function executes the data engineering pipeline required to transform raw API\n",
        "    data into a canonical, mathematically sound artifact. It normalizes the column\n",
        "    structure, applies deterministic missing-data handling, and rigorously re-validates\n",
        "    the integrity of the final matrix before releasing it to downstream econometric\n",
        "    and quantum optimization modules.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw time-series price matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The canonical, cleansed, and normalized price matrix.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If structural, policy, or numerical invariants are violated.\n",
        "        TypeError: If data casting fails.\n",
        "    \"\"\"\n",
        "    logger.info(\"Commencing price data cleansing and normalization pipeline.\")\n",
        "\n",
        "    # Execute Step 1: Normalize column structure, flatten MultiIndex, and cast to float64\n",
        "    normalized_df = _normalize_column_structure(raw_price_df, config)\n",
        "    logger.info(\"Column structure normalized and cast to float64.\")\n",
        "\n",
        "    # Execute Step 2: Apply the deterministic missing-data handling policy\n",
        "    cleansed_df = _apply_missing_data_policy(normalized_df, config)\n",
        "\n",
        "    # Execute Step 3: Perform rigorous post-cleansing integrity validation\n",
        "    _post_clean_integrity_validation(cleansed_df, config)\n",
        "    logger.info(\"Post-cleansing integrity validation passed.\")\n",
        "\n",
        "    logger.info(\"Price data cleansing and normalization pipeline completed successfully.\")\n",
        "\n",
        "    # Return the canonical data artifact for the study\n",
        "    return cleansed_df\n"
      ],
      "metadata": {
        "id": "wADtswr8c3uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Construct deterministic monthly rebalance calendar for 2025 (build_rebalance_calendar)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Construct deterministic monthly rebalance calendar for 2025\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Generate 12 candidate month anchors (rule-based) or parse override list.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _align_timestamp_timezone(ts: pd.Timestamp, target_tz: Optional[datetime.tzinfo]) -> pd.Timestamp:\n",
        "    \"\"\"\n",
        "    Aligns a pandas Timestamp to a target timezone deterministically.\n",
        "\n",
        "    This helper function abstracts the verbose boilerplate required to safely compare\n",
        "    or slice DatetimeIndices that may be timezone-naive or timezone-aware.\n",
        "\n",
        "    Args:\n",
        "        ts (pd.Timestamp): The timestamp to align.\n",
        "        target_tz (Optional[datetime.tzinfo]): The target timezone (None if naive).\n",
        "\n",
        "    Returns:\n",
        "        pd.Timestamp: The aligned timestamp.\n",
        "    \"\"\"\n",
        "    if target_tz is not None:\n",
        "        # Target is aware\n",
        "        if ts.tz is None:\n",
        "            return ts.tz_localize(target_tz)\n",
        "        else:\n",
        "            return ts.tz_convert(target_tz)\n",
        "    else:\n",
        "        # Target is naive\n",
        "        if ts.tz is not None:\n",
        "            return ts.tz_localize(None)\n",
        "        else:\n",
        "            return ts\n",
        "\n",
        "\n",
        "def _generate_candidate_anchors(config: Dict[str, Any], target_tz: Optional[datetime.tzinfo]) -> List[pd.Timestamp]:\n",
        "    \"\"\"\n",
        "    Generates exactly 12 candidate month anchors for the backtest year.\n",
        "\n",
        "    This function enforces the precedence rule: if an explicit override list is provided,\n",
        "    it is parsed and used. Otherwise, it generates theoretical anchors for the first\n",
        "    calendar day of each month. It utilizes the dedicated helper function to ensure\n",
        "    clean, DRY timezone alignment.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        target_tz (Optional[datetime.tzinfo]): The timezone of the empirical price index.\n",
        "\n",
        "    Returns:\n",
        "        List[pd.Timestamp]: A list of exactly 12 chronological candidate anchors.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the override list is invalid, or if an unrecognized rule is specified.\n",
        "    \"\"\"\n",
        "    rebalance_calendar_cfg = config[\"rebalance_calendar\"]\n",
        "    override_dates = rebalance_calendar_cfg.get(\"rebalance_dates_override\")\n",
        "    backtest_year = config[\"global_setup\"][\"backtest_year\"]\n",
        "\n",
        "    anchors: List[pd.Timestamp] = []\n",
        "\n",
        "    # Precedence 1: Explicit override list\n",
        "    if override_dates is not None and len(override_dates) > 0:\n",
        "        logger.info(\"Rebalance schedule precedence: Using explicit 'rebalance_dates_override'.\")\n",
        "        if len(override_dates) != 12:\n",
        "            raise ValueError(f\"Override list must contain exactly 12 dates, got {len(override_dates)}.\")\n",
        "        try:\n",
        "            for date_str in override_dates:\n",
        "                ts = pd.to_datetime(date_str)\n",
        "                anchors.append(ts)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to parse override dates as ISO strings: {e}\")\n",
        "\n",
        "    # Precedence 2: Rule-based generation\n",
        "    else:\n",
        "        rule = rebalance_calendar_cfg.get(\"rebalance_rule\")\n",
        "        logger.info(f\"Rebalance schedule precedence: Using rule '{rule}'.\")\n",
        "        if rule != \"first_trading_day_of_month\":\n",
        "            raise ValueError(f\"Unrecognized rebalance_rule: '{rule}'. Expected 'first_trading_day_of_month'.\")\n",
        "\n",
        "        for month in range(1, 13):\n",
        "            ts = pd.Timestamp(year=backtest_year, month=month, day=1)\n",
        "            anchors.append(ts)\n",
        "\n",
        "    # Ensure timezone consistency using the abstracted helper function\n",
        "    aligned_anchors = [_align_timestamp_timezone(ts, target_tz) for ts in anchors]\n",
        "\n",
        "    return aligned_anchors\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Map each anchor to an actual trading day present in cleaned_price_df.index.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _map_anchors_to_trading_days(anchors: List[pd.Timestamp], price_index: pd.DatetimeIndex) -> List[pd.Timestamp]:\n",
        "    \"\"\"\n",
        "    Maps theoretical calendar anchors to realized trading days in the empirical dataset.\n",
        "\n",
        "    For each anchor, this function finds the earliest timestamp in the price index that\n",
        "    is greater than or equal to the anchor. This operationalizes the 'first trading day'\n",
        "    logic while strictly adhering to the realized data availability.\n",
        "\n",
        "    Args:\n",
        "        anchors (List[pd.Timestamp]): The 12 theoretical candidate anchors.\n",
        "        price_index (pd.DatetimeIndex): The chronological index of the cleansed price matrix.\n",
        "\n",
        "    Returns:\n",
        "        List[pd.Timestamp]: A list of exactly 12 realized trading timestamps.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an anchor falls beyond the available data, or if mapped dates are not strictly increasing.\n",
        "    \"\"\"\n",
        "    mapped_dates: List[pd.Timestamp] = []\n",
        "\n",
        "    for i, anchor in enumerate(anchors):\n",
        "        # Create a boolean mask for all realized dates >= the theoretical anchor\n",
        "        valid_dates_mask = price_index >= anchor\n",
        "\n",
        "        # Extract the subset of the index that satisfies the condition\n",
        "        valid_dates = price_index[valid_dates_mask]\n",
        "\n",
        "        # Assert that at least one valid date exists\n",
        "        if len(valid_dates) == 0:\n",
        "            raise ValueError(f\"Data coverage error: No trading days found on or after anchor {anchor.date()}. \"\n",
        "                             f\"The dataset does not cover the full backtest horizon.\")\n",
        "\n",
        "        # The earliest date in the valid subset is the realized trading day\n",
        "        realized_trading_day = valid_dates[0]\n",
        "        mapped_dates.append(realized_trading_day)\n",
        "\n",
        "    # Assert that the resulting mapped dates are strictly monotonically increasing\n",
        "    # This prevents duplicate rebalance dates if the dataset has multi-month gaps\n",
        "    for i in range(1, len(mapped_dates)):\n",
        "        if mapped_dates[i] <= mapped_dates[i-1]:\n",
        "            raise ValueError(f\"Mapping error: Mapped rebalance dates are not strictly increasing. \"\n",
        "                             f\"Conflict between {mapped_dates[i-1].date()} and {mapped_dates[i].date()}.\")\n",
        "\n",
        "    return mapped_dates\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Validate causality feasibility for every rebalance date and freeze schedule.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_causality_and_freeze(mapped_dates: List[pd.Timestamp], price_index: pd.DatetimeIndex, lookback_l: int) -> Tuple[pd.Timestamp, ...]:\n",
        "    \"\"\"\n",
        "    Validates the causal feasibility of the rebalance schedule and freezes it.\n",
        "\n",
        "    This function mathematically asserts that for every mapped rebalance date, there\n",
        "    exist at least L strictly prior trading days in the dataset. This guarantees that\n",
        "    the rolling estimation windows can be constructed without look-ahead bias.\n",
        "\n",
        "    Args:\n",
        "        mapped_dates (List[pd.Timestamp]): The 12 realized trading timestamps.\n",
        "        price_index (pd.DatetimeIndex): The chronological index of the cleansed price matrix.\n",
        "        lookback_l (int): The required number of historical trading days (L=180).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Timestamp, ...]: An immutable tuple of the 12 validated rebalance dates.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any rebalance date lacks sufficient historical data.\n",
        "    \"\"\"\n",
        "    for i, t in enumerate(mapped_dates):\n",
        "        # Create a boolean mask for strict inequality: tau < t\n",
        "        # This strictly prevents look-ahead bias by excluding the rebalance date itself\n",
        "        prior_days_mask = price_index < t\n",
        "\n",
        "        # Count the number of available historical trading days\n",
        "        available_history = int(prior_days_mask.sum())\n",
        "\n",
        "        # Assert that the available history meets or exceeds the lookback requirement\n",
        "        if available_history < lookback_l:\n",
        "            raise ValueError(f\"Causality violation at rebalance date {t.date()} (Month {i+1}). \"\n",
        "                             f\"Requires L={lookback_l} prior trading days, but only {available_history} are available.\")\n",
        "\n",
        "        logger.debug(f\"Rebalance {i+1} ({t.date()}): Causality verified. Available pre-history: {available_history} days.\")\n",
        "\n",
        "    # Convert the validated list to an immutable tuple to freeze the schedule\n",
        "    frozen_schedule = tuple(mapped_dates)\n",
        "    return frozen_schedule\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_rebalance_calendar(cleaned_price_df: pd.DataFrame, config: Dict[str, Any]) -> Tuple[pd.Timestamp, ...]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of a deterministic, causal monthly rebalance calendar.\n",
        "\n",
        "    This function generates theoretical calendar anchors based on the study configuration,\n",
        "    maps them to realized trading days within the empirical price dataset, and rigorously\n",
        "    validates that each resulting date possesses sufficient historical data to support\n",
        "    a causal lookback window of length L. It returns an immutable schedule.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical, cleansed price matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Timestamp, ...]: An immutable tuple containing exactly 12 validated rebalance timestamps.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the schedule cannot be generated, mapped, or causally validated.\n",
        "    \"\"\"\n",
        "    logger.info(\"Commencing deterministic rebalance calendar construction.\")\n",
        "\n",
        "    # Extract the timezone of the empirical data to ensure consistency\n",
        "    target_tz = cleaned_price_df.index.tz\n",
        "\n",
        "    # Execute Step 1: Generate theoretical candidate anchors\n",
        "    anchors = _generate_candidate_anchors(config, target_tz)\n",
        "    logger.info(f\"Generated {len(anchors)} theoretical candidate anchors.\")\n",
        "\n",
        "    # Execute Step 2: Map anchors to realized trading days\n",
        "    mapped_dates = _map_anchors_to_trading_days(anchors, cleaned_price_df.index)\n",
        "    logger.info(\"Successfully mapped anchors to realized empirical trading days.\")\n",
        "\n",
        "    # Extract the lookback window length L from the configuration\n",
        "    lookback_l = config[\"global_setup\"][\"lookback_window_L\"]\n",
        "\n",
        "    # Execute Step 3: Validate causality and freeze the schedule\n",
        "    frozen_schedule = _validate_causality_and_freeze(mapped_dates, cleaned_price_df.index, lookback_l)\n",
        "\n",
        "    # Log the final, authoritative schedule for institutional auditability\n",
        "    logger.info(\"Rebalance calendar causality validated and frozen.\")\n",
        "    for i, date in enumerate(frozen_schedule):\n",
        "        logger.info(f\"  Rebalance {i+1:02d}: {date.date()}\")\n",
        "\n",
        "    return frozen_schedule\n"
      ],
      "metadata": {
        "id": "s75ZuwJuNMAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Build causal rolling-window extraction callable (extract_lookback_window)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Build causal rolling-window extraction callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Implement strict causal slicing and take the last L rows prior to t.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _slice_causal_history(cleaned_price_df: pd.DataFrame, t: pd.Timestamp, L: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts a strictly causal historical window of length L prior to timestamp t.\n",
        "\n",
        "    This function enforces the interval (-infinity, t) by using a strict inequality\n",
        "    mask on the DatetimeIndex, mathematically guaranteeing the absence of look-ahead\n",
        "    bias. It then extracts the last L trading days to form the [t-L, t) window.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical, cleansed price matrix.\n",
        "        t (pd.Timestamp): The current rebalance timestamp.\n",
        "        L (int): The required lookback window length in trading days.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A deep copy of the causally sliced historical window.\n",
        "    \"\"\"\n",
        "    # Ensure timezone consistency between the rebalance timestamp and the DataFrame index\n",
        "    target_tz = cleaned_price_df.index.tz\n",
        "    if target_tz is not None:\n",
        "        if t.tz is None:\n",
        "            # Localize naive timestamp to match aware index\n",
        "            t_aligned = t.tz_localize(target_tz)\n",
        "        else:\n",
        "            # Convert aware timestamp to match index timezone\n",
        "            t_aligned = t.tz_convert(target_tz)\n",
        "    else:\n",
        "        if t.tz is not None:\n",
        "            # Strip timezone from aware timestamp to match naive index\n",
        "            t_aligned = t.tz_localize(None)\n",
        "        else:\n",
        "            t_aligned = t\n",
        "\n",
        "    # Create a boolean mask for strict inequality: tau < t\n",
        "    # This is the critical operation that prevents look-ahead bias\n",
        "    causal_mask = cleaned_price_df.index < t_aligned\n",
        "\n",
        "    # Apply the mask to filter out all data at or after the rebalance date\n",
        "    causal_history = cleaned_price_df.loc[causal_mask]\n",
        "\n",
        "    # Extract exactly the last L rows from the causal history\n",
        "    # We use .copy(deep=True) to ensure the returned window is an independent artifact\n",
        "    window_df = causal_history.tail(L).copy(deep=True)\n",
        "\n",
        "    if not window_df.empty:\n",
        "        window_start = window_df.index.min().date()\n",
        "        window_end = window_df.index.max().date()\n",
        "        logger.debug(f\"Sliced causal window for rebalance {t_aligned.date()}: [{window_start}, {window_end}].\")\n",
        "    else:\n",
        "        logger.debug(f\"Sliced causal window for rebalance {t_aligned.date()} is empty.\")\n",
        "\n",
        "    return window_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Enforce the minimum observation requirement (must be exactly L rows).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _enforce_window_length(window_df: pd.DataFrame, t: pd.Timestamp, L: int) -> None:\n",
        "    \"\"\"\n",
        "    Mathematically asserts that the extracted window contains exactly L trading days.\n",
        "\n",
        "    This function prevents the econometric estimators from processing undersized\n",
        "    samples, which would artificially inflate variance and violate the study's methodology.\n",
        "\n",
        "    Args:\n",
        "        window_df (pd.DataFrame): The causally sliced historical window.\n",
        "        t (pd.Timestamp): The current rebalance timestamp (for error reporting).\n",
        "        L (int): The required lookback window length.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of rows in the window is not exactly equal to L.\n",
        "    \"\"\"\n",
        "    # Calculate the actual number of rows in the extracted window\n",
        "    actual_rows = len(window_df)\n",
        "\n",
        "    # Assert exact equality with the required lookback length\n",
        "    if actual_rows != L:\n",
        "        shortfall = L - actual_rows\n",
        "        raise ValueError(f\"Insufficient historical data for rebalance date {t.date()}. \"\n",
        "                         f\"Expected exactly L={L} rows, but observed {actual_rows} rows \"\n",
        "                         f\"(shortfall of {shortfall} trading days).\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Return the window DataFrame with guaranteed shape and integrity.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_window_integrity(window_df: pd.DataFrame, universe: List[str]) -> None:\n",
        "    \"\"\"\n",
        "    Performs a final integrity validation on the extracted window DataFrame.\n",
        "\n",
        "    This function acts as a belt-and-suspenders check, asserting that the window\n",
        "    maintains the canonical column ordering, contains zero NaNs, and consists\n",
        "    entirely of strictly positive prices.\n",
        "\n",
        "    Args:\n",
        "        window_df (pd.DataFrame): The causally sliced and length-validated window.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If column ordering, NaN, or positivity invariants are violated.\n",
        "    \"\"\"\n",
        "    # Assert that the columns exactly match the canonical universe ordering\n",
        "    # This is critical for mapping matrix indices to specific assets downstream\n",
        "    if window_df.columns.tolist() != universe:\n",
        "        raise ValueError(\"Window integrity violation: Column ordering does not match the canonical universe.\")\n",
        "\n",
        "    # Assert that the window contains exactly zero NaN values\n",
        "    if window_df.isna().sum().sum() != 0:\n",
        "        raise ValueError(\"Window integrity violation: NaN values detected in the extracted window.\")\n",
        "\n",
        "    # Assert that all prices in the window are strictly positive\n",
        "    # This guarantees that subsequent logarithmic return calculations are mathematically defined\n",
        "    if not (window_df > 0).all().all():\n",
        "        raise ValueError(\"Window integrity violation: Non-positive prices (<= 0) detected in the extracted window.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_lookback_window(cleaned_price_df: pd.DataFrame, t: pd.Timestamp, L: int, universe: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction of a strictly causal, validated rolling estimation window.\n",
        "\n",
        "    This function isolates the historical price data required for a specific rebalance\n",
        "    period. It mathematically guarantees the absence of look-ahead bias by strictly\n",
        "    excluding data at or after timestamp t. It enforces the exact sample size requirement\n",
        "    (L) and re-validates the structural and numerical integrity of the resulting matrix.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical, cleansed price matrix.\n",
        "        t (pd.Timestamp): The current rebalance timestamp.\n",
        "        L (int): The required lookback window length in trading days (e.g., 180).\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A deep copy of the validated, causal historical window of shape (L, N).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the window cannot be extracted, lacks sufficient data, or fails integrity checks.\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Extracting causal lookback window for rebalance date: {t.date()}\")\n",
        "\n",
        "    # Execute Step 1: Slice the causal history and extract the last L rows\n",
        "    window_df = _slice_causal_history(cleaned_price_df, t, L)\n",
        "\n",
        "    # Execute Step 2: Enforce the exact minimum observation requirement\n",
        "    _enforce_window_length(window_df, t, L)\n",
        "\n",
        "    # Execute Step 3: Re-validate the structural and numerical integrity of the window\n",
        "    _validate_window_integrity(window_df, universe)\n",
        "\n",
        "    logger.debug(f\"Successfully extracted and validated window of shape {window_df.shape}.\")\n",
        "\n",
        "    # Return the canonical estimation window artifact for the current month\n",
        "    return window_df\n"
      ],
      "metadata": {
        "id": "awu14z9QcdjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Build daily log-return computation callable (compute_daily_log_returns)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Build daily log-return computation callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Compute element-wise log returns consistent with the pinned return convention.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_raw_log_returns(window_df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the element-wise logarithmic returns for the provided price window.\n",
        "\n",
        "    This function enforces the pinned 'log' return convention. It calculates the\n",
        "    natural logarithm of the ratio between the current day's price and the previous\n",
        "    day's price for all assets simultaneously using vectorized operations.\n",
        "\n",
        "    Args:\n",
        "        window_df (pd.DataFrame): The causally sliced, strictly positive price matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the raw log returns (first row will be NaN).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the configuration does not specify 'log' returns, or if non-positive prices exist.\n",
        "    \"\"\"\n",
        "    # Extract the pinned return type convention\n",
        "    return_type = config[\"return_conventions\"][\"return_type\"]\n",
        "\n",
        "    # Assert strict adherence to the logarithmic return requirement\n",
        "    if return_type != \"log\":\n",
        "        raise ValueError(f\"Methodological violation: Expected return_type 'log', but config specifies '{return_type}'.\")\n",
        "\n",
        "    # Defensive check: Ensure all prices are strictly positive to prevent undefined logarithms\n",
        "    if not (window_df > 0).all().all():\n",
        "        raise ValueError(\"Mathematical violation: Non-positive prices detected. Logarithm is undefined.\")\n",
        "\n",
        "    # Compute logarithmic returns: R_{tau, i} = ln(P_{tau, i} / P_{tau-1, i})\n",
        "    # The .shift(1) operation shifts the data down by one row along the DatetimeIndex\n",
        "    # np.log applies the natural logarithm element-wise to the resulting ratio matrix\n",
        "    raw_returns_df = np.log(window_df / window_df.shift(1))\n",
        "\n",
        "    return raw_returns_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 2: Remove the first row (NaN from shifting) and assert shape and NaN-free output.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _remove_first_row_and_validate(raw_returns_df: pd.DataFrame, expected_L: int, expected_N: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Truncates the shift-induced NaN row and mathematically asserts matrix dimensions.\n",
        "\n",
        "    This function removes the first chronological observation (which lacks a preceding\n",
        "    denominator for the return calculation) and rigorously verifies that the resulting\n",
        "    matrix matches the expected (L-1, N) shape and contains absolutely no missing values.\n",
        "\n",
        "    Args:\n",
        "        raw_returns_df (pd.DataFrame): The raw log returns matrix containing a leading NaN row.\n",
        "        expected_L (int): The expected lookback window length (e.g., 180).\n",
        "        expected_N (int): The expected number of assets in the universe (e.g., 10).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The truncated, NaN-free returns matrix.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the resulting shape is incorrect or if unexpected NaNs are present.\n",
        "    \"\"\"\n",
        "    # Deterministically remove the first row using integer-location based indexing\n",
        "    # This drops index 0 and retains indices 1 through L-1\n",
        "    truncated_returns_df = raw_returns_df.iloc[1:].copy(deep=True)\n",
        "\n",
        "    # Calculate the expected number of rows after truncation\n",
        "    expected_rows = expected_L - 1\n",
        "\n",
        "    # Assert the exact shape of the resulting matrix\n",
        "    actual_shape = truncated_returns_df.shape\n",
        "    expected_shape = (expected_rows, expected_N)\n",
        "\n",
        "    if actual_shape != expected_shape:\n",
        "        raise ValueError(f\"Dimensionality violation: Expected returns matrix shape {expected_shape}, \"\n",
        "                         f\"but observed {actual_shape}.\")\n",
        "\n",
        "    # Assert that exactly zero NaNs remain in the truncated matrix\n",
        "    # Any remaining NaNs indicate missing data that bypassed the Phase 1 cleansing\n",
        "    if truncated_returns_df.isna().sum().sum() != 0:\n",
        "        raise ValueError(\"Data integrity violation: Unexpected NaN values detected in the returns matrix \"\n",
        "                         \"after removing the initial shift-induced row.\")\n",
        "\n",
        "    return truncated_returns_df\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Return the daily returns DataFrame with canonical ordering and numeric integrity.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_returns_integrity_and_format(returns_df: pd.DataFrame, universe: List[str]) -> None:\n",
        "    \"\"\"\n",
        "    Performs a final integrity validation on the truncated returns DataFrame.\n",
        "\n",
        "    This function acts as a strict gatekeeper, ensuring the columns perfectly align\n",
        "    with the canonical universe, the data type is float64, and all computed returns\n",
        "    are finite (no infinity or negative infinity).\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The truncated, NaN-free returns matrix.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If column ordering is corrupted or if infinite values are detected.\n",
        "        TypeError: If the matrix is not composed of float64 data.\n",
        "    \"\"\"\n",
        "    # Assert that the columns exactly match the canonical universe ordering\n",
        "    if returns_df.columns.tolist() != universe:\n",
        "        raise ValueError(\"Structural violation: Returns matrix column ordering does not match the canonical universe.\")\n",
        "\n",
        "    # Assert that the data type is float64 to guarantee precision for covariance shrinkage\n",
        "    if not all(returns_df.dtypes == np.float64):\n",
        "        raise TypeError(\"Type violation: Returns matrix must be strictly float64.\")\n",
        "\n",
        "    # Assert that all values are finite (no inf or -inf from extreme price movements)\n",
        "    if not np.isfinite(returns_df.values).all():\n",
        "        raise ValueError(\"Mathematical violation: Infinite values (inf or -inf) detected in the returns matrix.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_daily_log_returns(window_df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of the daily logarithmic returns matrix.\n",
        "\n",
        "    This function transforms a causally sliced price window into a mathematically\n",
        "    pristine returns matrix. It enforces the logarithmic return convention, handles\n",
        "    the shift-induced missing data deterministically, and rigorously validates the\n",
        "    shape, ordering, and numerical finiteness of the resulting artifact before\n",
        "    releasing it to the econometric estimation modules.\n",
        "\n",
        "    Args:\n",
        "        window_df (pd.DataFrame): The causally sliced, strictly positive price matrix of shape (L, N).\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The canonical daily log-returns matrix of shape (L-1, N).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If mathematical, structural, or configuration invariants are violated.\n",
        "        TypeError: If data type constraints are violated.\n",
        "    \"\"\"\n",
        "    logger.debug(\"Commencing daily log-return computation.\")\n",
        "\n",
        "    # Extract expected dimensions from the configuration\n",
        "    expected_L = config[\"global_setup\"][\"lookback_window_L\"]\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    expected_N = len(universe)\n",
        "\n",
        "    # Execute Step 1: Compute element-wise log returns\n",
        "    raw_returns_df = _compute_raw_log_returns(window_df, config)\n",
        "\n",
        "    # Execute Step 2: Remove the first row and assert shape/NaN-free output\n",
        "    truncated_returns_df = _remove_first_row_and_validate(raw_returns_df, expected_L, expected_N)\n",
        "\n",
        "    # Execute Step 3: Validate canonical ordering and numeric integrity\n",
        "    _validate_returns_integrity_and_format(truncated_returns_df, universe)\n",
        "\n",
        "    logger.debug(f\"Successfully computed log-returns matrix of shape {truncated_returns_df.shape}.\")\n",
        "\n",
        "    # Return the canonical returns artifact for the current month\n",
        "    return truncated_returns_df\n"
      ],
      "metadata": {
        "id": "PWJ0r46QdOCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Build annualized expected return ((\\mu)) estimation callable (estimate_mu_annualized)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Build annualized expected return (mu) estimation callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Compute the daily sample mean vector from the daily log-return matrix.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_daily_sample_mean(returns_df: pd.DataFrame, universe: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the daily sample mean return vector from the log-returns matrix.\n",
        "\n",
        "    This function calculates the arithmetic mean of the daily log-returns for each\n",
        "    asset across the chronological index (T=179). It enforces the canonical universe\n",
        "    ordering and extracts the result as a high-performance numpy array.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D array containing the daily expected returns for each asset.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input matrix contains NaNs or if column ordering fails.\n",
        "    \"\"\"\n",
        "    # Defensive check: Ensure the input matrix is mathematically pristine\n",
        "    if returns_df.isna().sum().sum() != 0:\n",
        "        raise ValueError(\"Econometric violation: Cannot compute mean on a matrix containing NaN values.\")\n",
        "\n",
        "    try:\n",
        "        # Enforce canonical ordering before computation to guarantee index alignment\n",
        "        aligned_returns = returns_df[universe]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Structural violation: Returns matrix is missing canonical tickers: {e}\")\n",
        "\n",
        "    # Compute the arithmetic mean along the time axis (axis=0)\n",
        "    # Equation: mu_i^(daily) = (1/T) * sum(R_{tau, i})\n",
        "    daily_mean_series = aligned_returns.mean(axis=0)\n",
        "\n",
        "    # Extract the underlying values as a float64 numpy array\n",
        "    # This strips the pandas index overhead, preparing the vector for matrix algebra\n",
        "    mu_daily = daily_mean_series.values.astype(np.float64)\n",
        "\n",
        "    return mu_daily\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 2: Annualize the mean vector using the configured annualization factor.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _annualize_mean_vector(mu_daily: np.ndarray, config: Dict[str, Any]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Annualizes the daily expected return vector using linear scaling.\n",
        "\n",
        "    This function applies the pinned annualization factor (typically 252) to the\n",
        "    daily mean vector. It strictly adheres to the linear scaling convention for\n",
        "    logarithmic returns, avoiding geometric compounding.\n",
        "\n",
        "    Args:\n",
        "        mu_daily (np.ndarray): The 1D array of daily expected returns.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D array containing the annualized expected returns.\n",
        "    \"\"\"\n",
        "    # Extract the pinned annualization factor from the configuration\n",
        "    annualization_factor = config[\"global_setup\"][\"annualization_factor\"]\n",
        "\n",
        "    # Apply linear scaling: mu_i^(ann) = A * mu_i^(daily)\n",
        "    # This is a vectorized scalar-array multiplication\n",
        "    mu_annualized = mu_daily * annualization_factor\n",
        "\n",
        "    return mu_annualized\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Validate and return the annualized mean vector.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_mu_vector_integrity(mu_annualized: np.ndarray, expected_N: int) -> None:\n",
        "    \"\"\"\n",
        "    Performs a rigorous integrity validation on the annualized expected return vector.\n",
        "\n",
        "    This function mathematically asserts that the resulting vector possesses the\n",
        "    exact expected shape (N,), is composed strictly of float64 data, and contains\n",
        "    only finite real numbers.\n",
        "\n",
        "    Args:\n",
        "        mu_annualized (np.ndarray): The annualized expected return vector.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the shape is incorrect or if infinite/NaN values are detected.\n",
        "        TypeError: If the array is not of type float64.\n",
        "    \"\"\"\n",
        "    # Assert the exact shape of the vector\n",
        "    if mu_annualized.shape != (expected_N,):\n",
        "        raise ValueError(f\"Dimensionality violation: Expected mu vector shape ({expected_N},), \"\n",
        "                         f\"but observed {mu_annualized.shape}.\")\n",
        "\n",
        "    # Assert the data type is strictly float64\n",
        "    if mu_annualized.dtype != np.float64:\n",
        "        raise TypeError(f\"Type violation: mu vector must be strictly float64, got {mu_annualized.dtype}.\")\n",
        "\n",
        "    # Assert that all elements are finite real numbers\n",
        "    if not np.isfinite(mu_annualized).all():\n",
        "        raise ValueError(\"Mathematical violation: Infinite or NaN values detected in the annualized mu vector.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_mu_annualized(returns_df: pd.DataFrame, config: Dict[str, Any]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of the annualized expected return vector (mu).\n",
        "\n",
        "    This function computes the daily sample mean from the causally sliced log-returns\n",
        "    matrix, scales it to an annualized basis using the configured factor, and rigorously\n",
        "    validates the structural and numerical integrity of the resulting vector. The output\n",
        "    is the canonical mu vector used in both the quantum (QAOA) and classical (SA) solvers.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The validated, annualized expected return vector of shape (N,).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If mathematical, structural, or configuration invariants are violated.\n",
        "        TypeError: If data type constraints are violated.\n",
        "    \"\"\"\n",
        "    logger.debug(\"Commencing annualized expected return (mu) estimation.\")\n",
        "\n",
        "    # Extract the canonical universe to ensure perfect index alignment\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    expected_N = len(universe)\n",
        "\n",
        "    # Execute Step 1: Compute the daily sample mean vector\n",
        "    mu_daily = _compute_daily_sample_mean(returns_df, universe)\n",
        "\n",
        "    # Execute Step 2: Annualize the mean vector\n",
        "    mu_annualized = _annualize_mean_vector(mu_daily, config)\n",
        "\n",
        "    # Execute Step 3: Validate the structural and numerical integrity of the vector\n",
        "    _validate_mu_vector_integrity(mu_annualized, expected_N)\n",
        "\n",
        "    logger.debug(f\"Successfully estimated annualized mu vector of shape {mu_annualized.shape}.\")\n",
        "\n",
        "    # Return the canonical mu artifact for the current month\n",
        "    return mu_annualized\n"
      ],
      "metadata": {
        "id": "pIFtaZFF97wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Build annualized Ledoit–Wolf covariance ((\\Sigma)) estimation callable (estimate_sigma_annualized)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Build annualized Ledoit–Wolf covariance (Sigma) estimation callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Compute the daily Ledoit–Wolf shrinkage covariance matrix.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_daily_ledoit_wolf_covariance(returns_df: pd.DataFrame, universe: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the daily Ledoit-Wolf shrinkage covariance matrix from log-returns.\n",
        "\n",
        "    This function utilizes PyPortfolioOpt to shrink the sample covariance matrix\n",
        "    towards a structured target (constant correlation). This ensures the resulting\n",
        "    matrix is well-conditioned and positive-definite, even when N is close to T.\n",
        "    It explicitly operates on returns data to prevent methodological ambiguity.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D array representing the daily covariance matrix.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input matrix contains NaNs or if column ordering fails.\n",
        "    \"\"\"\n",
        "    # Defensive check: Ensure the input matrix is mathematically pristine\n",
        "    if returns_df.isna().sum().sum() != 0:\n",
        "        raise ValueError(\"Econometric violation: Cannot compute covariance on a matrix containing NaN values.\")\n",
        "\n",
        "    try:\n",
        "        # Enforce canonical ordering before computation to guarantee index alignment\n",
        "        aligned_returns = returns_df[universe]\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Structural violation: Returns matrix is missing canonical tickers: {e}\")\n",
        "\n",
        "    # Instantiate the CovarianceShrinkage object.\n",
        "    # We explicitly set returns_data=True to pin the convention and avoid ambiguity.\n",
        "    shrinkage_model = risk_models.CovarianceShrinkage(aligned_returns, returns_data=True)\n",
        "\n",
        "    # Compute the Ledoit-Wolf shrinkage estimator\n",
        "    # This returns a pandas DataFrame with the assets as both index and columns\n",
        "    daily_cov_df = shrinkage_model.ledoit_wolf()\n",
        "\n",
        "    # Extract the underlying values as a float64 numpy array\n",
        "    # This strips the pandas index overhead, preparing the matrix for downstream algebra\n",
        "    sigma_daily = daily_cov_df.values.astype(np.float64)\n",
        "\n",
        "    return sigma_daily\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 2: Annualize the covariance matrix.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _annualize_covariance_matrix(sigma_daily: np.ndarray, config: Dict[str, Any]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Annualizes the daily covariance matrix using linear scaling.\n",
        "\n",
        "    This function applies the pinned annualization factor (typically 252) to the\n",
        "    daily covariance matrix. It strictly adheres to the linear scaling property\n",
        "    of variance and covariance with respect to time.\n",
        "\n",
        "    Args:\n",
        "        sigma_daily (np.ndarray): The 2D array representing the daily covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D array representing the annualized covariance matrix.\n",
        "    \"\"\"\n",
        "    # Extract the pinned annualization factor from the configuration\n",
        "    annualization_factor = config[\"global_setup\"][\"annualization_factor\"]\n",
        "\n",
        "    # Apply linear scaling: Sigma_{ij}^(ann) = A * Sigma_{ij}^(daily)\n",
        "    # This is a vectorized scalar-matrix multiplication\n",
        "    sigma_annualized = sigma_daily * annualization_factor\n",
        "\n",
        "    return sigma_annualized\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Validate symmetry, positive definiteness (within tolerance), finiteness, and return Sigma^(ann).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_sigma_matrix_integrity(sigma_annualized: np.ndarray, expected_N: int) -> None:\n",
        "    \"\"\"\n",
        "    Performs a rigorous integrity validation on the annualized covariance matrix.\n",
        "\n",
        "    This function mathematically asserts that the resulting matrix possesses the\n",
        "    exact expected shape (N, N), is strictly symmetric, is positive-definite\n",
        "    (allowing for microscopic floating-point noise), and contains only finite numbers.\n",
        "\n",
        "    Args:\n",
        "        sigma_annualized (np.ndarray): The annualized covariance matrix.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If shape, symmetry, positive-definiteness, or finiteness invariants are violated.\n",
        "        TypeError: If the array is not of type float64.\n",
        "    \"\"\"\n",
        "    # Assert the exact shape of the matrix\n",
        "    if sigma_annualized.shape != (expected_N, expected_N):\n",
        "        raise ValueError(f\"Dimensionality violation: Expected Sigma matrix shape ({expected_N}, {expected_N}), \"\n",
        "                         f\"but observed {sigma_annualized.shape}.\")\n",
        "\n",
        "    # Assert the data type is strictly float64\n",
        "    if sigma_annualized.dtype != np.float64:\n",
        "        raise TypeError(f\"Type violation: Sigma matrix must be strictly float64, got {sigma_annualized.dtype}.\")\n",
        "\n",
        "    # Assert that all elements are finite real numbers\n",
        "    if not np.isfinite(sigma_annualized).all():\n",
        "        raise ValueError(\"Mathematical violation: Infinite or NaN values detected in the annualized Sigma matrix.\")\n",
        "\n",
        "    # Assert strict symmetry: ||Sigma - Sigma^T||_max < 10^-12\n",
        "    max_asymmetry = np.max(np.abs(sigma_annualized - sigma_annualized.T))\n",
        "    if max_asymmetry >= 1e-12:\n",
        "        raise ValueError(f\"Mathematical violation: Sigma matrix is not symmetric. Max asymmetry: {max_asymmetry}\")\n",
        "\n",
        "    # Assert positive definiteness by computing the eigenvalues\n",
        "    # We use eigvalsh because it is highly optimized for symmetric/Hermitian matrices\n",
        "    eigenvalues = np.linalg.eigvalsh(sigma_annualized)\n",
        "    min_eigenvalue = np.min(eigenvalues)\n",
        "\n",
        "    # We allow a microscopic negative tolerance (-10^-10) to account for floating-point noise\n",
        "    if min_eigenvalue <= -1e-10:\n",
        "        raise ValueError(f\"Mathematical violation: Sigma matrix is not positive-definite. \"\n",
        "                         f\"Minimum eigenvalue: {min_eigenvalue}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_sigma_annualized(returns_df: pd.DataFrame, config: Dict[str, Any]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of the annualized Ledoit-Wolf covariance matrix (Sigma).\n",
        "\n",
        "    This function computes the daily shrinkage covariance from the causally sliced\n",
        "    log-returns matrix, scales it to an annualized basis using the configured factor,\n",
        "    and rigorously validates the structural, symmetric, and positive-definite integrity\n",
        "    of the resulting matrix. The output is the canonical Sigma matrix used in both\n",
        "    the quantum (QAOA) and classical (SA) solvers.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The validated, annualized covariance matrix of shape (N, N).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If mathematical, structural, or configuration invariants are violated.\n",
        "        TypeError: If data type constraints are violated.\n",
        "    \"\"\"\n",
        "    logger.debug(\"Commencing annualized Ledoit-Wolf covariance (Sigma) estimation.\")\n",
        "\n",
        "    # Extract the canonical universe to ensure perfect index alignment\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    expected_N = len(universe)\n",
        "\n",
        "    # Execute Step 1: Compute the daily Ledoit-Wolf shrinkage covariance matrix\n",
        "    sigma_daily = _compute_daily_ledoit_wolf_covariance(returns_df, universe)\n",
        "\n",
        "    # Execute Step 2: Annualize the covariance matrix\n",
        "    sigma_annualized = _annualize_covariance_matrix(sigma_daily, config)\n",
        "\n",
        "    # Execute Step 3: Validate the structural and numerical integrity of the matrix\n",
        "    _validate_sigma_matrix_integrity(sigma_annualized, expected_N)\n",
        "\n",
        "    logger.debug(f\"Successfully estimated annualized Sigma matrix of shape {sigma_annualized.shape}.\")\n",
        "\n",
        "    # Return the canonical Sigma artifact for the current month\n",
        "    return sigma_annualized\n"
      ],
      "metadata": {
        "id": "5r_FeAwY-y3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Build continuity state management callable (update_continuity_state)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Build continuity state management callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Define initialization for first rebalance month (no prior selection).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _initialize_zero_continuity_state(expected_N: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Initializes a zero-vector continuity state for the first rebalance month.\n",
        "\n",
        "    This function handles the edge case where no prior portfolio exists. It returns\n",
        "    a mathematically pristine zero vector, ensuring that downstream equations\n",
        "    (which subtract the continuity bonus) operate correctly without requiring\n",
        "    complex conditional logic for the first month.\n",
        "\n",
        "    Args:\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D integer array of zeros of shape (N,).\n",
        "    \"\"\"\n",
        "    # Generate a zero vector of the exact required length\n",
        "    # We explicitly use np.int32 to prevent accidental floating-point comparisons downstream\n",
        "    s_prev = np.zeros(expected_N, dtype=np.int32)\n",
        "\n",
        "    logger.debug(\"No previous selection provided. Initialized zero continuity state.\")\n",
        "    return s_prev\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 2: Convert previous selection into an indicator vector aligned to canonical universe order.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_and_cast_previous_selection(previous_selection: np.ndarray, expected_N: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Validates and casts the previous selection into a canonical indicator vector.\n",
        "\n",
        "    This function enforces a strict data contract: the input must be a 1D numpy array\n",
        "    of shape (N,) containing only binary values {0, 1}. It casts the array to an integer\n",
        "    type to guarantee exact mathematical operations in the solver modules.\n",
        "\n",
        "    Args:\n",
        "        previous_selection (np.ndarray): The raw previous selection vector.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The validated, integer-cast indicator vector.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a numpy array.\n",
        "        ValueError: If the shape is incorrect or if non-binary values are detected.\n",
        "    \"\"\"\n",
        "    # Assert the input is strictly a numpy array\n",
        "    if not isinstance(previous_selection, np.ndarray):\n",
        "        raise TypeError(f\"State violation: previous_selection must be a numpy.ndarray, got {type(previous_selection).__name__}.\")\n",
        "\n",
        "    # Assert the exact shape of the vector\n",
        "    if previous_selection.shape != (expected_N,):\n",
        "        raise ValueError(f\"Dimensionality violation: Expected previous_selection shape ({expected_N},), \"\n",
        "                         f\"but observed {previous_selection.shape}.\")\n",
        "\n",
        "    # Assert that the array contains only binary values (0 or 1)\n",
        "    # We use np.unique to inspect the set of realized values\n",
        "    unique_values = np.unique(previous_selection)\n",
        "    if not np.isin(unique_values, [0, 1]).all():\n",
        "        raise ValueError(f\"Mathematical violation: previous_selection must contain only binary values {{0, 1}}. \"\n",
        "                         f\"Observed values: {unique_values}\")\n",
        "\n",
        "    # Cast the vector to a strict integer type to prevent floating-point drift\n",
        "    s_prev = previous_selection.astype(np.int32)\n",
        "\n",
        "    return s_prev\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Step 3: Persist and return the indicator vector; log active continuity assets for audit.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _log_and_return_continuity_state(s_prev: np.ndarray, universe: List[str], solver_name: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Logs the active continuity assets for institutional auditability and returns the vector.\n",
        "\n",
        "    This function extracts the indices of the previously held assets, maps them to their\n",
        "    canonical ticker symbols, and logs them. This provides a transparent audit trail\n",
        "    proving exactly which assets will receive the continuity bonus in the current month.\n",
        "\n",
        "    Args:\n",
        "        s_prev (np.ndarray): The validated, integer-cast indicator vector.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "        solver_name (str): The identifier of the solver (e.g., 'SA' or 'QAOA') for logging context.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The canonical continuity indicator vector.\n",
        "    \"\"\"\n",
        "    # Extract the integer indices where the indicator vector equals 1\n",
        "    active_indices = np.where(s_prev == 1)[0]\n",
        "\n",
        "    # Map the active indices to their corresponding ticker symbols\n",
        "    active_tickers = [universe[i] for i in active_indices]\n",
        "\n",
        "    if len(active_tickers) > 0:\n",
        "        logger.info(f\"[{solver_name}] Continuity bonus will be applied to {len(active_tickers)} assets: {active_tickers}\")\n",
        "    else:\n",
        "        logger.info(f\"[{solver_name}] No continuity bonus applied (zero active assets).\")\n",
        "\n",
        "    return s_prev\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def update_continuity_state(previous_selection: Optional[np.ndarray], config: Dict[str, Any], solver_name: str = \"UNKNOWN\") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrates the management of the continuity state indicator vector.\n",
        "\n",
        "    This function generates the canonical indicator vector s_prev used to apply the\n",
        "    continuity bonus (kappa) in both the SA QUBO and QAOA cost Hamiltonian. It handles\n",
        "    the initialization for the first month and rigorously validates the state carried\n",
        "    over from previous months, ensuring perfect alignment with the canonical universe.\n",
        "\n",
        "    Args:\n",
        "        previous_selection (Optional[np.ndarray]): The binary selection vector from the prior month, or None.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        solver_name (str): An identifier for the calling solver (used for audit logging).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The canonical continuity indicator vector of shape (N,) and type int32.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If structural or mathematical invariants of the previous selection are violated.\n",
        "        TypeError: If the previous selection is of an incorrect data type.\n",
        "    \"\"\"\n",
        "    logger.debug(f\"[{solver_name}] Commencing continuity state update.\")\n",
        "\n",
        "    # Extract the canonical universe to ensure perfect index alignment\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    expected_N = len(universe)\n",
        "\n",
        "    # Branch logic based on the presence of a previous selection\n",
        "    if previous_selection is None:\n",
        "        # Execute Step 1: Initialize zero state for the first month\n",
        "        s_prev = _initialize_zero_continuity_state(expected_N)\n",
        "    else:\n",
        "        # Execute Step 2: Validate and cast the existing state\n",
        "        s_prev = _validate_and_cast_previous_selection(previous_selection, expected_N)\n",
        "\n",
        "    # Execute Step 3: Log the active assets and return the canonical vector\n",
        "    s_prev_final = _log_and_return_continuity_state(s_prev, universe, solver_name)\n",
        "\n",
        "    return s_prev_final\n"
      ],
      "metadata": {
        "id": "OlliySS0H-Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Build penalized QUBO matrix construction callable (build_qubo_matrix)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Build penalized QUBO matrix construction callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Compute penalty magnitude P deterministically and log it.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_deterministic_penalty(mu_ann: np.ndarray, sigma_ann: np.ndarray, q: float, N: int, penalty_scalar: float) -> float:\n",
        "    \"\"\"\n",
        "    Computes the deterministic penalty magnitude P for the QUBO formulation.\n",
        "\n",
        "    This function rigorously defines max_coeff as the maximum absolute value among\n",
        "    all raw return contributions and all raw covariance contributions (including diagonals).\n",
        "    It then scales this maximum by the configured scalar and the universe size N to\n",
        "    ensure the penalty strictly enforces the cardinality constraint without causing\n",
        "    numerical overflow in the energy landscape.\n",
        "\n",
        "    Args:\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        q (float): The risk aversion parameter.\n",
        "        N (int): The number of assets in the universe.\n",
        "        penalty_scalar (float): The scaling multiplier (e.g., 2.5).\n",
        "\n",
        "    Returns:\n",
        "        float: The computed penalty magnitude P.\n",
        "    \"\"\"\n",
        "    # Compute the absolute magnitudes of the linear return contributions: |(1-q) * mu_i|\n",
        "    return_magnitudes = np.abs((1.0 - q) * mu_ann)\n",
        "    max_return_mag = np.max(return_magnitudes)\n",
        "\n",
        "    # Extract the upper triangular elements of the covariance matrix (including the diagonal)\n",
        "    # This ensures we evaluate all unique variances and pairwise covariances\n",
        "    upper_tri_indices = np.triu_indices_from(sigma_ann)\n",
        "    sigma_upper_tri = sigma_ann[upper_tri_indices]\n",
        "\n",
        "    # Compute the absolute magnitudes of the quadratic covariance contributions: |q * Sigma_ij|\n",
        "    covariance_magnitudes = np.abs(q * sigma_upper_tri)\n",
        "    max_cov_mag = np.max(covariance_magnitudes)\n",
        "\n",
        "    # Pin max_coeff as the global maximum across both sets of contributions\n",
        "    max_coeff = max(max_return_mag, max_cov_mag)\n",
        "\n",
        "    # Compute the final penalty magnitude P exactly as specified in the manuscript\n",
        "    # Equation: P = scalar * max_coeff * N\n",
        "    penalty_P = penalty_scalar * max_coeff * N\n",
        "\n",
        "    # Determine the dominant contributor type for audit logging\n",
        "    dominant_type = \"Return\" if max_return_mag > max_cov_mag else \"Covariance\"\n",
        "\n",
        "    logger.debug(f\"QUBO Penalty Computation: max_coeff={max_coeff:.6f} (Dominant: {dominant_type}), \"\n",
        "                 f\"Scalar={penalty_scalar}, N={N} -> P={penalty_P:.6f}\")\n",
        "\n",
        "    return float(penalty_P)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2: Construct QUBO diagonal and off-diagonal coefficients exactly per Eq. (7).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_base_qubo_matrix(mu_ann: np.ndarray, sigma_ann: np.ndarray, q: float, K: int, P: float, N: int) -> Dict[Tuple[int, int], float]:\n",
        "    \"\"\"\n",
        "    Constructs the base Quadratic Unconstrained Binary Optimization (QUBO) dictionary.\n",
        "\n",
        "    This function directly translates Equation (7) from the manuscript into a sparse\n",
        "    dictionary representation. It strictly populates only the upper triangle (i <= j)\n",
        "    to prevent double-counting of symmetric interactions in the dimod BQM.\n",
        "\n",
        "    Args:\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        q (float): The risk aversion parameter.\n",
        "        K (int): The cardinality constraint.\n",
        "        P (float): The computed penalty magnitude.\n",
        "        N (int): The number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        Dict[Tuple[int, int], float]: The upper-triangular QUBO dictionary.\n",
        "    \"\"\"\n",
        "    qubo: Dict[Tuple[int, int], float] = {}\n",
        "\n",
        "    # Iterate over all possible pairs of assets to construct the upper-triangular matrix\n",
        "    for i in range(N):\n",
        "        for j in range(i, N):\n",
        "            if i == j:\n",
        "                # Diagonal terms (Linear contributions + Diagonal variance + Penalty linear expansion)\n",
        "                # Equation (7a): Q_ii = q * Sigma_ii - (1-q) * mu_i + P * (1 - 2K)\n",
        "                q_ii = (q * sigma_ann[i, i]) - ((1.0 - q) * mu_ann[i]) + (P * (1.0 - 2.0 * K))\n",
        "                qubo[(i, i)] = float(q_ii)\n",
        "            else:\n",
        "                # Off-diagonal terms (Pairwise covariance + Penalty quadratic expansion)\n",
        "                # Equation (7b): Q_ij = 2q * Sigma_ij + 2P  (for i < j)\n",
        "                q_ij = (2.0 * q * sigma_ann[i, j]) + (2.0 * P)\n",
        "                qubo[(i, j)] = float(q_ij)\n",
        "\n",
        "    return qubo\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 3: Apply continuity bonus on the diagonal exactly once, using the pinned arithmetic rule, and return.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_continuity_bonus(qubo: Dict[Tuple[int, int], float], s_prev: np.ndarray, kappa: float, N: int, universe: List[str]) -> Dict[Tuple[int, int], float]:\n",
        "    \"\"\"\n",
        "    Applies the temporal regularization (continuity bonus) to the QUBO diagonal.\n",
        "\n",
        "    This function enforces the pinned arithmetic rule 'subtract_kappa_from_linear_term'.\n",
        "    It reduces the energy cost of previously held assets, discouraging unnecessary\n",
        "    turnover. It logs the exact modifications for institutional auditability.\n",
        "\n",
        "    Args:\n",
        "        qubo (Dict[Tuple[int, int], float]): The base QUBO dictionary.\n",
        "        s_prev (np.ndarray): The binary continuity indicator vector.\n",
        "        kappa (float): The continuity bonus strength.\n",
        "        N (int): The number of assets in the universe.\n",
        "        universe (List[str]): The canonical list of asset tickers (for logging).\n",
        "\n",
        "    Returns:\n",
        "        Dict[Tuple[int, int], float]: The regularized QUBO dictionary.\n",
        "    \"\"\"\n",
        "    modifications = []\n",
        "\n",
        "    for i in range(N):\n",
        "        # Check if the asset was held in the previous month\n",
        "        if s_prev[i] == 1:\n",
        "            original_val = qubo[(i, i)]\n",
        "            # Apply the pinned rule: Q_ii <- Q_ii - kappa\n",
        "            new_val = original_val - kappa\n",
        "            qubo[(i, i)] = new_val\n",
        "\n",
        "            # Record the modification for the audit trail\n",
        "            modifications.append(f\"{universe[i]}: {original_val:.4f} -> {new_val:.4f}\")\n",
        "\n",
        "    if modifications:\n",
        "        logger.debug(f\"Applied continuity bonus (kappa={kappa}) to QUBO diagonals: \" + \" | \".join(modifications))\n",
        "\n",
        "    return qubo\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_qubo_matrix(mu_ann: np.ndarray, sigma_ann: np.ndarray, config: Dict[str, Any], s_prev: np.ndarray) -> Dict[Tuple[int, int], float]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the penalized, regularized QUBO matrix.\n",
        "\n",
        "    This function translates the constrained portfolio optimization problem into a\n",
        "    Quadratic Unconstrained Binary Optimization (QUBO) format suitable for Simulated\n",
        "    Annealing. It computes a deterministic penalty to enforce the K-of-N constraint,\n",
        "    constructs the base energy landscape per Equation (7), and applies a continuity\n",
        "    bonus to penalize turnover.\n",
        "\n",
        "    Args:\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        s_prev (np.ndarray): The binary continuity indicator vector from the prior month.\n",
        "\n",
        "    Returns:\n",
        "        Dict[Tuple[int, int], float]: The finalized, upper-triangular QUBO dictionary.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the continuity application rule is unrecognized.\n",
        "    \"\"\"\n",
        "    logger.debug(\"Commencing penalized QUBO matrix construction.\")\n",
        "\n",
        "    # Extract required parameters from the configuration\n",
        "    q = config[\"objective_function\"][\"risk_aversion_q\"]\n",
        "    K = config[\"objective_function\"][\"cardinality_K\"]\n",
        "    penalty_scalar = config[\"sa_solver\"][\"sa_penalty_scalar\"]\n",
        "    kappa = config[\"objective_function\"][\"continuity_bonus_kappa\"]\n",
        "    rule = config[\"objective_function\"][\"continuity_bonus_application_rule\"]\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    N = len(universe)\n",
        "\n",
        "    # Assert the continuity rule matches the pinned convention\n",
        "    if rule != \"subtract_kappa_from_linear_term\":\n",
        "        raise ValueError(f\"Unrecognized continuity rule: '{rule}'. Expected 'subtract_kappa_from_linear_term'.\")\n",
        "\n",
        "    # Execute Step 1: Compute the deterministic penalty magnitude P\n",
        "    P = _compute_deterministic_penalty(mu_ann, sigma_ann, q, N, penalty_scalar)\n",
        "\n",
        "    # Execute Step 2: Construct the base QUBO dictionary per Equation (7)\n",
        "    base_qubo = _construct_base_qubo_matrix(mu_ann, sigma_ann, q, K, P, N)\n",
        "\n",
        "    # Execute Step 3: Apply the continuity bonus to the diagonal terms\n",
        "    final_qubo = _apply_continuity_bonus(base_qubo, s_prev, kappa, N, universe)\n",
        "\n",
        "    logger.debug(f\"Successfully constructed QUBO matrix with {len(final_qubo)} terms.\")\n",
        "\n",
        "    # Return the canonical classical objective function artifact\n",
        "    return final_qubo\n"
      ],
      "metadata": {
        "id": "-kGW2Z5DI6Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Build SA sampling callable (run_sa_sampling)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Build SA sampling callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Convert QUBO dict into a dimod.BinaryQuadraticModel (BQM) with pinned variable labeling.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _convert_qubo_to_bqm(qubo: Dict[Tuple[int, int], float], expected_N: int) -> dimod.BinaryQuadraticModel:\n",
        "    \"\"\"\n",
        "    Converts the sparse QUBO dictionary into a dimod BinaryQuadraticModel (BQM).\n",
        "\n",
        "    This function rigorously enforces the upper-triangular invariant of the input\n",
        "    dictionary to prevent double-counting of off-diagonal penalties. It instantiates\n",
        "    the BQM and mathematically asserts that the variable labels are strictly integers\n",
        "    from 0 to N-1, guaranteeing deterministic mapping back to the canonical asset universe.\n",
        "\n",
        "    Args:\n",
        "        qubo (Dict[Tuple[int, int], float]): The upper-triangular QUBO dictionary.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        dimod.BinaryQuadraticModel: The instantiated BQM object.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the QUBO is not upper-triangular or if variable labels are corrupted.\n",
        "    \"\"\"\n",
        "    # Assert the upper-triangular invariant: i <= j for all keys\n",
        "    for (i, j) in qubo.keys():\n",
        "        if i > j:\n",
        "            raise ValueError(f\"QUBO invariant violation: Found lower-triangular key ({i}, {j}). \"\n",
        "                             f\"QUBO must be strictly upper-triangular to prevent double-counting.\")\n",
        "\n",
        "    # Instantiate the BQM from the QUBO dictionary\n",
        "    # We explicitly specify the Vartype as BINARY (0 or 1)\n",
        "    bqm = dimod.BinaryQuadraticModel.from_qubo(qubo)\n",
        "\n",
        "    # Extract the variables present in the BQM\n",
        "    bqm_variables = list(bqm.variables)\n",
        "\n",
        "    # Assert that exactly N variables exist in the model\n",
        "    if len(bqm_variables) != expected_N:\n",
        "        raise ValueError(f\"BQM dimensionality violation: Expected {expected_N} variables, \"\n",
        "                         f\"but BQM contains {len(bqm_variables)}.\")\n",
        "\n",
        "    # Assert that the variables are strictly integers from 0 to N-1\n",
        "    expected_variables = list(range(expected_N))\n",
        "    if sorted(bqm_variables) != expected_variables:\n",
        "        raise ValueError(f\"BQM labeling violation: Expected integer variables {expected_variables}, \"\n",
        "                         f\"but observed {sorted(bqm_variables)}.\")\n",
        "\n",
        "    return bqm\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Instantiate neal.SimulatedAnnealingSampler and set seed behavior explicitly.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _prepare_sampler_and_hyperparameters(config: Dict[str, Any]) -> Tuple[neal.SimulatedAnnealingSampler, int, int, int]:\n",
        "    \"\"\"\n",
        "    Instantiates the Simulated Annealing sampler and extracts pinned hyperparameters.\n",
        "\n",
        "    This function enforces the determinism contract by explicitly extracting the\n",
        "    random seed, along with the required number of reads and sweeps, ensuring the\n",
        "    stochastic heuristic is executed as reproducibly as the underlying library permits.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[neal.SimulatedAnnealingSampler, int, int, int]:\n",
        "            The sampler instance, num_reads, num_sweeps, and the random seed.\n",
        "    \"\"\"\n",
        "    # Instantiate the highly optimized C++ backed Simulated Annealing sampler\n",
        "    sampler = neal.SimulatedAnnealingSampler()\n",
        "\n",
        "    # Extract the pinned hyperparameters from the configuration\n",
        "    num_reads = config[\"sa_solver\"][\"sa_num_reads\"]\n",
        "    num_sweeps = config[\"sa_solver\"][\"sa_num_sweeps\"]\n",
        "    sa_seed = config[\"randomness\"][\"sa_seed\"]\n",
        "\n",
        "    logger.debug(f\"Prepared SA Sampler. Hyperparameters: reads={num_reads}, sweeps={num_sweeps}, seed={sa_seed}\")\n",
        "\n",
        "    return sampler, num_reads, num_sweeps, sa_seed\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 3: Execute sampling and return a dimod.SampleSet without filtering.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_sa_sampling(bqm: dimod.BinaryQuadraticModel, sampler: neal.SimulatedAnnealingSampler,\n",
        "                         num_reads: int, num_sweeps: int, sa_seed: int) -> dimod.SampleSet:\n",
        "    \"\"\"\n",
        "    Executes the Simulated Annealing heuristic on the BQM to generate raw samples.\n",
        "\n",
        "    This function calls the sampler with the pinned hyperparameters. It explicitly\n",
        "    returns the raw, unfiltered SampleSet, delegating the domain-specific feasibility\n",
        "    filtering (sum x_i = K) to the subsequent task to maintain strict separation of concerns.\n",
        "\n",
        "    Args:\n",
        "        bqm (dimod.BinaryQuadraticModel): The validated BQM representing the energy landscape.\n",
        "        sampler (neal.SimulatedAnnealingSampler): The instantiated SA sampler.\n",
        "        num_reads (int): The number of independent heuristic runs.\n",
        "        num_sweeps (int): The number of sweeps per run.\n",
        "        sa_seed (int): The random seed for the stochastic process.\n",
        "\n",
        "    Returns:\n",
        "        dimod.SampleSet: The raw collection of binary samples and their associated energies.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the sampler fails to return the requested number of reads.\n",
        "    \"\"\"\n",
        "    # Execute the sampling process\n",
        "    # We pass the seed explicitly to enforce the determinism contract\n",
        "    sample_set = sampler.sample(bqm, num_reads=num_reads, num_sweeps=num_sweeps, seed=sa_seed)\n",
        "\n",
        "    # Calculate the total number of samples returned (should equal num_reads)\n",
        "    total_samples_returned = sum(sample_set.record.num_occurrences)\n",
        "\n",
        "    # Assert that the sampler honored the num_reads request\n",
        "    if total_samples_returned != num_reads:\n",
        "        raise RuntimeError(f\"Sampling execution failure: Requested {num_reads} reads, \"\n",
        "                           f\"but sampler returned {total_samples_returned}.\")\n",
        "\n",
        "    # Extract minimum and maximum energies for diagnostic logging\n",
        "    min_energy = sample_set.first.energy\n",
        "    max_energy = sample_set.record.energy.max()\n",
        "\n",
        "    logger.debug(f\"SA Sampling completed. Returned {total_samples_returned} samples. \"\n",
        "                 f\"Energy range: [{min_energy:.4f}, {max_energy:.4f}]\")\n",
        "\n",
        "    return sample_set\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_sa_sampling(qubo: Dict[Tuple[int, int], float], expected_N: int, config: Dict[str, Any]) -> dimod.SampleSet:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of the Simulated Annealing baseline solver.\n",
        "\n",
        "    This function translates the sparse QUBO dictionary into a rigorous BinaryQuadraticModel,\n",
        "    instantiates the neal sampler, and executes the stochastic heuristic using the pinned\n",
        "    hyperparameters and determinism contract. It returns the raw, unfiltered SampleSet\n",
        "    for downstream feasibility analysis.\n",
        "\n",
        "    Args:\n",
        "        qubo (Dict[Tuple[int, int], float]): The upper-triangular QUBO dictionary.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        dimod.SampleSet: The raw collection of binary samples and their associated energies.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the QUBO structure or BQM labeling is invalid.\n",
        "        RuntimeError: If the sampling execution fails.\n",
        "    \"\"\"\n",
        "    logger.info(\"Commencing Simulated Annealing (SA) sampling execution.\")\n",
        "\n",
        "    # Execute Step 1: Convert the QUBO dictionary to a validated BQM\n",
        "    bqm = _convert_qubo_to_bqm(qubo, expected_N)\n",
        "\n",
        "    # Execute Step 2: Prepare the sampler and extract hyperparameters\n",
        "    sampler, num_reads, num_sweeps, sa_seed = _prepare_sampler_and_hyperparameters(config)\n",
        "\n",
        "    # Execute Step 3: Execute the sampling process and retrieve the raw SampleSet\n",
        "    sample_set = _execute_sa_sampling(bqm, sampler, num_reads, num_sweeps, sa_seed)\n",
        "\n",
        "    logger.info(\"Simulated Annealing sampling execution completed successfully.\")\n",
        "\n",
        "    # Return the raw SampleSet artifact\n",
        "    return sample_set\n"
      ],
      "metadata": {
        "id": "cPqtwZPOJ4WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Build SA feasibility filtering and selection callable (filter_sa_samples)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Build SA feasibility filtering and selection callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Iterate through SampleSet and filter to feasibility (sum x_i = K) exactly.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _filter_feasible_samples(sample_set: Any, K: int, expected_N: int) -> List[Tuple[np.ndarray, float]]:\n",
        "    \"\"\"\n",
        "    Filters the raw SampleSet to retain only strictly feasible bitstrings.\n",
        "\n",
        "    This function iterates through the unique aggregated records of the stochastic samples,\n",
        "    mathematically asserting the hard cardinality constraint (sum x_i = K). By iterating\n",
        "    over .record instead of .data(), it eliminates redundant processing of identical samples,\n",
        "    significantly improving computational efficiency.\n",
        "\n",
        "    Args:\n",
        "        sample_set (dimod.SampleSet): The raw output from the SA sampler.\n",
        "        K (int): The exact cardinality constraint.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[np.ndarray, float]]: A list of tuples containing feasible binary vectors and their energies.\n",
        "    \"\"\"\n",
        "    feasible_candidates: List[Tuple[np.ndarray, float]] = []\n",
        "\n",
        "    # Iterate through the unique aggregated records to avoid redundant processing\n",
        "    for record in sample_set.record:\n",
        "        # Extract the sample array directly. Because we pinned the BQM variables to 0..N-1\n",
        "        # in Task 11, this array is already perfectly aligned with the canonical universe.\n",
        "        x_vector = record.sample.astype(np.int32)\n",
        "\n",
        "        # Compute the Hamming weight (sum of the binary vector)\n",
        "        hamming_weight = np.sum(x_vector)\n",
        "\n",
        "        # Enforce the strict feasibility constraint\n",
        "        if hamming_weight == K:\n",
        "            # record.energy is a scalar float\n",
        "            feasible_candidates.append((x_vector, float(record.energy)))\n",
        "\n",
        "    return feasible_candidates\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 2: Select the minimum-energy feasible sample with deterministic tie-breaking.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _select_optimal_deterministic_candidate(feasible_candidates: List[Tuple[np.ndarray, float]]) -> Tuple[np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Selects the minimum-energy candidate with a strict lexicographic tie-breaking rule.\n",
        "\n",
        "    This function sorts the feasible candidates primarily by energy (ascending). To\n",
        "    guarantee absolute determinism across different environments, it resolves energy\n",
        "    ties (within a 1e-12 tolerance) by selecting the lexicographically smallest bitstring.\n",
        "\n",
        "    Args:\n",
        "        feasible_candidates (List[Tuple[np.ndarray, float]]): The list of feasible candidates.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, float]: The optimal binary vector and its energy.\n",
        "    \"\"\"\n",
        "    # Define a custom sort key that returns a tuple: (energy, lexicographic_tuple)\n",
        "    # Python sorts tuples element by element, perfectly implementing our tie-break logic\n",
        "    def sort_key(candidate: Tuple[np.ndarray, float]) -> Tuple[float, Tuple[int, ...]]:\n",
        "        x_vector, energy = candidate\n",
        "        # Round energy to 12 decimal places to group floating-point near-ties\n",
        "        rounded_energy = round(energy, 12)\n",
        "        # Convert the numpy array to a standard Python tuple for lexicographic comparison\n",
        "        lexicographic_tuple = tuple(x_vector.tolist())\n",
        "        return (rounded_energy, lexicographic_tuple)\n",
        "\n",
        "    # Sort the candidates using the deterministic key\n",
        "    feasible_candidates.sort(key=sort_key)\n",
        "\n",
        "    # The optimal candidate is the first element in the sorted list\n",
        "    optimal_candidate = feasible_candidates[0]\n",
        "\n",
        "    return optimal_candidate\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Return the selected bitstring as a canonical numpy vector; log feasibility rate and selected assets.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _log_diagnostics_and_format_output(optimal_x: np.ndarray, optimal_energy: float,\n",
        "                                       total_reads: int, feasible_count: int, universe: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Logs comprehensive diagnostic telemetry and returns the canonical selection vector.\n",
        "\n",
        "    This function maps the selected binary vector back to the canonical ticker symbols\n",
        "    for institutional auditability. It also logs the feasibility rate, which is critical\n",
        "    for diagnosing the efficacy of the QUBO penalty scalar.\n",
        "\n",
        "    Args:\n",
        "        optimal_x (np.ndarray): The selected optimal binary vector.\n",
        "        optimal_energy (float): The energy of the selected vector.\n",
        "        total_reads (int): The total number of samples generated by the SA solver.\n",
        "        feasible_count (int): The number of samples that satisfied the cardinality constraint.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The canonical selection vector x_SA.\n",
        "    \"\"\"\n",
        "    # Calculate the feasibility rate\n",
        "    feasible_fraction = feasible_count / total_reads\n",
        "\n",
        "    # Extract the indices of the selected assets\n",
        "    selected_indices = np.where(optimal_x == 1)[0]\n",
        "\n",
        "    # Map indices to ticker symbols\n",
        "    selected_tickers = [universe[i] for i in selected_indices]\n",
        "\n",
        "    logger.info(f\"SA Selection Complete. Optimal Energy: {optimal_energy:.6f}\")\n",
        "    logger.info(f\"SA Feasibility Rate: {feasible_fraction:.2%} ({feasible_count}/{total_reads} reads)\")\n",
        "    logger.info(f\"SA Selected Assets: {selected_tickers}\")\n",
        "\n",
        "    return optimal_x\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def filter_sa_samples(sample_set: dimod.SampleSet, config: Dict[str, Any]) -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the feasibility filtering and optimal selection of Simulated Annealing samples.\n",
        "\n",
        "    This function processes the raw stochastic output from the SA solver. It rigorously\n",
        "    enforces the K-of-N cardinality constraint, deterministically selects the minimum-energy\n",
        "    solution using lexicographic tie-breaking, and logs critical diagnostic telemetry.\n",
        "    If no feasible samples are found, it gracefully returns None to trigger the HRP fallback.\n",
        "\n",
        "    Args:\n",
        "        sample_set (dimod.SampleSet): The raw output from the SA sampler.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Optional[np.ndarray]: The canonical binary selection vector x_SA, or None if zero feasible samples exist.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the sample dictionaries are malformed.\n",
        "    \"\"\"\n",
        "    logger.debug(\"Commencing SA feasibility filtering and optimal selection.\")\n",
        "\n",
        "    # Extract required parameters from the configuration\n",
        "    K = config[\"objective_function\"][\"cardinality_K\"]\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    expected_N = len(universe)\n",
        "    total_reads = config[\"sa_solver\"][\"sa_num_reads\"]\n",
        "\n",
        "    # Execute Step 1: Filter the raw SampleSet to strictly feasible candidates\n",
        "    feasible_candidates = _filter_feasible_samples(sample_set, K, expected_N)\n",
        "    feasible_count = len(feasible_candidates)\n",
        "\n",
        "    # Handle the critical edge case: Zero feasible samples\n",
        "    if feasible_count == 0:\n",
        "        logger.warning(f\"CRITICAL: SA solver produced ZERO feasible samples (K={K}, N={expected_N}). \"\n",
        "                       f\"Penalty scalar may be insufficient. Triggering HRP fallback.\")\n",
        "        return None\n",
        "\n",
        "    # Execute Step 2: Select the optimal candidate deterministically\n",
        "    optimal_x, optimal_energy = _select_optimal_deterministic_candidate(feasible_candidates)\n",
        "\n",
        "    # Execute Step 3: Log diagnostics and format the final output\n",
        "    final_x_sa = _log_diagnostics_and_format_output(optimal_x, optimal_energy, total_reads, feasible_count, universe)\n",
        "\n",
        "    return final_x_sa\n"
      ],
      "metadata": {
        "id": "RQ_3RBxqLDr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Build Dicke state preparation callable (prepare_dicke_state_vector)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Build Dicke state preparation callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Enumerate all computational basis indices and identify those with Hamming weight K.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _identify_valid_basis_indices(N: int, K: int) -> List[int]:\n",
        "    \"\"\"\n",
        "    Enumerates the 2^N Hilbert space and identifies indices with Hamming weight K.\n",
        "\n",
        "    This function enforces a strict big-endian bit-ordering convention:\n",
        "    k = sum(x_i * 2^(N-1-i)). This guarantees that wire 0 corresponds to the most\n",
        "    significant bit (asset 0), and wire N-1 corresponds to the least significant bit.\n",
        "\n",
        "    Args:\n",
        "        N (int): The number of qubits (assets in the universe).\n",
        "        K (int): The required Hamming weight (cardinality constraint).\n",
        "\n",
        "    Returns:\n",
        "        List[int]: A list of integer indices corresponding to the valid basis states.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of valid indices does not match the binomial coefficient.\n",
        "    \"\"\"\n",
        "    valid_indices: List[int] = []\n",
        "    total_states = 2 ** N\n",
        "\n",
        "    # Iterate through all possible computational basis states\n",
        "    for k in range(total_states):\n",
        "        # Use Python's highly optimized native bit counting method\n",
        "        if k.bit_count() == K:\n",
        "            valid_indices.append(k)\n",
        "\n",
        "    # Mathematically assert that the number of valid states equals N choose K\n",
        "    expected_count = math.comb(N, K)\n",
        "    if len(valid_indices) != expected_count:\n",
        "        raise ValueError(f\"Combinatorial violation: Expected {expected_count} valid states \"\n",
        "                         f\"for N={N}, K={K}, but found {len(valid_indices)}.\")\n",
        "\n",
        "    return valid_indices\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Construct the amplitude vector of length 2^N with equal amplitudes on the valid indices.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_amplitude_vector(valid_indices: List[int], N: int, K: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the dense statevector array for the Dicke state |D^K_N>.\n",
        "\n",
        "    This function initializes a zero vector of length 2^N and assigns the uniform\n",
        "    real amplitude to all valid indices. It explicitly casts the Python list of indices\n",
        "    to a numpy array before applying the mask, ensuring optimal memory access patterns\n",
        "    and preventing implicit type coercion overhead.\n",
        "\n",
        "    Args:\n",
        "        valid_indices (List[int]): The list of integer indices with Hamming weight K.\n",
        "        N (int): The number of qubits.\n",
        "        K (int): The required Hamming weight.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The dense 1D statevector array.\n",
        "    \"\"\"\n",
        "    total_states = 2 ** N\n",
        "\n",
        "    # Initialize a strictly real, double-precision zero vector\n",
        "    statevector = np.zeros(total_states, dtype=np.float64)\n",
        "\n",
        "    # Compute the exact uniform amplitude\n",
        "    combinatorial_count = math.comb(N, K)\n",
        "    uniform_amplitude = 1.0 / math.sqrt(combinatorial_count)\n",
        "\n",
        "    # Explicitly cast the Python list to a numpy array of integers\n",
        "    # This is a critical type-hygiene step for optimal advanced indexing\n",
        "    indices_array = np.array(valid_indices, dtype=np.int32)\n",
        "\n",
        "    # Apply the amplitude to all valid indices simultaneously\n",
        "    statevector[indices_array] = uniform_amplitude\n",
        "\n",
        "    return statevector\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Validate normalization and sparsity exactly; return the amplitude vector.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_statevector_integrity(statevector: np.ndarray, N: int, K: int) -> None:\n",
        "    \"\"\"\n",
        "    Performs rigorous quantum mechanical validation on the constructed statevector.\n",
        "\n",
        "    This function mathematically asserts that the statevector is perfectly normalized\n",
        "    (L2 norm squared equals 1.0) and strictly obeys the sparsity constraint (exactly\n",
        "    N choose K non-zero elements).\n",
        "\n",
        "    Args:\n",
        "        statevector (np.ndarray): The constructed statevector array.\n",
        "        N (int): The number of qubits.\n",
        "        K (int): The required Hamming weight.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If normalization or sparsity invariants are violated.\n",
        "    \"\"\"\n",
        "    # Assert perfect normalization: sum(|a_k|^2) == 1.0\n",
        "    # We use a strict tolerance of 1e-12 to account for floating-point arithmetic\n",
        "    l2_norm_squared = np.sum(np.square(statevector))\n",
        "    if not math.isclose(l2_norm_squared, 1.0, abs_tol=1e-12):\n",
        "        raise ValueError(f\"Quantum mechanics violation: Statevector is not normalized. \"\n",
        "                         f\"L2 norm squared = {l2_norm_squared:.15f}\")\n",
        "\n",
        "    # Assert strict sparsity: exactly N choose K non-zero elements\n",
        "    expected_non_zeros = math.comb(N, K)\n",
        "    actual_non_zeros = np.count_nonzero(statevector)\n",
        "    if actual_non_zeros != expected_non_zeros:\n",
        "        raise ValueError(f\"Sparsity violation: Expected exactly {expected_non_zeros} non-zero amplitudes, \"\n",
        "                         f\"but observed {actual_non_zeros}.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prepare_dicke_state_vector(N: int, K: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrates the preparation of the canonical Dicke state |D^K_N> statevector.\n",
        "\n",
        "    This function generates the exact initial quantum state required for the constraint-\n",
        "    preserving QAOA ansatz. It enumerates the Hilbert space under a pinned big-endian\n",
        "    convention, constructs the uniform superposition over the feasible K-hot subspace,\n",
        "    and rigorously validates the quantum mechanical properties of the resulting vector.\n",
        "\n",
        "    Args:\n",
        "        N (int): The number of qubits (assets in the universe).\n",
        "        K (int): The cardinality constraint (number of assets to select).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The validated, dense statevector of length 2^N and type float64.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If combinatorial, normalization, or sparsity invariants are violated.\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Commencing Dicke state preparation for N={N}, K={K}.\")\n",
        "\n",
        "    # Execute Step 1: Identify all valid basis indices in the 2^N space\n",
        "    valid_indices = _identify_valid_basis_indices(N, K)\n",
        "\n",
        "    # Execute Step 2: Construct the dense amplitude vector\n",
        "    statevector = _construct_amplitude_vector(valid_indices, N, K)\n",
        "\n",
        "    # Execute Step 3: Validate normalization and sparsity\n",
        "    _validate_statevector_integrity(statevector, N, K)\n",
        "\n",
        "    logger.debug(f\"Successfully prepared and validated Dicke statevector of length {len(statevector)}.\")\n",
        "\n",
        "    # Return the canonical initial state artifact\n",
        "    return statevector\n"
      ],
      "metadata": {
        "id": "zxcuo28cMIyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Build XY mixer Hamiltonian callable (build_xy_mixer_hamiltonian)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Build XY mixer Hamiltonian callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Generate the interaction edge set E deterministically from the configured topology.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_interaction_edges(N: int, graph_type: str) -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Generates the deterministic edge set for the XY mixer interaction graph.\n",
        "\n",
        "    This function enforces the pinned 'complete' graph topology specified in the\n",
        "    manuscript. It generates all unique pairwise interactions (i < j) in a strict\n",
        "    lexicographic order to guarantee absolute reproducibility of the Hamiltonian structure.\n",
        "\n",
        "    Args:\n",
        "        N (int): The number of qubits (assets in the universe).\n",
        "        graph_type (str): The configured graph topology (must be 'complete').\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[int, int]]: A lexicographically sorted list of edge tuples.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the graph topology is unrecognized or if the edge count is incorrect.\n",
        "    \"\"\"\n",
        "    # Assert strict adherence to the pinned complete graph topology\n",
        "    if graph_type != \"complete\":\n",
        "        raise ValueError(f\"Topology violation: Expected 'complete' graph type, got '{graph_type}'.\")\n",
        "\n",
        "    edges: List[Tuple[int, int]] = []\n",
        "\n",
        "    # Generate edges lexicographically: 0 <= i < j <= N-1\n",
        "    for i in range(N):\n",
        "        for j in range(i + 1, N):\n",
        "            edges.append((i, j))\n",
        "\n",
        "    # Mathematically assert the correct number of edges for a complete graph\n",
        "    expected_edges = math.comb(N, 2)\n",
        "    if len(edges) != expected_edges:\n",
        "        raise ValueError(f\"Combinatorial violation: Expected {expected_edges} edges for a complete graph \"\n",
        "                         f\"of size {N}, but generated {len(edges)}.\")\n",
        "\n",
        "    return edges\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Construct H_XY term-by-term as a PennyLane Hamiltonian with exactly 2|E| Pauli terms.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_xy_hamiltonian(edges: List[Tuple[int, int]]) -> qml.Hamiltonian:\n",
        "    \"\"\"\n",
        "    Constructs the PennyLane Hamiltonian object for the XY mixer.\n",
        "\n",
        "    This function directly translates Equation (3) into quantum observables. For each\n",
        "    edge (i, j), it generates the partial SWAP generators (X_i @ X_j) and (Y_i @ Y_j)\n",
        "    with a strict coefficient of 1.0.\n",
        "\n",
        "    NOTE: Utilizing qml.Hamiltonian for exact fidelity with the manuscript's era.\n",
        "    In modern PennyLane (v0.36+), this should be migrated to qml.ops.LinearCombination\n",
        "    or constructed via qml.dot(coeffs, observables) for optimal performance and\n",
        "    future-proofing.\n",
        "\n",
        "    Args:\n",
        "        edges (List[Tuple[int, int]]): The deterministic list of interaction edges.\n",
        "\n",
        "    Returns:\n",
        "        qml.Hamiltonian: The instantiated XY mixer Hamiltonian.\n",
        "    \"\"\"\n",
        "    coeffs: List[float] = []\n",
        "    observables: List[Any] = []\n",
        "\n",
        "    # Iterate through the deterministic edge list\n",
        "    for i, j in edges:\n",
        "        # Term 1: X_i (x) X_j\n",
        "        coeffs.append(1.0)\n",
        "        observables.append(qml.PauliX(i) @ qml.PauliX(j))\n",
        "\n",
        "        # Term 2: Y_i (x) Y_j\n",
        "        coeffs.append(1.0)\n",
        "        observables.append(qml.PauliY(i) @ qml.PauliY(j))\n",
        "\n",
        "    # Instantiate the PennyLane Hamiltonian\n",
        "    # This object encapsulates the entire mixing evolution operator\n",
        "    h_xy = qml.Hamiltonian(coeffs, observables)\n",
        "\n",
        "    return h_xy\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Validate expected term count and rely on the commutation property as the feasibility guarantee.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_hamiltonian_structure(h_xy: qml.Hamiltonian, expected_edges: int) -> None:\n",
        "    \"\"\"\n",
        "    Performs rigorous structural validation on the constructed XY Hamiltonian.\n",
        "\n",
        "    This function mathematically asserts that the Hamiltonian contains exactly the\n",
        "    expected number of terms (2 * |E|). This structural guarantee is the operational\n",
        "    condition under which the physical commutation property [H_XY, sum(Z_i)] = 0 holds,\n",
        "    ensuring the QAOA evolution remains strictly within the feasible K-hot subspace.\n",
        "\n",
        "    Args:\n",
        "        h_xy (qml.Hamiltonian): The constructed XY mixer Hamiltonian.\n",
        "        expected_edges (int): The number of edges in the interaction graph.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of terms in the Hamiltonian is incorrect.\n",
        "    \"\"\"\n",
        "    # Calculate the expected number of Pauli terms\n",
        "    expected_terms = 2 * expected_edges\n",
        "\n",
        "    # Extract the actual number of coefficients in the Hamiltonian\n",
        "    actual_terms = len(h_xy.coeffs)\n",
        "\n",
        "    # Assert strict equality\n",
        "    if actual_terms != expected_terms:\n",
        "        raise ValueError(f\"Structural violation: Expected exactly {expected_terms} terms in the XY Hamiltonian, \"\n",
        "                         f\"but observed {actual_terms}.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_xy_mixer_hamiltonian(N: int, config: Dict[str, Any]) -> qml.Hamiltonian:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the constraint-preserving XY mixer Hamiltonian.\n",
        "\n",
        "    This function generates the canonical mixing operator required for the QAOA ansatz.\n",
        "    It enforces the complete graph topology, constructs the exact Pauli tensor products\n",
        "    required for the particle-preserving partial SWAP operations, and rigorously validates\n",
        "    the structural integrity of the resulting quantum object. This Hamiltonian is built\n",
        "    once and reused across all rebalance months.\n",
        "\n",
        "    Args:\n",
        "        N (int): The number of qubits (assets in the universe).\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        qml.Hamiltonian: The validated, canonical XY mixer Hamiltonian.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If topological or structural invariants are violated.\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Commencing XY mixer Hamiltonian construction for N={N}.\")\n",
        "\n",
        "    # Extract the pinned graph topology from the configuration\n",
        "    graph_type = config[\"qaoa_architecture\"][\"xy_mixer_graph_type\"]\n",
        "\n",
        "    # Execute Step 1: Generate the deterministic interaction edge set\n",
        "    edges = _generate_interaction_edges(N, graph_type)\n",
        "\n",
        "    # Execute Step 2: Construct the Hamiltonian term-by-term\n",
        "    h_xy = _construct_xy_hamiltonian(edges)\n",
        "\n",
        "    # Execute Step 3: Validate the structural integrity of the Hamiltonian\n",
        "    _validate_hamiltonian_structure(h_xy, len(edges))\n",
        "\n",
        "    logger.debug(f\"Successfully constructed XY mixer Hamiltonian with {len(h_xy.coeffs)} terms.\")\n",
        "\n",
        "    # Return the canonical mixer artifact\n",
        "    return h_xy\n"
      ],
      "metadata": {
        "id": "4Rt1e4MPNPhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Build cost Hamiltonian construction callable (build_cost_hamiltonian)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Build cost Hamiltonian construction callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1: Compute linear coefficients alpha_i exactly as defined in Eq. (5), apply continuity bonus, and log.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_linear_coefficients(mu_ann: np.ndarray, q: float, s_prev: np.ndarray, kappa: float, universe: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the linear coefficients (alpha_i) for the cost Hamiltonian.\n",
        "\n",
        "    This function strictly implements the definition from Equation (5):\n",
        "    alpha_i = -(1-q) * mu_i. It then applies the pinned continuity rule by\n",
        "    subtracting kappa for previously held assets, lowering their energy cost.\n",
        "\n",
        "    Args:\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        q (float): The risk aversion parameter.\n",
        "        s_prev (np.ndarray): The binary continuity indicator vector.\n",
        "        kappa (float): The continuity bonus strength.\n",
        "        universe (List[str]): The canonical list of asset tickers (for logging).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The computed alpha vector of shape (N,).\n",
        "    \"\"\"\n",
        "    # Compute the base linear coefficients exactly per Eq. (5)\n",
        "    alpha_base = -(1.0 - q) * mu_ann\n",
        "\n",
        "    # Create a copy to store the regularized coefficients\n",
        "    alpha_final = np.copy(alpha_base)\n",
        "\n",
        "    modifications = []\n",
        "    N = len(universe)\n",
        "\n",
        "    # Apply the continuity bonus (discount) to previously held assets\n",
        "    for i in range(N):\n",
        "        if s_prev[i] == 1:\n",
        "            original_val = alpha_base[i]\n",
        "            # Apply the pinned rule: alpha_i <- alpha_i - kappa\n",
        "            new_val = original_val - kappa\n",
        "            alpha_final[i] = new_val\n",
        "\n",
        "            # Record the modification for the audit trail\n",
        "            modifications.append(f\"{universe[i]}: {original_val:.4f} -> {new_val:.4f}\")\n",
        "\n",
        "    if modifications:\n",
        "        logger.debug(f\"Applied continuity discount (kappa={kappa}) to Cost Hamiltonian alphas: \" + \" | \".join(modifications))\n",
        "\n",
        "    return alpha_final\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Compute pairwise coefficients beta_ij exactly and only for i < j.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_pairwise_coefficients(sigma_ann: np.ndarray, q: float, N: int) -> List[Tuple[Tuple[int, int], float]]:\n",
        "    \"\"\"\n",
        "    Computes the pairwise quadratic coefficients (beta_ij) for the cost Hamiltonian.\n",
        "\n",
        "    This function strictly implements the definition from Equation (5):\n",
        "    beta_ij = 2q * Sigma_ij. It enforces the strict inequality i < j to extract\n",
        "    exactly the unique off-diagonal interactions, preventing the inclusion of\n",
        "    identity-shifting diagonal terms.\n",
        "\n",
        "    Args:\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        q (float): The risk aversion parameter.\n",
        "        N (int): The number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[Tuple[int, int], float]]: A list of tuples containing the edge (i, j) and its beta coefficient.\n",
        "    \"\"\"\n",
        "    beta_coefficients: List[Tuple[Tuple[int, int], float]] = []\n",
        "\n",
        "    # Iterate over all unique pairs (i < j)\n",
        "    for i in range(N):\n",
        "        for j in range(i + 1, N):\n",
        "            # Compute the pairwise coefficient exactly per Eq. (5)\n",
        "            beta_ij = 2.0 * q * sigma_ann[i, j]\n",
        "            beta_coefficients.append(((i, j), float(beta_ij)))\n",
        "\n",
        "    # Mathematically assert the correct number of pairwise coefficients\n",
        "    expected_count = math.comb(N, 2)\n",
        "    if len(beta_coefficients) != expected_count:\n",
        "        raise ValueError(f\"Combinatorial violation: Expected {expected_count} beta coefficients, \"\n",
        "                         f\"but computed {len(beta_coefficients)}.\")\n",
        "\n",
        "    return beta_coefficients\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 3: Assemble the PennyLane Hamiltonian with exactly 55 terms and return it as H_C.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _assemble_cost_hamiltonian(alpha: np.ndarray, beta_list: List[Tuple[Tuple[int, int], float]], N: int) -> qml.Hamiltonian:\n",
        "    \"\"\"\n",
        "    Assembles the PennyLane Hamiltonian object for the cost function.\n",
        "\n",
        "    This function constructs the Ising Hamiltonian H_C = sum(alpha_i * Z_i) + sum(beta_ij * Z_i Z_j).\n",
        "    It rigorously validates that the resulting quantum object contains exactly the\n",
        "    expected number of terms (N + N choose 2).\n",
        "\n",
        "    Note: This Hamiltonian represents the proxy energy landscape for QAOA training.\n",
        "    Final portfolio selection relies on classical rescoring via Eq. (1).\n",
        "\n",
        "    Args:\n",
        "        alpha (np.ndarray): The linear coefficient vector.\n",
        "        beta_list (List[Tuple[Tuple[int, int], float]]): The list of pairwise coefficients.\n",
        "        N (int): The number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        qml.Hamiltonian: The instantiated cost Hamiltonian.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of terms in the Hamiltonian is incorrect.\n",
        "    \"\"\"\n",
        "    coeffs: List[float] = []\n",
        "    observables: List[Any] = []\n",
        "\n",
        "    # Add the linear terms: alpha_i * Z_i\n",
        "    for i in range(N):\n",
        "        coeffs.append(float(alpha[i]))\n",
        "        observables.append(qml.PauliZ(i))\n",
        "\n",
        "    # Add the quadratic terms: beta_ij * (Z_i @ Z_j)\n",
        "    for (i, j), beta_ij in beta_list:\n",
        "        coeffs.append(beta_ij)\n",
        "        observables.append(qml.PauliZ(i) @ qml.PauliZ(j))\n",
        "\n",
        "    # Instantiate the PennyLane Hamiltonian\n",
        "    h_c = qml.Hamiltonian(coeffs, observables)\n",
        "\n",
        "    # Calculate the expected number of terms: N linear + (N choose 2) quadratic\n",
        "    expected_terms = N + math.comb(N, 2)\n",
        "\n",
        "    # Extract the actual number of coefficients in the Hamiltonian\n",
        "    actual_terms = len(h_c.coeffs)\n",
        "\n",
        "    # Assert strict equality\n",
        "    if actual_terms != expected_terms:\n",
        "        raise ValueError(f\"Structural violation: Expected exactly {expected_terms} terms in the Cost Hamiltonian, \"\n",
        "                         f\"but observed {actual_terms}.\")\n",
        "\n",
        "    return h_c\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_cost_hamiltonian(mu_ann: np.ndarray, sigma_ann: np.ndarray, config: Dict[str, Any], s_prev: np.ndarray) -> qml.Hamiltonian:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of the regularized QAOA cost Hamiltonian.\n",
        "\n",
        "    This function translates the econometric parameters into the Ising model coefficients\n",
        "    defined in Equation (5). It applies the temporal regularization (continuity bonus)\n",
        "    to the linear terms, constructs the exact Pauli-Z tensor products, and rigorously\n",
        "    validates the structural integrity of the resulting quantum object.\n",
        "\n",
        "    Args:\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        s_prev (np.ndarray): The binary continuity indicator vector from the prior month.\n",
        "\n",
        "    Returns:\n",
        "        qml.Hamiltonian: The validated, canonical cost Hamiltonian for the current month.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the continuity application rule is unrecognized or structural invariants fail.\n",
        "    \"\"\"\n",
        "    logger.debug(\"Commencing cost Hamiltonian construction.\")\n",
        "\n",
        "    # Extract required parameters from the configuration\n",
        "    q = config[\"objective_function\"][\"risk_aversion_q\"]\n",
        "    kappa = config[\"objective_function\"][\"continuity_bonus_kappa\"]\n",
        "    rule = config[\"objective_function\"][\"continuity_bonus_application_rule\"]\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    N = len(universe)\n",
        "\n",
        "    # Assert the continuity rule matches the pinned convention\n",
        "    if rule != \"subtract_kappa_from_linear_term\":\n",
        "        raise ValueError(f\"Unrecognized continuity rule: '{rule}'. Expected 'subtract_kappa_from_linear_term'.\")\n",
        "\n",
        "    # Execute Step 1: Compute linear coefficients (alpha) and apply continuity bonus\n",
        "    alpha = _compute_linear_coefficients(mu_ann, q, s_prev, kappa, universe)\n",
        "\n",
        "    # Execute Step 2: Compute pairwise coefficients (beta)\n",
        "    beta_list = _compute_pairwise_coefficients(sigma_ann, q, N)\n",
        "\n",
        "    # Execute Step 3: Assemble and validate the PennyLane Hamiltonian\n",
        "    h_c = _assemble_cost_hamiltonian(alpha, beta_list, N)\n",
        "\n",
        "    logger.debug(f\"Successfully constructed Cost Hamiltonian with {len(h_c.coeffs)} terms.\")\n",
        "\n",
        "    # Return the canonical cost Hamiltonian artifact\n",
        "    return h_c\n"
      ],
      "metadata": {
        "id": "PU7MWEACOXHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Build Trotterized initialization and single-depth QAOA training callable (train_qaoa_single_depth)\n",
        "\n",
        "# ==================================================================================\n",
        "# Task 16: Build Trotterized initialization and single-depth QAOA training callable\n",
        "# ==================================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Generate initial parameters via Trotterized ramp.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_trotterized_parameters(p: int, config: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates the initial QAOA parameters using a deterministic Trotterized ramp.\n",
        "\n",
        "    This function strictly implements the discretized adiabatic schedule derived from\n",
        "    Equation (6). It computes a linear ramp between the configured bounds, ensuring\n",
        "    gamma monotonically increases and beta monotonically decreases. It explicitly\n",
        "    handles the p=1 degeneracy by returning the maximum bounds.\n",
        "\n",
        "    Args:\n",
        "        p (int): The circuit depth.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: The initial gamma and beta parameter arrays of length p.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the initialization rule is unrecognized.\n",
        "    \"\"\"\n",
        "    # Extract the pinned initialization rule and bounds\n",
        "    rule = config[\"qaoa_architecture\"][\"trotter_initialization_rule\"]\n",
        "    if rule != \"linear_ramp_between_bounds_per_depth\":\n",
        "        raise ValueError(f\"Unrecognized trotter rule: '{rule}'.\")\n",
        "\n",
        "    gamma_min, gamma_max = config[\"qaoa_architecture\"][\"qaoa_gamma_bounds\"]\n",
        "    beta_max, beta_min = config[\"qaoa_architecture\"][\"qaoa_beta_bounds\"]\n",
        "\n",
        "    gamma_init = np.zeros(p, dtype=np.float64)\n",
        "    beta_init = np.zeros(p, dtype=np.float64)\n",
        "\n",
        "    # Compute the linear ramp: l ranges from 1 to p\n",
        "    for index in range(p):\n",
        "        l = index + 1\n",
        "        # Equation: gamma_l = gamma_min + (l/p) * (gamma_max - gamma_min)\n",
        "        gamma_init[index] = gamma_min + (l / p) * (gamma_max - gamma_min)\n",
        "        # Equation: beta_l = beta_max - (l/p) * (beta_max - beta_min)\n",
        "        beta_init[index] = beta_max - (l / p) * (beta_max - beta_min)\n",
        "\n",
        "    # Log the initialization vectors for auditability\n",
        "    logger.debug(f\"Trotter Init (p={p}): gamma={np.round(gamma_init, 4).tolist()}, beta={np.round(beta_init, 4).tolist()}\")\n",
        "\n",
        "    return gamma_init, beta_init\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2: Construct the PennyLane QNode exactly.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _construct_qaoa_qnode(dicke_vector: np.ndarray, h_c: qml.Hamiltonian, h_xy: qml.Hamiltonian, p: int, N: int) -> qml.QNode:\n",
        "    \"\"\"\n",
        "    Constructs the exact PennyLane QNode for the constraint-preserving QAOA ansatz.\n",
        "\n",
        "    This function defines the quantum execution graph. It initializes the system in\n",
        "    the canonical Dicke state, applies p alternating layers of the Cost and Mixer\n",
        "    Hamiltonians using exactly 1 Trotter step, and returns the expectation value\n",
        "    of the Cost Hamiltonian.\n",
        "\n",
        "    Args:\n",
        "        dicke_vector (np.ndarray): The dense amplitude vector for |D^K_N>.\n",
        "        h_c (qml.Hamiltonian): The proxy cost Hamiltonian.\n",
        "        h_xy (qml.Hamiltonian): The constraint-preserving XY mixer Hamiltonian.\n",
        "        p (int): The circuit depth.\n",
        "        N (int): The number of qubits.\n",
        "\n",
        "    Returns:\n",
        "        qml.QNode: The callable quantum node.\n",
        "    \"\"\"\n",
        "    # Instantiate the statevector simulator device\n",
        "    dev = qml.device(\"default.qubit\", wires=N)\n",
        "\n",
        "    @qml.qnode(dev)\n",
        "    def qaoa_circuit(params: List[np.ndarray]) -> float:\n",
        "        gamma, beta = params[0], params[1]\n",
        "\n",
        "        # Step 1: Prepare the initial Dicke state\n",
        "        qml.QubitStateVector(dicke_vector, wires=range(N))\n",
        "\n",
        "        # Step 2: Apply p alternating layers of Cost and Mixer evolutions\n",
        "        for l in range(p):\n",
        "            # Apply Cost evolution: exp(-i * gamma_l * H_C)\n",
        "            qml.ApproxTimeEvolution(h_c, gamma[l], 1)\n",
        "            # Apply Mixer evolution: exp(-i * beta_l * H_XY)\n",
        "            qml.ApproxTimeEvolution(h_xy, beta[l], 1)\n",
        "\n",
        "        # Step 3: Measure the expectation value of the Cost Hamiltonian\n",
        "        return qml.expval(h_c)\n",
        "\n",
        "    return qaoa_circuit\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 3: Run Adam optimization with deterministic early stopping; log trajectory.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_adam_optimization(qnode: qml.QNode, initial_params: List[np.ndarray], config: Dict[str, Any]) -> Tuple[List[np.ndarray], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the Adam optimization loop with rigorous early stopping and optimized telemetry.\n",
        "\n",
        "    This function trains the QAOA parameters to minimize the expected cost. It utilizes\n",
        "    qml.math.value_and_grad to extract both the cost and the gradient in a single quantum\n",
        "    circuit evaluation, eliminating the severe computational overhead of redundant passes.\n",
        "    It manually applies the gradient update and logs the exact L2 gradient norm.\n",
        "\n",
        "    Args:\n",
        "        qnode (qml.QNode): The callable QAOA circuit.\n",
        "        initial_params (List[np.ndarray]): The initial [gamma, beta] arrays.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], Dict[str, Any]]: The optimized parameters and the comprehensive training log.\n",
        "    \"\"\"\n",
        "    # Extract optimization hyperparameters\n",
        "    opt_cfg = config[\"qaoa_optimization\"]\n",
        "    stepsize = opt_cfg[\"qaoa_stepsize\"]\n",
        "    epsilon = opt_cfg[\"qaoa_epsilon\"]\n",
        "    max_iterations = opt_cfg[\"qaoa_max_iterations\"]\n",
        "\n",
        "    # Extract early stopping parameters\n",
        "    es_cfg = opt_cfg[\"qaoa_early_stopping\"]\n",
        "    patience = es_cfg[\"patience\"]\n",
        "    min_delta = es_cfg[\"min_delta\"]\n",
        "\n",
        "    # Instantiate the PennyLane Adam optimizer\n",
        "    opt = qml.AdamOptimizer(stepsize=stepsize, eps=epsilon)\n",
        "\n",
        "    # Initialize state variables\n",
        "    params = initial_params\n",
        "    best_cost = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Initialize telemetry storage\n",
        "    training_log: Dict[str, Any] = {\n",
        "        \"cost_trajectory\": [],\n",
        "        \"gradient_norms\": [],\n",
        "        \"iterations_to_convergence\": max_iterations,\n",
        "        \"stopping_reason\": \"max_iterations_reached\",\n",
        "        \"final_expected_cost\": 0.0\n",
        "    }\n",
        "\n",
        "    # Define the single-pass value and gradient function\n",
        "    # This is the critical optimization that halves the quantum simulation overhead\n",
        "    val_and_grad_fn = qml.math.value_and_grad(qnode)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # Execute the single forward/backward pass\n",
        "        # cost is a scalar float, gradients is a tuple of arrays matching params\n",
        "        cost, gradients = val_and_grad_fn(params)\n",
        "\n",
        "        # Compute the L2 norm of the flattened gradient vector\n",
        "        # Equation: ||nabla||_2 = sqrt( sum(dC/dgamma)^2 + sum(dC/dbeta)^2 )\n",
        "        flat_grad = np.concatenate([np.ravel(g) for g in gradients])\n",
        "        grad_norm = float(np.linalg.norm(flat_grad))\n",
        "\n",
        "        # Store telemetry\n",
        "        training_log[\"cost_trajectory\"].append(float(cost))\n",
        "        training_log[\"gradient_norms\"].append(grad_norm)\n",
        "\n",
        "        # Manually apply the gradient update using the optimizer's step function\n",
        "        # This replaces the redundant opt.step_and_cost call\n",
        "        params = opt.apply_grad(gradients, params)\n",
        "\n",
        "        # Evaluate deterministic early stopping logic\n",
        "        if cost < best_cost - min_delta:\n",
        "            best_cost = cost\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            training_log[\"iterations_to_convergence\"] = i + 1\n",
        "            training_log[\"stopping_reason\"] = f\"early_stopping_triggered_at_patience_{patience}\"\n",
        "            logger.debug(f\"Early stopping triggered at iteration {i+1}. Best cost: {best_cost:.6f}\")\n",
        "            break\n",
        "\n",
        "    training_log[\"final_expected_cost\"] = float(best_cost)\n",
        "\n",
        "    return params, training_log\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def train_qaoa_single_depth(p: int, dicke_vector: np.ndarray, h_c: qml.Hamiltonian, h_xy: qml.Hamiltonian, config: Dict[str, Any]) -> Tuple[List[np.ndarray], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the Trotterized initialization and training of a single-depth QAOA circuit.\n",
        "\n",
        "    This function executes the Variational Quantum Algorithm (VQA) loop for a specific\n",
        "    circuit depth p. It generates deterministic initial parameters to mitigate Barren\n",
        "    Plateaus, constructs the exact constraint-preserving quantum execution graph, and\n",
        "    optimizes the parameters using Adam with rigorous early stopping. It returns the\n",
        "    optimized parameters and the comprehensive telemetry required for Table II diagnostics.\n",
        "\n",
        "    Args:\n",
        "        p (int): The circuit depth to train.\n",
        "        dicke_vector (np.ndarray): The canonical initial statevector.\n",
        "        h_c (qml.Hamiltonian): The proxy cost Hamiltonian.\n",
        "        h_xy (qml.Hamiltonian): The constraint-preserving XY mixer Hamiltonian.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], Dict[str, Any]]: The optimized [gamma, beta] parameters and the training log.\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Commencing QAOA training for depth p={p}.\")\n",
        "\n",
        "    # Extract the universe size to define the number of qubits\n",
        "    N = len(config[\"global_setup\"][\"universe\"])\n",
        "\n",
        "    # Execute Step 1: Generate initial parameters via Trotterized ramp\n",
        "    gamma_init, beta_init = _generate_trotterized_parameters(p, config)\n",
        "    initial_params = [gamma_init, beta_init]\n",
        "\n",
        "    # Execute Step 2: Construct the PennyLane QNode\n",
        "    qnode = _construct_qaoa_qnode(dicke_vector, h_c, h_xy, p, N)\n",
        "\n",
        "    # Execute Step 3: Run Adam optimization and collect telemetry\n",
        "    optimized_params, training_log = _execute_adam_optimization(qnode, initial_params, config)\n",
        "\n",
        "    logger.debug(f\"QAOA training for p={p} completed in {training_log['iterations_to_convergence']} iterations. \"\n",
        "                 f\"Final Cost: {training_log['final_expected_cost']:.6f}, \"\n",
        "                 f\"Final Grad Norm: {training_log['gradient_norms'][-1]:.6e}\")\n",
        "\n",
        "    # Return the trained parameters and the diagnostic artifact\n",
        "    return optimized_params, training_log\n"
      ],
      "metadata": {
        "id": "J3LjaucGPp9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Build QAOA readout, filtering, and classical rescoring callable (qaoa_readout_and_rescore)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Build QAOA readout, filtering, and classical rescoring callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1: Compute full probability vector Pr(x) from the trained circuit (statevector mode, no shots).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_full_probability_vector(trained_params: List[np.ndarray], dicke_vector: np.ndarray,\n",
        "                                     h_c: qml.Hamiltonian, h_xy: qml.Hamiltonian, p: int, N: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the exact probability distribution of the trained QAOA circuit.\n",
        "\n",
        "    This function constructs a dedicated readout QNode that executes the trained\n",
        "    ansatz and returns the full statevector probabilities. It strictly enforces the\n",
        "    pinned 'statevector_full_probs' measurement mode, ensuring no shot noise\n",
        "    corrupts the distribution.\n",
        "\n",
        "    Args:\n",
        "        trained_params (List[np.ndarray]): The optimized [gamma, beta] parameters.\n",
        "        dicke_vector (np.ndarray): The canonical initial statevector.\n",
        "        h_c (qml.Hamiltonian): The proxy cost Hamiltonian.\n",
        "        h_xy (qml.Hamiltonian): The constraint-preserving XY mixer Hamiltonian.\n",
        "        p (int): The circuit depth.\n",
        "        N (int): The number of qubits.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The exact probability vector of length 2^N.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the resulting probability vector is not normalized.\n",
        "    \"\"\"\n",
        "    # Instantiate the statevector simulator device explicitly without shots\n",
        "    dev = qml.device(\"default.qubit\", wires=N)\n",
        "\n",
        "    @qml.qnode(dev)\n",
        "    def readout_circuit(params: List[np.ndarray]) -> np.ndarray:\n",
        "        gamma, beta = params[0], params[1]\n",
        "\n",
        "        # Prepare the initial Dicke state\n",
        "        qml.QubitStateVector(dicke_vector, wires=range(N))\n",
        "\n",
        "        # Apply p alternating layers of Cost and Mixer evolutions\n",
        "        for l in range(p):\n",
        "            qml.ApproxTimeEvolution(h_c, gamma[l], 1)\n",
        "            qml.ApproxTimeEvolution(h_xy, beta[l], 1)\n",
        "\n",
        "        # Return the exact probability of each computational basis state\n",
        "        return qml.probs(wires=range(N))\n",
        "\n",
        "    # Execute the circuit with the trained parameters\n",
        "    prob_vector = readout_circuit(trained_params)\n",
        "\n",
        "    # Mathematically assert that the probability vector sums to 1.0\n",
        "    prob_sum = np.sum(prob_vector)\n",
        "    if not math.isclose(prob_sum, 1.0, abs_tol=1e-9):\n",
        "        raise ValueError(f\"Quantum mechanics violation: Probability vector sum is {prob_sum:.9f}, expected 1.0.\")\n",
        "\n",
        "    return prob_vector\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 2: Apply the two-stage readout filter: feasibility |x|=K and probability Pr(x) >= 1%.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_readout_filters(prob_vector: np.ndarray, K: int, N: int, prob_threshold: float) -> List[Tuple[np.ndarray, float, int]]:\n",
        "    \"\"\"\n",
        "    Applies the strict feasibility and probability filters using optimized bitwise operations.\n",
        "\n",
        "    This function iterates through the 2^N Hilbert space. It enforces the hard cardinality\n",
        "    constraint and the probability threshold. It replaces suboptimal string formatting with\n",
        "    high-performance bitwise shifts to construct the canonical binary numpy arrays directly\n",
        "    from the integer basis indices.\n",
        "\n",
        "    Args:\n",
        "        prob_vector (np.ndarray): The exact probability vector.\n",
        "        K (int): The cardinality constraint.\n",
        "        N (int): The number of qubits.\n",
        "        prob_threshold (float): The minimum probability required for retention.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[np.ndarray, float, int]]: A list of surviving candidates: (binary_vector, probability, basis_index).\n",
        "    \"\"\"\n",
        "    surviving_candidates: List[Tuple[np.ndarray, float, int]] = []\n",
        "    feasible_mass = 0.0\n",
        "    max_feasible_prob = 0.0\n",
        "\n",
        "    total_states = 2 ** N\n",
        "\n",
        "    # Pre-compute the array of bit shifts required for big-endian extraction\n",
        "    # e.g., for N=10: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
        "    shifts = np.arange(N - 1, -1, -1, dtype=np.int32)\n",
        "\n",
        "    for k in range(total_states):\n",
        "        # Filter 1: Feasibility (Hamming weight == K)\n",
        "        if k.bit_count() == K:\n",
        "            prob = float(prob_vector[k])\n",
        "            feasible_mass += prob\n",
        "\n",
        "            if prob > max_feasible_prob:\n",
        "                max_feasible_prob = prob\n",
        "\n",
        "            # Filter 2: Probability threshold (>= 1%)\n",
        "            if prob >= prob_threshold:\n",
        "                # Construct the canonical binary vector using optimized bitwise operations\n",
        "                # We right-shift the integer k by the pre-computed shifts and apply bitwise AND 1\n",
        "                x_vector = (k >> shifts) & 1\n",
        "                x_vector = x_vector.astype(np.int32)\n",
        "\n",
        "                surviving_candidates.append((x_vector, prob, k))\n",
        "\n",
        "    # Log the constraint-preservation diagnostics\n",
        "    logger.debug(f\"QAOA Readout: Feasible subspace mass M_K = {feasible_mass:.6f} (Expected ~1.0). \"\n",
        "                 f\"Max feasible prob = {max_feasible_prob:.6f}.\")\n",
        "\n",
        "    return surviving_candidates\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 3: Classically rescore candidates using Eq. (1) and select minimum-cost string with deterministic tie-break.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _classically_rescore_and_select(candidates: List[Tuple[np.ndarray, float, int]],\n",
        "                                    mu_ann: np.ndarray, sigma_ann: np.ndarray, q: float) -> Tuple[np.ndarray, float, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Classically rescores the surviving candidates using the true Markowitz objective (Eq. 1).\n",
        "\n",
        "    This function evaluates the full quadratic form (including diagonal variance) for\n",
        "    each candidate. It selects the candidate that minimizes this true classical cost,\n",
        "    employing a strict lexicographic tie-breaking rule to guarantee absolute determinism.\n",
        "\n",
        "    Args:\n",
        "        candidates (List[Tuple[np.ndarray, float, int]]): The filtered list of candidates.\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        q (float): The risk aversion parameter.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, float, List[Dict[str, Any]]]: The optimal binary vector, its classical cost, and the full diagnostic table.\n",
        "    \"\"\"\n",
        "    scored_candidates = []\n",
        "    diagnostic_table = []\n",
        "\n",
        "    for x_vector, prob, k in candidates:\n",
        "        # Evaluate the true classical objective (Eq. 1)\n",
        "        # f(x) = q * (x^T * Sigma * x) - (1-q) * (mu^T * x)\n",
        "        # This explicitly includes the diagonal elements of Sigma\n",
        "        variance_term = np.dot(x_vector, np.dot(sigma_ann, x_vector))\n",
        "        return_term = np.dot(mu_ann, x_vector)\n",
        "\n",
        "        classical_cost = (q * variance_term) - ((1.0 - q) * return_term)\n",
        "\n",
        "        scored_candidates.append((x_vector, float(classical_cost), prob, k))\n",
        "\n",
        "        # Append to the diagnostic table for institutional auditability\n",
        "        diagnostic_table.append({\n",
        "            \"basis_index\": k,\n",
        "            \"probability\": prob,\n",
        "            \"classical_cost\": float(classical_cost),\n",
        "            \"bitstring\": x_vector.tolist()\n",
        "        })\n",
        "\n",
        "    # Define a deterministic sort key: (classical_cost, lexicographic_tuple)\n",
        "    def sort_key(item: Tuple[np.ndarray, float, float, int]) -> Tuple[float, Tuple[int, ...]]:\n",
        "        x_vec, cost, _, _ = item\n",
        "        rounded_cost = round(cost, 12)\n",
        "        lexicographic_tuple = tuple(x_vec.tolist())\n",
        "        return (rounded_cost, lexicographic_tuple)\n",
        "\n",
        "    # Sort candidates to find the minimum classical cost\n",
        "    scored_candidates.sort(key=sort_key)\n",
        "\n",
        "    # Extract the optimal candidate\n",
        "    optimal_x, optimal_cost, optimal_prob, _ = scored_candidates[0]\n",
        "\n",
        "    logger.debug(f\"QAOA Rescoring Complete. Selected candidate prob: {optimal_prob:.4f}, Classical Cost: {optimal_cost:.6f}\")\n",
        "\n",
        "    return optimal_x, optimal_cost, diagnostic_table\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def qaoa_readout_and_rescore(trained_params: List[np.ndarray], dicke_vector: np.ndarray,\n",
        "                             h_c: qml.Hamiltonian, h_xy: qml.Hamiltonian, p: int,\n",
        "                             mu_ann: np.ndarray, sigma_ann: np.ndarray, config: Dict[str, Any]) -> Tuple[Optional[np.ndarray], Optional[float], List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the QAOA readout, feasibility filtering, and classical rescoring phase.\n",
        "\n",
        "    This function bridges the quantum simulation with the classical financial objective.\n",
        "    It computes the exact statevector probabilities, rigorously filters out infeasible\n",
        "    or low-probability states, and evaluates the surviving candidates against the true\n",
        "    Markowitz risk-return trade-off (Eq. 1). It returns the optimal deterministic selection.\n",
        "\n",
        "    Args:\n",
        "        trained_params (List[np.ndarray]): The optimized QAOA parameters.\n",
        "        dicke_vector (np.ndarray): The canonical initial statevector.\n",
        "        h_c (qml.Hamiltonian): The proxy cost Hamiltonian.\n",
        "        h_xy (qml.Hamiltonian): The constraint-preserving XY mixer Hamiltonian.\n",
        "        p (int): The circuit depth.\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[np.ndarray], Optional[float], List[Dict[str, Any]]]:\n",
        "            The optimal binary vector (or None), its classical cost (or None), and the diagnostic table.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the measurement mode is not 'statevector_full_probs'.\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Commencing QAOA readout and rescoring for depth p={p}.\")\n",
        "\n",
        "    # Extract required parameters from the configuration\n",
        "    measurement_mode = config[\"qaoa_architecture\"][\"qaoa_measurement_mode\"]\n",
        "    if measurement_mode != \"statevector_full_probs\":\n",
        "        raise ValueError(f\"Unsupported measurement mode: '{measurement_mode}'. Expected 'statevector_full_probs'.\")\n",
        "\n",
        "    K = config[\"objective_function\"][\"cardinality_K\"]\n",
        "    q = config[\"objective_function\"][\"risk_aversion_q\"]\n",
        "    prob_threshold = config[\"qaoa_optimization\"][\"qaoa_readout_filter_prob\"]\n",
        "    N = len(config[\"global_setup\"][\"universe\"])\n",
        "\n",
        "    # Execute Step 1: Compute the exact full probability vector\n",
        "    prob_vector = _compute_full_probability_vector(trained_params, dicke_vector, h_c, h_xy, p, N)\n",
        "\n",
        "    # Execute Step 2: Apply the feasibility and probability filters\n",
        "    surviving_candidates = _apply_readout_filters(prob_vector, K, N, prob_threshold)\n",
        "\n",
        "    # Handle the edge case: Zero candidates survived the filters\n",
        "    if not surviving_candidates:\n",
        "        logger.warning(f\"QAOA Readout (p={p}): ZERO candidates survived the {prob_threshold*100}% probability threshold.\")\n",
        "        return None, None, []\n",
        "\n",
        "    # Execute Step 3: Classically rescore and select the optimal candidate\n",
        "    optimal_x, optimal_cost, diagnostic_table = _classically_rescore_and_select(surviving_candidates, mu_ann, sigma_ann, q)\n",
        "\n",
        "    return optimal_x, optimal_cost, diagnostic_table\n"
      ],
      "metadata": {
        "id": "IdVhgnhQRXek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Build QAOA cross-depth best-selection callable (qaoa_select_best_across_depths)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Build QAOA cross-depth best-selection callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1: Iterate over depths p in {1..6} and run training + readout-rescoring, storing full diagnostics.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_depth_scan(qaoa_depths: List[int], dicke_vector: np.ndarray, h_c: qml.Hamiltonian, h_xy: qml.Hamiltonian,\n",
        "                        mu_ann: np.ndarray, sigma_ann: np.ndarray, config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the QAOA training and readout pipeline across a configured list of circuit depths.\n",
        "\n",
        "    This function iterates through each depth p, invoking the Trotterized training loop\n",
        "    (Task 16) and the classical rescoring readout (Task 17). It aggregates the optimized\n",
        "    parameters, classical costs, and comprehensive telemetry into a structured diagnostic\n",
        "    table required for reproducing the manuscript's Table II.\n",
        "\n",
        "    Args:\n",
        "        qaoa_depths (List[int]): The list of circuit depths to evaluate.\n",
        "        dicke_vector (np.ndarray): The canonical initial statevector.\n",
        "        h_c (qml.Hamiltonian): The proxy cost Hamiltonian.\n",
        "        h_xy (qml.Hamiltonian): The constraint-preserving XY mixer Hamiltonian.\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of diagnostic dictionaries, one for each evaluated depth.\n",
        "    \"\"\"\n",
        "    depth_diagnostics: List[Dict[str, Any]] = []\n",
        "\n",
        "    for p in qaoa_depths:\n",
        "        logger.debug(f\"--- Initiating QAOA pipeline for depth p={p} ---\")\n",
        "\n",
        "        # Execute Task 16: Train the circuit to obtain optimized parameters and telemetry\n",
        "        trained_params, training_log = train_qaoa_single_depth(p, dicke_vector, h_c, h_xy, config)\n",
        "\n",
        "        # Execute Task 17: Measure probabilities, filter, and classically rescore\n",
        "        optimal_x, optimal_cost, readout_table = qaoa_readout_and_rescore(\n",
        "            trained_params, dicke_vector, h_c, h_xy, p, mu_ann, sigma_ann, config\n",
        "        )\n",
        "\n",
        "        # Extract specific metrics required for Table II\n",
        "        final_expected_cost = training_log[\"final_expected_cost\"]\n",
        "        iterations = training_log[\"iterations_to_convergence\"]\n",
        "        # The gradient norm is the last element in the trajectory\n",
        "        final_grad_norm = training_log[\"gradient_norms\"][-1] if training_log[\"gradient_norms\"] else float('nan')\n",
        "\n",
        "        # Construct the comprehensive record for this depth\n",
        "        depth_record = {\n",
        "            \"p\": p,\n",
        "            \"trained_gamma\": trained_params[0].tolist(),\n",
        "            \"trained_beta\": trained_params[1].tolist(),\n",
        "            \"final_expected_cost\": final_expected_cost,\n",
        "            \"iterations_to_convergence\": iterations,\n",
        "            \"final_gradient_norm\": final_grad_norm,\n",
        "            \"best_candidate_x\": optimal_x,  # May be None if filters eliminated all states\n",
        "            \"best_classical_cost\": optimal_cost, # May be None\n",
        "            \"surviving_candidate_count\": len(readout_table),\n",
        "            \"full_training_log\": training_log,\n",
        "            \"full_readout_table\": readout_table\n",
        "        }\n",
        "\n",
        "        depth_diagnostics.append(depth_record)\n",
        "\n",
        "    return depth_diagnostics\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 2: Select the best candidate across depths by minimum classical cost f(x); break ties by smallest depth.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _select_optimal_depth_candidate(depth_diagnostics: List[Dict[str, Any]]) -> Tuple[Optional[int], Optional[np.ndarray], Optional[float]]:\n",
        "    \"\"\"\n",
        "    Selects the globally optimal candidate across all evaluated circuit depths.\n",
        "\n",
        "    This function filters out depths that failed to produce a valid candidate. It then\n",
        "    sorts the successful depths primarily by their true classical cost (Eq. 1) in ascending\n",
        "    order. To guarantee absolute determinism, it resolves cost ties (within a 1e-12 tolerance)\n",
        "    by selecting the shallower circuit (smaller p).\n",
        "\n",
        "    Args:\n",
        "        depth_diagnostics (List[Dict[str, Any]]): The complete list of per-depth diagnostic records.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[int], Optional[np.ndarray], Optional[float]]:\n",
        "            The winning depth p*, the optimal binary vector x*, and its classical cost.\n",
        "            Returns (None, None, None) if all depths failed.\n",
        "    \"\"\"\n",
        "    # Filter to include only depths that produced a valid candidate\n",
        "    valid_records = [record for record in depth_diagnostics if record[\"best_candidate_x\"] is not None]\n",
        "\n",
        "    # Handle the critical edge case: All depths failed\n",
        "    if not valid_records:\n",
        "        logger.warning(\"CRITICAL: QAOA solver produced ZERO valid candidates across all depths. \"\n",
        "                       \"Probability threshold may be too high. Triggering HRP fallback.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Define a deterministic sort key: (classical_cost, depth_p)\n",
        "    # Python sorts tuples element by element, perfectly implementing our tie-break logic\n",
        "    def sort_key(record: Dict[str, Any]) -> Tuple[float, int]:\n",
        "        cost = record[\"best_classical_cost\"]\n",
        "        p = record[\"p\"]\n",
        "        # Round cost to 12 decimal places to group floating-point near-ties\n",
        "        rounded_cost = round(cost, 12)\n",
        "        return (rounded_cost, p)\n",
        "\n",
        "    # Sort the valid records using the deterministic key\n",
        "    valid_records.sort(key=sort_key)\n",
        "\n",
        "    # The optimal record is the first element in the sorted list\n",
        "    optimal_record = valid_records[0]\n",
        "\n",
        "    p_star = optimal_record[\"p\"]\n",
        "    x_star = optimal_record[\"best_candidate_x\"]\n",
        "    cost_star = optimal_record[\"best_classical_cost\"]\n",
        "\n",
        "    return p_star, x_star, cost_star\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 3: Package and return x_t^{QAOA} and the complete depth-scaling diagnostics table.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _log_diagnostics_and_format_output(p_star: Optional[int], x_star: Optional[np.ndarray], cost_star: Optional[float],\n",
        "                                       universe: List[str]) -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Logs comprehensive diagnostic telemetry and returns the canonical selection vector.\n",
        "\n",
        "    This function maps the selected binary vector back to the canonical ticker symbols\n",
        "    for institutional auditability. It logs the winning depth and its classical cost.\n",
        "\n",
        "    Args:\n",
        "        p_star (Optional[int]): The winning circuit depth.\n",
        "        x_star (Optional[np.ndarray]): The selected optimal binary vector.\n",
        "        cost_star (Optional[float]): The classical cost of the selected vector.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Returns:\n",
        "        Optional[np.ndarray]: The canonical selection vector x_QAOA.\n",
        "    \"\"\"\n",
        "    if x_star is not None:\n",
        "        # Extract the indices of the selected assets\n",
        "        selected_indices = np.where(x_star == 1)[0]\n",
        "\n",
        "        # Map indices to ticker symbols\n",
        "        selected_tickers = [universe[i] for i in selected_indices]\n",
        "\n",
        "        logger.info(f\"QAOA Cross-Depth Selection Complete. Winning Depth: p*={p_star}\")\n",
        "        logger.info(f\"QAOA Optimal Classical Cost: {cost_star:.6f}\")\n",
        "        logger.info(f\"QAOA Selected Assets: {selected_tickers}\")\n",
        "    else:\n",
        "        logger.info(\"QAOA Cross-Depth Selection failed to produce a valid candidate.\")\n",
        "\n",
        "    return x_star\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def qaoa_select_best_across_depths(dicke_vector: np.ndarray, h_c: qml.Hamiltonian, h_xy: qml.Hamiltonian,\n",
        "                                   mu_ann: np.ndarray, sigma_ann: np.ndarray, config: Dict[str, Any]) -> Tuple[Optional[np.ndarray], List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the QAOA depth scan and optimal candidate selection.\n",
        "\n",
        "    This function executes the full quantum-classical hybrid loop for a configured list\n",
        "    of circuit depths. It aggregates the comprehensive telemetry required for Table II\n",
        "    diagnostics, deterministically selects the globally optimal candidate based on the\n",
        "    true Markowitz objective, and returns the canonical selection vector alongside the\n",
        "    full diagnostic table.\n",
        "\n",
        "    Args:\n",
        "        dicke_vector (np.ndarray): The canonical initial statevector.\n",
        "        h_c (qml.Hamiltonian): The proxy cost Hamiltonian.\n",
        "        h_xy (qml.Hamiltonian): The constraint-preserving XY mixer Hamiltonian.\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[np.ndarray], List[Dict[str, Any]]]:\n",
        "            The canonical binary selection vector x_QAOA (or None), and the complete depth-scaling diagnostics table.\n",
        "    \"\"\"\n",
        "    logger.info(\"Commencing QAOA cross-depth scan and optimal selection.\")\n",
        "\n",
        "    # Extract required parameters from the configuration\n",
        "    qaoa_depths = config[\"qaoa_architecture\"][\"qaoa_depths_p\"]\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "\n",
        "    # Execute Step 1: Run training and readout for all configured depths\n",
        "    depth_diagnostics = _execute_depth_scan(qaoa_depths, dicke_vector, h_c, h_xy, mu_ann, sigma_ann, config)\n",
        "\n",
        "    # Execute Step 2: Select the optimal candidate across all depths\n",
        "    p_star, x_star, cost_star = _select_optimal_depth_candidate(depth_diagnostics)\n",
        "\n",
        "    # Execute Step 3: Log diagnostics and format the final output\n",
        "    final_x_qaoa = _log_diagnostics_and_format_output(p_star, x_star, cost_star, universe)\n",
        "\n",
        "    # Return the canonical selection vector and the full Table II-grade diagnostic artifact\n",
        "    return final_x_qaoa, depth_diagnostics\n"
      ],
      "metadata": {
        "id": "7PYS5N-NSyHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Build SLSQP Sharpe-max allocation callable (allocate_sharpe_max)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Build SLSQP Sharpe-max allocation callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1: Define the constrained Sharpe-max optimization problem on the selected subset S.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _prepare_subset_optimization_problem(x_t: np.ndarray, mu_ann: np.ndarray, sigma_ann: np.ndarray,\n",
        "                                         config: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Extracts the subset parameters and defines the objective function constants.\n",
        "\n",
        "    This function isolates the K active assets identified by the selection vector.\n",
        "    By reducing the dimensionality from N to K, it significantly improves the stability\n",
        "    and speed of the continuous optimizer.\n",
        "\n",
        "    Args:\n",
        "        x_t (np.ndarray): The binary selection vector of shape (N,).\n",
        "        mu_ann (np.ndarray): The full annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The full annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
        "            The subset indices S, subset returns mu_S, subset covariance Sigma_S, and the risk-free rate.\n",
        "    \"\"\"\n",
        "    # Extract the integer indices of the selected assets (where x_i == 1)\n",
        "    S_indices = np.where(x_t == 1)[0]\n",
        "\n",
        "    # Slice the expected return vector to include only the selected assets\n",
        "    mu_S = mu_ann[S_indices]\n",
        "\n",
        "    # Slice the covariance matrix to include only the interactions between selected assets\n",
        "    # np.ix_ creates a meshgrid for advanced indexing of the 2D array\n",
        "    sigma_S = sigma_ann[np.ix_(S_indices, S_indices)]\n",
        "\n",
        "    # Extract the pinned risk-free rate from the configuration\n",
        "    r_f = config[\"allocation\"][\"risk_free_rate_annual\"]\n",
        "\n",
        "    return S_indices, mu_S, sigma_S, r_f\n",
        "\n",
        "def _negative_sharpe_objective(w_S: np.ndarray, mu_S: np.ndarray, sigma_S: np.ndarray, r_f: float) -> float:\n",
        "    \"\"\"\n",
        "    Computes the negative Sharpe ratio for the subset portfolio.\n",
        "\n",
        "    This is the objective function to be minimized by the SLSQP solver. It includes\n",
        "    a defensive guardrail against non-positive variance, which can occur if the solver\n",
        "    explores extreme or degenerate weight combinations.\n",
        "\n",
        "    Args:\n",
        "        w_S (np.ndarray): The subset weight vector.\n",
        "        mu_S (np.ndarray): The subset expected return vector.\n",
        "        sigma_S (np.ndarray): The subset covariance matrix.\n",
        "        r_f (float): The annualized risk-free rate.\n",
        "\n",
        "    Returns:\n",
        "        float: The negative Sharpe ratio, or a massive penalty if variance is invalid.\n",
        "    \"\"\"\n",
        "    # Compute portfolio expected return\n",
        "    port_return = np.dot(w_S, mu_S)\n",
        "\n",
        "    # Compute portfolio variance\n",
        "    port_variance = np.dot(w_S, np.dot(sigma_S, w_S))\n",
        "\n",
        "    # Defensive check: Ensure variance is strictly positive to prevent ZeroDivisionError or NaN\n",
        "    if port_variance <= 1e-12:\n",
        "        return 1e9  # Return a massive penalty to force the optimizer away from this region\n",
        "\n",
        "    port_volatility = np.sqrt(port_variance)\n",
        "\n",
        "    # Compute the Sharpe ratio\n",
        "    sharpe_ratio = (port_return - r_f) / port_volatility\n",
        "\n",
        "    # Return the negative Sharpe ratio for minimization\n",
        "    return -float(sharpe_ratio)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Define constraints/bounds/initialization and invoke SLSQP deterministically.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_slsqp_optimization(mu_S: np.ndarray, sigma_S: np.ndarray, r_f: float,\n",
        "                                K: int, bounds_cfg: Tuple[float, float]) -> OptimizeResult:\n",
        "    \"\"\"\n",
        "    Configures and executes the Sequential Least Squares Programming (SLSQP) solver.\n",
        "\n",
        "    This function enforces the pinned equality constraint (sum w_i = 1) and the box\n",
        "    constraints (w_min <= w_i <= w_max). It initializes the solver at the equally\n",
        "    weighted portfolio to guarantee a feasible starting point.\n",
        "\n",
        "    Args:\n",
        "        mu_S (np.ndarray): The subset expected return vector.\n",
        "        sigma_S (np.ndarray): The subset covariance matrix.\n",
        "        r_f (float): The annualized risk-free rate.\n",
        "        K (int): The cardinality constraint (number of active assets).\n",
        "        bounds_cfg (Tuple[float, float]): The (min, max) weight bounds.\n",
        "\n",
        "    Returns:\n",
        "        OptimizeResult: The raw result object from the SciPy optimizer.\n",
        "    \"\"\"\n",
        "    # Define the equality constraint: sum(w) - 1 = 0\n",
        "    constraints = [\n",
        "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
        "    ]\n",
        "\n",
        "    # Define the box bounds for each active asset\n",
        "    w_min, w_max = bounds_cfg\n",
        "    bounds = tuple((w_min, w_max) for _ in range(K))\n",
        "\n",
        "    # Initialize with the equally weighted portfolio (feasible by definition)\n",
        "    w_initial = np.full(K, 1.0 / K)\n",
        "\n",
        "    # Execute the SLSQP optimization\n",
        "    result = minimize(\n",
        "        fun=_negative_sharpe_objective,\n",
        "        x0=w_initial,\n",
        "        args=(mu_S, sigma_S, r_f),\n",
        "        method='SLSQP',\n",
        "        bounds=bounds,\n",
        "        constraints=constraints,\n",
        "        options={'ftol': 1e-9, 'disp': False}\n",
        "    )\n",
        "\n",
        "    return result\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 3: Validate the optimizer output, expand to full-universe weight vector, and return or fail.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_and_expand_weights(result: OptimizeResult, S_indices: np.ndarray, N: int,\n",
        "                                 bounds_cfg: Tuple[float, float]) -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Rigorously validates the solver output and expands it to the full universe dimension.\n",
        "\n",
        "    This function does not blindly trust the solver's success flag. It mathematically\n",
        "    re-verifies the sum-to-one constraint and the box bounds within a strict tolerance.\n",
        "    If valid, it scatters the subset weights back into a zero-initialized N-vector.\n",
        "\n",
        "    Args:\n",
        "        result (OptimizeResult): The raw result object from the SciPy optimizer.\n",
        "        S_indices (np.ndarray): The integer indices of the selected assets.\n",
        "        N (int): The total number of assets in the universe.\n",
        "        bounds_cfg (Tuple[float, float]): The (min, max) weight bounds.\n",
        "\n",
        "    Returns:\n",
        "        Optional[np.ndarray]: The expanded weight vector of shape (N,), or None if validation fails.\n",
        "    \"\"\"\n",
        "    # Check the solver's internal success flag\n",
        "    if not result.success:\n",
        "        logger.warning(f\"SLSQP Allocation Failed: Solver non-convergence. Message: {result.message}\")\n",
        "        return None\n",
        "\n",
        "    w_S = result.x\n",
        "\n",
        "    # Check for NaN values resulting from objective function singularities\n",
        "    if np.isnan(w_S).any():\n",
        "        logger.warning(\"SLSQP Allocation Failed: Solver returned NaN weights.\")\n",
        "        return None\n",
        "\n",
        "    # Re-verify the equality constraint: |sum(w) - 1| < 1e-8\n",
        "    if not np.isclose(np.sum(w_S), 1.0, atol=1e-8):\n",
        "        logger.warning(f\"SLSQP Allocation Failed: Sum constraint violated. Sum = {np.sum(w_S):.8f}\")\n",
        "        return None\n",
        "\n",
        "    # Re-verify the box bounds: w_min - tol <= w_i <= w_max + tol\n",
        "    w_min, w_max = bounds_cfg\n",
        "    if np.any(w_S < w_min - 1e-8) or np.any(w_S > w_max + 1e-8):\n",
        "        logger.warning(\"SLSQP Allocation Failed: Box bounds violated.\")\n",
        "        return None\n",
        "\n",
        "    # Expand the valid subset weights to the full N-dimensional universe\n",
        "    w_full = np.zeros(N, dtype=np.float64)\n",
        "    w_full[S_indices] = w_S\n",
        "\n",
        "    return w_full\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def allocate_sharpe_max(x_t: Optional[np.ndarray], mu_ann: np.ndarray, sigma_ann: np.ndarray, config: Dict[str, Any]) -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the continuous Sharpe-maximizing weight allocation for a selected subset.\n",
        "\n",
        "    This function translates the discrete selection vector into a constrained continuous\n",
        "    optimization problem. It isolates the active assets, executes the SLSQP solver to\n",
        "    find the optimal risk-adjusted weights, rigorously validates the constraints, and\n",
        "    expands the result back to the canonical universe dimension. If any step fails,\n",
        "    it gracefully returns None to trigger the HRP fallback.\n",
        "\n",
        "    Args:\n",
        "        x_t (Optional[np.ndarray]): The binary selection vector, or None if selection failed.\n",
        "        mu_ann (np.ndarray): The full annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The full annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Optional[np.ndarray]: The canonical weight vector w_t of shape (N,), or None on failure.\n",
        "    \"\"\"\n",
        "    # Handle the edge case where the discrete selector failed to provide a valid subset\n",
        "    if x_t is None:\n",
        "        logger.debug(\"Allocation skipped: Input selection vector is None.\")\n",
        "        return None\n",
        "\n",
        "    logger.debug(\"Commencing SLSQP Sharpe-max allocation.\")\n",
        "\n",
        "    # Extract required parameters from the configuration\n",
        "    K = config[\"objective_function\"][\"cardinality_K\"]\n",
        "    N = len(config[\"global_setup\"][\"universe\"])\n",
        "    bounds_cfg = config[\"capital_and_bounds\"][\"allocation_bounds\"]\n",
        "\n",
        "    # Defensive check: Ensure the selection vector has exactly K active assets\n",
        "    if np.sum(x_t) != K:\n",
        "        logger.warning(f\"Allocation Failed: Selection vector does not satisfy cardinality K={K}.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Extract subset parameters and define constants\n",
        "        S_indices, mu_S, sigma_S, r_f = _prepare_subset_optimization_problem(x_t, mu_ann, sigma_ann, config)\n",
        "\n",
        "        # Execute Step 2: Execute the SLSQP optimization\n",
        "        result = _execute_slsqp_optimization(mu_S, sigma_S, r_f, K, bounds_cfg)\n",
        "\n",
        "        # Execute Step 3: Validate the output and expand to the full universe\n",
        "        w_full = _validate_and_expand_weights(result, S_indices, N, bounds_cfg)\n",
        "\n",
        "        if w_full is not None:\n",
        "            logger.debug(\"SLSQP Sharpe-max allocation completed successfully.\")\n",
        "\n",
        "        return w_full\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected solver exceptions (e.g., LinAlgError inside SciPy)\n",
        "        logger.error(f\"SLSQP Allocation encountered a fatal exception: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "oxk-szrkUKax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Build HRP baseline computation callable (compute_hrp_weights)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Build HRP baseline computation callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1: Instantiate the HRP optimizer on the monthly lookback return matrix.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _instantiate_hrp_optimizer(returns_df: pd.DataFrame, universe: List[str]) -> HRPOpt:\n",
        "    \"\"\"\n",
        "    Instantiates the Hierarchical Risk Parity (HRP) optimizer using the lookback returns.\n",
        "\n",
        "    This function rigorously validates the input returns matrix to ensure it aligns\n",
        "    with the canonical universe and contains no missing or infinite values. It then\n",
        "    initializes the PyPortfolioOpt HRPOpt object, which will internally compute the\n",
        "    distance matrix and linkage dendrogram required for clustering.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix of shape (L-1, N).\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "\n",
        "    Returns:\n",
        "        HRPOpt: The instantiated Hierarchical Risk Parity optimizer object.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the returns matrix is misaligned, contains NaNs, or has infinite values.\n",
        "    \"\"\"\n",
        "    # Assert that the columns of the returns matrix exactly match the canonical universe\n",
        "    if returns_df.columns.tolist() != universe:\n",
        "        raise ValueError(\"HRP Instantiation Failed: Returns matrix columns do not match the canonical universe.\")\n",
        "\n",
        "    # Assert that the returns matrix contains exactly zero NaN values\n",
        "    if returns_df.isna().sum().sum() != 0:\n",
        "        raise ValueError(\"HRP Instantiation Failed: Returns matrix contains NaN values.\")\n",
        "\n",
        "    # Assert that all values in the returns matrix are finite real numbers\n",
        "    if not np.isfinite(returns_df.values).all():\n",
        "        raise ValueError(\"HRP Instantiation Failed: Returns matrix contains infinite values.\")\n",
        "\n",
        "    # Instantiate the HRPOpt object explicitly using the returns data\n",
        "    # This ensures the internal correlation and distance matrices are computed correctly\n",
        "    hrp_optimizer = HRPOpt(returns=returns_df)\n",
        "\n",
        "    return hrp_optimizer\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 2: Compute HRP weights and convert to a canonical N-vector in universe order.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_and_align_hrp_weights(hrp_optimizer: HRPOpt, universe: List[str], expected_N: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Executes the HRP optimization and aligns the resulting weights to the canonical universe.\n",
        "\n",
        "    This function triggers the quasi-diagonalization and recursive bisection algorithms.\n",
        "    It extracts the resulting ticker-to-weight dictionary and maps it into a dense,\n",
        "    strictly ordered numpy array, guaranteeing perfect alignment for downstream operations.\n",
        "\n",
        "    Args:\n",
        "        hrp_optimizer (HRPOpt): The instantiated HRP optimizer object.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The canonical HRP weight vector of shape (N,).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the optimizer drops an asset, resulting in a missing key.\n",
        "    \"\"\"\n",
        "    # Execute the HRP optimization algorithm to generate the weight dictionary\n",
        "    raw_weights_dict = hrp_optimizer.optimize()\n",
        "\n",
        "    # Initialize an empty list to hold the strictly ordered weights\n",
        "    aligned_weights_list: List[float] = []\n",
        "\n",
        "    # Iterate through the canonical universe to guarantee exact ordering\n",
        "    for ticker in universe:\n",
        "        # Attempt to retrieve the weight for the specific ticker\n",
        "        if ticker not in raw_weights_dict:\n",
        "            # Raise a fatal error if the optimizer silently dropped an asset\n",
        "            raise ValueError(f\"HRP Alignment Failed: Ticker '{ticker}' is missing from the optimizer output.\")\n",
        "\n",
        "        # Append the extracted weight to the aligned list\n",
        "        aligned_weights_list.append(raw_weights_dict[ticker])\n",
        "\n",
        "    # Convert the aligned list into a dense, double-precision numpy array\n",
        "    hrp_weights_vector = np.array(aligned_weights_list, dtype=np.float64)\n",
        "\n",
        "    # Assert the exact shape of the resulting vector\n",
        "    if hrp_weights_vector.shape != (expected_N,):\n",
        "        raise ValueError(f\"HRP Alignment Failed: Expected vector shape ({expected_N},), \"\n",
        "                         f\"but observed {hrp_weights_vector.shape}.\")\n",
        "\n",
        "    return hrp_weights_vector\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 3: Validate HRP weights and return as baseline and fallback artifact.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_hrp_weights(hrp_weights: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Performs rigorous mathematical validation on the canonical HRP weight vector.\n",
        "\n",
        "    This function asserts the fundamental properties of a fully invested, long-only\n",
        "    portfolio: strict non-negativity, sum-to-one (within floating-point tolerance),\n",
        "    and absolute finiteness.\n",
        "\n",
        "    Args:\n",
        "        hrp_weights (np.ndarray): The canonical HRP weight vector.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If non-negativity, sum-to-one, or finiteness invariants are violated.\n",
        "    \"\"\"\n",
        "    # Assert that all elements are finite real numbers\n",
        "    if not np.isfinite(hrp_weights).all():\n",
        "        raise ValueError(\"HRP Validation Failed: Weight vector contains infinite or NaN values.\")\n",
        "\n",
        "    # Assert strict non-negativity (allowing a microscopic tolerance for numerical noise)\n",
        "    if np.any(hrp_weights < -1e-10):\n",
        "        raise ValueError(\"HRP Validation Failed: Weight vector contains negative values.\")\n",
        "\n",
        "    # Calculate the sum of the weights\n",
        "    weights_sum = np.sum(hrp_weights)\n",
        "\n",
        "    # Assert the sum-to-one constraint using a strict absolute tolerance\n",
        "    if not np.isclose(weights_sum, 1.0, atol=1e-8):\n",
        "        raise ValueError(f\"HRP Validation Failed: Weights do not sum to 1.0. Observed sum: {weights_sum:.8f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_hrp_weights(returns_df: pd.DataFrame, config: Dict[str, Any]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of the Hierarchical Risk Parity (HRP) baseline weights.\n",
        "\n",
        "    This function serves as the entry point for generating the HRP allocation. It\n",
        "    instantiates the clustering optimizer using the causal lookback returns, executes\n",
        "    the recursive bisection algorithm, aligns the output to the canonical universe\n",
        "    ordering, and rigorously validates the mathematical properties of the final vector.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The validated, canonical HRP weight vector of shape (N,).\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the PyPortfolioOpt library encounters an internal failure.\n",
        "        ValueError: If structural, alignment, or mathematical invariants are violated.\n",
        "    \"\"\"\n",
        "    logger.debug(\"Commencing Hierarchical Risk Parity (HRP) weight computation.\")\n",
        "\n",
        "    # Extract the canonical universe and its expected length\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    expected_N = len(universe)\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Instantiate the HRP optimizer on the lookback returns\n",
        "        hrp_optimizer = _instantiate_hrp_optimizer(returns_df, universe)\n",
        "\n",
        "        # Execute Step 2: Compute the weights and align them to the canonical N-vector\n",
        "        hrp_weights = _compute_and_align_hrp_weights(hrp_optimizer, universe, expected_N)\n",
        "\n",
        "        # Execute Step 3: Validate the mathematical properties of the weight vector\n",
        "        _validate_hrp_weights(hrp_weights)\n",
        "\n",
        "        logger.debug(\"HRP weight computation and validation completed successfully.\")\n",
        "\n",
        "        # Return the canonical HRP weight artifact\n",
        "        return hrp_weights\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected exceptions from the underlying library and raise a descriptive error\n",
        "        logger.error(f\"Fatal error during HRP computation: {e}\")\n",
        "        raise RuntimeError(f\"Failed to compute HRP baseline weights: {e}\") from e\n"
      ],
      "metadata": {
        "id": "oV2Uevq5V19Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — Build allocation dispatcher with fallback callable (dispatch_allocation)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Build allocation dispatcher with fallback callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Attempt primary Sharpe-max allocation on the selector's chosen subset.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _attempt_primary_allocation(x_t: Optional[np.ndarray], mu_ann: np.ndarray, sigma_ann: np.ndarray,\n",
        "                                config: Dict[str, Any]) -> Tuple[Optional[np.ndarray], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Attempts to compute the primary Sharpe-maximizing continuous allocation.\n",
        "\n",
        "    This function evaluates the discrete selection vector. If valid, it invokes the\n",
        "    SLSQP optimizer (Task 19) to find the optimal weights within the [0.05, 0.50] bounds.\n",
        "    If the selection is missing or the optimizer fails, it returns None alongside a\n",
        "    deterministic failure reason code.\n",
        "\n",
        "    Args:\n",
        "        x_t (Optional[np.ndarray]): The binary selection vector, or None if selection failed.\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Optional[np.ndarray], Optional[str]]:\n",
        "            The intermediate weight vector (or None), and the failure reason code (or None).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input arrays suffer from dimensional misalignment.\n",
        "    \"\"\"\n",
        "    # Extract the canonical universe size\n",
        "    expected_N = len(config[\"global_setup\"][\"universe\"])\n",
        "\n",
        "    # Handle the case where the discrete solver failed to produce a valid subset\n",
        "    if x_t is None:\n",
        "        logger.debug(\"Primary allocation bypassed: Input selection vector is None.\")\n",
        "        return None, \"selection_none\"\n",
        "\n",
        "    # Assert strict dimensional alignment to prevent silent dot-product corruption\n",
        "    if x_t.shape != (expected_N,) or mu_ann.shape != (expected_N,) or sigma_ann.shape != (expected_N, expected_N):\n",
        "        raise ValueError(f\"Dimensionality violation in allocation dispatcher. Expected N={expected_N}. \"\n",
        "                         f\"Got x_t:{x_t.shape}, mu:{mu_ann.shape}, Sigma:{sigma_ann.shape}.\")\n",
        "\n",
        "    # Invoke the primary SLSQP Sharpe-max allocator (Task 19)\n",
        "    w_t = allocate_sharpe_max(x_t, mu_ann, sigma_ann, config)\n",
        "\n",
        "    # Evaluate the result of the primary allocation\n",
        "    if w_t is None:\n",
        "        # The allocator failed (e.g., non-convergence, invalid variance, bound violation)\n",
        "        logger.debug(\"Primary allocation failed during SLSQP optimization.\")\n",
        "        return None, \"allocation_failed\"\n",
        "\n",
        "    # Allocation succeeded\n",
        "    return w_t, None\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Handle failure via HRP fallback.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _handle_fallback_allocation(w_t: Optional[np.ndarray], failure_reason: Optional[str],\n",
        "                                returns_df: pd.DataFrame, config: Dict[str, Any],\n",
        "                                selector_name: str, rebalance_date: str) -> Tuple[np.ndarray, bool]:\n",
        "    \"\"\"\n",
        "    Evaluates the allocation state and triggers the Hierarchical Risk Parity fallback if necessary.\n",
        "\n",
        "    This function guarantees that the pipeline always produces a valid portfolio. If the\n",
        "    primary allocation yielded None, it invokes the HRP baseline (Task 20) using the\n",
        "    causal lookback returns. It logs the exact failure context for institutional auditability.\n",
        "\n",
        "    Args:\n",
        "        w_t (Optional[np.ndarray]): The intermediate weight vector from the primary allocator.\n",
        "        failure_reason (Optional[str]): The reason code if the primary allocation failed.\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        selector_name (str): The identifier of the strategy (e.g., 'QAOA_XY', 'SA').\n",
        "        rebalance_date (str): The ISO-formatted date of the current rebalance.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, bool]: The resolved weight vector and a boolean indicating if fallback was used.\n",
        "    \"\"\"\n",
        "    if w_t is not None:\n",
        "        # Primary allocation was successful; no fallback required\n",
        "        return w_t, False\n",
        "\n",
        "    # Primary allocation failed; trigger the fallback mechanism\n",
        "    logger.warning(f\"Fallback Triggered | Date: {rebalance_date} | Strategy: {selector_name} | \"\n",
        "                   f\"Reason: {failure_reason}. Computing HRP baseline weights.\")\n",
        "\n",
        "    # Invoke the HRP baseline computation (Task 20)\n",
        "    # This function is guaranteed to return a valid N-vector or raise a fatal RuntimeError\n",
        "    hrp_weights = compute_hrp_weights(returns_df, config)\n",
        "\n",
        "    return hrp_weights, True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Return final weights and assert validity.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_final_weights(w_t: np.ndarray, expected_N: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs universal, rigorous mathematical validation on the final portfolio weights.\n",
        "\n",
        "    This function asserts the fundamental invariants of the portfolio: full investment\n",
        "    (sum w_i = 1) and long-only constraints (w_i >= 0). It applies microscopic clipping\n",
        "    to eliminate floating-point noise below zero, ensuring pristine data for downstream\n",
        "    performance accounting.\n",
        "\n",
        "    Args:\n",
        "        w_t (np.ndarray): The resolved weight vector.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The validated and microscopically cleaned weight vector.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the weights violate the sum constraint, non-negativity, or finiteness.\n",
        "    \"\"\"\n",
        "    # Assert the exact shape of the vector\n",
        "    if w_t.shape != (expected_N,):\n",
        "        raise ValueError(f\"Final Weight Validation Failed: Expected shape ({expected_N},), got {w_t.shape}.\")\n",
        "\n",
        "    # Assert that all elements are finite real numbers\n",
        "    if not np.isfinite(w_t).all():\n",
        "        raise ValueError(\"Final Weight Validation Failed: Weight vector contains infinite or NaN values.\")\n",
        "\n",
        "    # Assert strict non-negativity (allowing a microscopic tolerance for numerical noise)\n",
        "    if np.any(w_t < -1e-10):\n",
        "        raise ValueError(\"Final Weight Validation Failed: Weight vector contains negative values exceeding tolerance.\")\n",
        "\n",
        "    # Clip microscopic negative noise exactly to 0.0 to prevent downstream accounting bugs\n",
        "    w_t_clean = np.clip(w_t, a_min=0.0, a_max=None)\n",
        "\n",
        "    # Calculate the sum of the cleaned weights\n",
        "    # Equation: sum_{i=1}^N w_i = 1\n",
        "    weights_sum = np.sum(w_t_clean)\n",
        "\n",
        "    # Assert the sum-to-one constraint using a strict absolute tolerance\n",
        "    if not np.isclose(weights_sum, 1.0, atol=1e-8):\n",
        "        raise ValueError(f\"Final Weight Validation Failed: Weights do not sum to 1.0. Observed sum: {weights_sum:.8f}\")\n",
        "\n",
        "    # Normalize by the sum to absorb any remaining floating-point drift, ensuring exact unity\n",
        "    w_t_final = w_t_clean / weights_sum\n",
        "\n",
        "    return w_t_final\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def dispatch_allocation(x_t: Optional[np.ndarray], mu_ann: np.ndarray, sigma_ann: np.ndarray,\n",
        "                        returns_df: pd.DataFrame, config: Dict[str, Any],\n",
        "                        selector_name: str, rebalance_date: pd.Timestamp) -> Tuple[np.ndarray, bool]:\n",
        "    \"\"\"\n",
        "    Orchestrates the portfolio allocation process with a robust HRP fallback mechanism.\n",
        "\n",
        "    This function acts as the central routing hub for continuous weight allocation. It\n",
        "    first attempts to solve the constrained Sharpe-maximization problem for the discrete\n",
        "    subset selected by the quantum or classical solver. If the selection is invalid or\n",
        "    the convex optimizer fails, it transparently degrades to the Hierarchical Risk Parity\n",
        "    baseline. It rigorously validates the final portfolio invariants before returning.\n",
        "\n",
        "    Args:\n",
        "        x_t (Optional[np.ndarray]): The binary selection vector from the discrete solver.\n",
        "        mu_ann (np.ndarray): The annualized expected return vector.\n",
        "        sigma_ann (np.ndarray): The annualized covariance matrix.\n",
        "        returns_df (pd.DataFrame): The canonical daily log-returns matrix (for HRP).\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        selector_name (str): The identifier of the strategy (e.g., 'QAOA_XY', 'SA').\n",
        "        rebalance_date (pd.Timestamp): The timestamp of the current rebalance.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, bool]:\n",
        "            The validated, canonical weight vector w_t of shape (N,), and a boolean\n",
        "            flag indicating whether the HRP fallback was triggered.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If both the primary allocation and the fallback mechanism fail.\n",
        "    \"\"\"\n",
        "    date_str = rebalance_date.date().isoformat()\n",
        "    logger.debug(f\"Commencing allocation dispatch for {selector_name} on {date_str}.\")\n",
        "\n",
        "    expected_N = len(config[\"global_setup\"][\"universe\"])\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Attempt the primary Sharpe-max allocation\n",
        "        w_t_primary, failure_reason = _attempt_primary_allocation(x_t, mu_ann, sigma_ann, config)\n",
        "\n",
        "        # Execute Step 2: Evaluate state and trigger HRP fallback if necessary\n",
        "        w_t_resolved, fallback_used = _handle_fallback_allocation(\n",
        "            w_t_primary, failure_reason, returns_df, config, selector_name, date_str\n",
        "        )\n",
        "\n",
        "        # Execute Step 3: Rigorously validate the final portfolio invariants\n",
        "        w_t_final = _validate_final_weights(w_t_resolved, expected_N)\n",
        "\n",
        "        logger.debug(f\"Allocation dispatch completed for {selector_name}. Fallback used: {fallback_used}.\")\n",
        "\n",
        "        return w_t_final, fallback_used\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected exceptions and raise a fatal error, as an unallocated month breaks the backtest\n",
        "        logger.error(f\"Fatal error during allocation dispatch for {selector_name} on {date_str}: {e}\")\n",
        "        raise RuntimeError(f\"Allocation dispatch failed completely for {selector_name}: {e}\") from e\n"
      ],
      "metadata": {
        "id": "1QTda6ZDdhvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22 — Build holding-period return and turnover computation callable (compute_holding_returns_and_turnover)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Build holding-period return and turnover computation callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Compute per-asset holding-period returns using the pinned \"price_relative\" convention.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_asset_holding_returns(cleaned_price_df: pd.DataFrame, t: pd.Timestamp, t_plus: Optional[pd.Timestamp],\n",
        "                                   config: Dict[str, Any]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the exact holding-period return for each asset between t and t^+.\n",
        "\n",
        "    This function enforces the pinned 'price_relative' convention. It handles the\n",
        "    critical edge case of the final backtest month (December) deterministically.\n",
        "    It utilizes the dedicated helper function to ensure clean, DRY timezone alignment.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical, cleansed price matrix.\n",
        "        t (pd.Timestamp): The current rebalance timestamp (entry).\n",
        "        t_plus (Optional[pd.Timestamp]): The next rebalance timestamp (exit), or None for the last month.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The vector of asset returns r_{t -> t^+} of shape (N,).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the required timestamps are missing from the price index.\n",
        "    \"\"\"\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    target_tz = cleaned_price_df.index.tz\n",
        "\n",
        "    # Align entry timestamp using the DRY helper function\n",
        "    t_aligned = _align_timestamp_timezone(t, target_tz)\n",
        "\n",
        "    # Resolve t_plus for the final month (December edge case)\n",
        "    if t_plus is None:\n",
        "        backtest_year = config[\"global_setup\"][\"backtest_year\"]\n",
        "        next_year_start = pd.Timestamp(year=backtest_year + 1, month=1, day=1)\n",
        "        next_year_start = _align_timestamp_timezone(next_year_start, target_tz)\n",
        "\n",
        "        valid_exit_dates = cleaned_price_df.index[cleaned_price_df.index >= next_year_start]\n",
        "        if len(valid_exit_dates) == 0:\n",
        "            raise ValueError(f\"Data coverage error: Cannot compute final holding return. \"\n",
        "                             f\"No trading days found on or after {next_year_start.date()}.\")\n",
        "        t_plus_aligned = valid_exit_dates[0]\n",
        "        logger.debug(f\"Resolved final month exit date (t^+) to {t_plus_aligned.date()}.\")\n",
        "    else:\n",
        "        # Align exit timestamp using the DRY helper function\n",
        "        t_plus_aligned = _align_timestamp_timezone(t_plus, target_tz)\n",
        "\n",
        "    try:\n",
        "        prices_t = cleaned_price_df.loc[t_aligned, universe].values.astype(np.float64)\n",
        "        prices_t_plus = cleaned_price_df.loc[t_plus_aligned, universe].values.astype(np.float64)\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Timestamp missing from price index during return computation: {e}\")\n",
        "\n",
        "    # Compute the price relative return\n",
        "    asset_returns = (prices_t_plus / prices_t) - 1.0\n",
        "\n",
        "    if not np.isfinite(asset_returns).all():\n",
        "        raise ValueError(\"Mathematical violation: Infinite or NaN values detected in asset holding returns.\")\n",
        "\n",
        "    return asset_returns\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Compute gross portfolio return as the dot product of weights and asset holding returns.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_gross_portfolio_return(w_t: np.ndarray, asset_returns: np.ndarray, expected_N: int) -> float:\n",
        "    \"\"\"\n",
        "    Computes the gross portfolio return via the linear combination of weights and asset returns.\n",
        "\n",
        "    This function mathematically asserts dimensional alignment before executing the\n",
        "    highly optimized numpy dot product, ensuring exact operational accounting.\n",
        "\n",
        "    Args:\n",
        "        w_t (np.ndarray): The validated portfolio weight vector.\n",
        "        asset_returns (np.ndarray): The vector of asset holding-period returns.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        float: The scalar gross portfolio return.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If dimensional alignment invariants are violated.\n",
        "    \"\"\"\n",
        "    # Assert strict dimensional alignment\n",
        "    if w_t.shape != (expected_N,) or asset_returns.shape != (expected_N,):\n",
        "        raise ValueError(f\"Dimensionality violation: Expected shape ({expected_N},). \"\n",
        "                         f\"Got w_t:{w_t.shape}, returns:{asset_returns.shape}.\")\n",
        "\n",
        "    # Compute the gross return: r^{gross}_t = sum(w_{i,t} * r_{i,t->t^+})\n",
        "    gross_return = np.dot(w_t, asset_returns)\n",
        "\n",
        "    return float(gross_return)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Compute turnover as L1 norm of monthly weight change; handle the first month deterministically.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_portfolio_turnover(w_t: np.ndarray, w_t_minus_1: np.ndarray, expected_N: int) -> float:\n",
        "    \"\"\"\n",
        "    Computes the portfolio turnover using the L1 norm of the weight vector differences.\n",
        "\n",
        "    This function strictly implements the manuscript's turnover definition. It handles\n",
        "    the initialization phase deterministically: if the previous weight vector is all zeros,\n",
        "    the turnover will naturally evaluate to 1.0 (assuming w_t sums to 1), accurately\n",
        "    reflecting the cost of establishing the initial positions.\n",
        "\n",
        "    Args:\n",
        "        w_t (np.ndarray): The current portfolio weight vector.\n",
        "        w_t_minus_1 (np.ndarray): The previous month's portfolio weight vector.\n",
        "        expected_N (int): The expected number of assets in the universe.\n",
        "\n",
        "    Returns:\n",
        "        float: The scalar portfolio turnover.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If dimensional alignment invariants are violated.\n",
        "    \"\"\"\n",
        "    # Assert strict dimensional alignment\n",
        "    if w_t.shape != (expected_N,) or w_t_minus_1.shape != (expected_N,):\n",
        "        raise ValueError(f\"Dimensionality violation: Expected shape ({expected_N},). \"\n",
        "                         f\"Got w_t:{w_t.shape}, w_t_minus_1:{w_t_minus_1.shape}.\")\n",
        "\n",
        "    # Compute the L1 norm: Turnover_t = sum(|w_{i,t} - w_{i,t-1}|)\n",
        "    weight_differences = w_t - w_t_minus_1\n",
        "    turnover = np.sum(np.abs(weight_differences))\n",
        "\n",
        "    return float(turnover)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_holding_returns_and_turnover(cleaned_price_df: pd.DataFrame, t: pd.Timestamp, t_plus: Optional[pd.Timestamp],\n",
        "                                         w_t: np.ndarray, w_t_minus_1: np.ndarray, config: Dict[str, Any]) -> Tuple[float, np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of the gross holding-period return and portfolio turnover.\n",
        "\n",
        "    This function acts as the operational accounting engine for a single rebalance period.\n",
        "    It extracts the exact entry and exit prices, computes the relative asset returns,\n",
        "    aggregates them into a gross portfolio return using the current weights, and calculates\n",
        "    the L1 norm turnover against the previous weights. It returns the complete set of\n",
        "    metrics required for net return calculation and diagnostic logging.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical, cleansed price matrix.\n",
        "        t (pd.Timestamp): The current rebalance timestamp (entry).\n",
        "        t_plus (Optional[pd.Timestamp]): The next rebalance timestamp (exit), or None for the last month.\n",
        "        w_t (np.ndarray): The current portfolio weight vector.\n",
        "        w_t_minus_1 (np.ndarray): The previous month's portfolio weight vector.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, np.ndarray, float]:\n",
        "            The gross portfolio return, the vector of individual asset returns, and the portfolio turnover.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the accounting computations fail due to missing data or dimensional errors.\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Commencing holding-period return and turnover computation for period starting {t.date()}.\")\n",
        "\n",
        "    expected_N = len(config[\"global_setup\"][\"universe\"])\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Compute per-asset holding-period returns\n",
        "        asset_returns = _compute_asset_holding_returns(cleaned_price_df, t, t_plus, config)\n",
        "\n",
        "        # Execute Step 2: Compute gross portfolio return\n",
        "        gross_return = _compute_gross_portfolio_return(w_t, asset_returns, expected_N)\n",
        "\n",
        "        # Execute Step 3: Compute portfolio turnover\n",
        "        turnover = _compute_portfolio_turnover(w_t, w_t_minus_1, expected_N)\n",
        "\n",
        "        logger.debug(f\"Accounting complete. Gross Return: {gross_return:.4%}, Turnover: {turnover:.4f}\")\n",
        "\n",
        "        return gross_return, asset_returns, turnover\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected exceptions and raise a fatal error, as accounting failure invalidates the backtest\n",
        "        logger.error(f\"Fatal error during performance accounting for period {t.date()}: {e}\")\n",
        "        raise RuntimeError(f\"Holding-period return computation failed: {e}\") from e\n"
      ],
      "metadata": {
        "id": "YWo7D8m6fif4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 — Build portfolio value path and net-return callable (`compute_net_return_and_value`)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Build portfolio value path and net-return callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 1: Compute net return by applying transaction costs proportional to turnover.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_net_return(gross_return: float, turnover: float, config: Dict[str, Any]) -> float:\n",
        "    \"\"\"\n",
        "    Computes the net portfolio return by penalizing the gross return with transaction costs.\n",
        "\n",
        "    This function strictly implements the manuscript's friction model:\n",
        "    r^{net}_t = r^{gross}_t - (tau * Turnover_t). It rigorously validates the transaction\n",
        "    cost parameter to ensure it is correctly specified as a decimal rate, preventing\n",
        "    catastrophic scaling errors.\n",
        "\n",
        "    Args:\n",
        "        gross_return (float): The scalar gross portfolio return.\n",
        "        turnover (float): The scalar portfolio turnover (L1 norm of weight changes).\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        float: The scalar net portfolio return.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the transaction cost parameter is invalid or mis-scaled.\n",
        "    \"\"\"\n",
        "    # Extract the transaction cost rate from the configuration\n",
        "    tau = config[\"evaluation_metrics\"][\"transaction_cost_bps\"]\n",
        "\n",
        "    # Assert that tau is a valid decimal rate (e.g., 5 bps = 0.0005)\n",
        "    # If tau >= 0.01 (100 bps or 1%), it is highly likely misconfigured as an integer\n",
        "    if not (0.0 <= tau < 0.01):\n",
        "        raise ValueError(f\"Configuration violation: transaction_cost_bps ({tau}) must be a decimal rate \"\n",
        "                         f\"in the interval [0.0, 0.01). Check configuration scaling.\")\n",
        "\n",
        "    # Compute the transaction cost penalty\n",
        "    friction_penalty = tau * turnover\n",
        "\n",
        "    # Compute the net return\n",
        "    net_return = gross_return - friction_penalty\n",
        "\n",
        "    return float(net_return)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 2: Update portfolio value via multiplicative compounding.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _update_portfolio_value(v_t_minus_1: float, net_return: float) -> float:\n",
        "    \"\"\"\n",
        "    Updates the portfolio's Net Asset Value (NAV) via geometric compounding.\n",
        "\n",
        "    This function strictly implements the wealth evolution equation:\n",
        "    V_t = V_{t-1} * (1 + r^{net}_t). It preserves full floating-point precision\n",
        "    without premature rounding.\n",
        "\n",
        "    Args:\n",
        "        v_t_minus_1 (float): The portfolio value at the end of the previous period.\n",
        "        net_return (float): The realized net return for the current period.\n",
        "\n",
        "    Returns:\n",
        "        float: The updated portfolio value V_t.\n",
        "    \"\"\"\n",
        "    # Compute the new portfolio value\n",
        "    v_t = v_t_minus_1 * (1.0 + net_return)\n",
        "\n",
        "    return float(v_t)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Validate V_t > 0, return (r_t^{net}, V_t), and append to strategy state upstream.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_and_log_wealth_state(v_t_minus_1: float, gross_return: float, turnover: float,\n",
        "                                   net_return: float, v_t: float, strategy_name: str) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Performs rigorous mathematical validation on the updated wealth state and logs telemetry.\n",
        "\n",
        "    This function asserts the survival of the portfolio (V_t > 0) and the finiteness\n",
        "    of the computed values. It logs the complete state transition for institutional\n",
        "    auditability.\n",
        "\n",
        "    Args:\n",
        "        v_t_minus_1 (float): The previous portfolio value.\n",
        "        gross_return (float): The gross return.\n",
        "        turnover (float): The portfolio turnover.\n",
        "        net_return (float): The net return.\n",
        "        v_t (float): The updated portfolio value.\n",
        "        strategy_name (str): The identifier of the strategy for logging context.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: The validated net return and updated portfolio value.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the portfolio value drops to zero or becomes infinite.\n",
        "    \"\"\"\n",
        "    # Assert that the portfolio value is a finite real number\n",
        "    if not math.isfinite(v_t):\n",
        "        raise ValueError(f\"Wealth accounting violation: Portfolio value became infinite or NaN for {strategy_name}.\")\n",
        "\n",
        "    # Assert the survival of the portfolio\n",
        "    # A value <= 0 indicates total loss of capital, invalidating further backtesting\n",
        "    if v_t <= 0.0:\n",
        "        raise ValueError(f\"Wealth accounting violation: Portfolio wiped out (V_t <= 0) for {strategy_name}. \"\n",
        "                         f\"V_t = {v_t:.2f}\")\n",
        "\n",
        "    # Log the complete state transition\n",
        "    logger.debug(f\"[{strategy_name}] Wealth Update | Prev V: ${v_t_minus_1:,.2f} | \"\n",
        "                 f\"Gross: {gross_return:.4%} | TO: {turnover:.4f} | \"\n",
        "                 f\"Net: {net_return:.4%} | New V: ${v_t:,.2f}\")\n",
        "\n",
        "    return net_return, v_t\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_net_return_and_value(gross_return: float, turnover: float, v_t_minus_1: float,\n",
        "                                 config: Dict[str, Any], strategy_name: str = \"UNKNOWN\") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of the net portfolio return and the updated wealth path.\n",
        "\n",
        "    This function acts as the final accounting step for a single rebalance period. It\n",
        "    applies the configured transaction cost model to the gross return based on realized\n",
        "    turnover, geometrically compounds the previous portfolio value, and rigorously\n",
        "    validates the survival and mathematical integrity of the resulting wealth state.\n",
        "\n",
        "    Args:\n",
        "        gross_return (float): The scalar gross portfolio return.\n",
        "        turnover (float): The scalar portfolio turnover.\n",
        "        v_t_minus_1 (float): The portfolio value at the end of the previous period.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        strategy_name (str): The identifier of the strategy (e.g., 'QAOA_XY', 'SA').\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "            The validated net return r^{net}_t and the updated portfolio value V_t.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the wealth accounting computations fail due to invalid inputs or state violations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Execute Step 1: Compute net return by applying transaction costs\n",
        "        net_return = _compute_net_return(gross_return, turnover, config)\n",
        "\n",
        "        # Execute Step 2: Update portfolio value via multiplicative compounding\n",
        "        v_t = _update_portfolio_value(v_t_minus_1, net_return)\n",
        "\n",
        "        # Execute Step 3: Validate the new wealth state and log telemetry\n",
        "        validated_net_return, validated_v_t = _validate_and_log_wealth_state(\n",
        "            v_t_minus_1, gross_return, turnover, net_return, v_t, strategy_name\n",
        "        )\n",
        "\n",
        "        return validated_net_return, validated_v_t\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected exceptions and raise a fatal error, as accounting failure invalidates the backtest\n",
        "        logger.error(f\"Fatal error during wealth accounting for {strategy_name}: {e}\")\n",
        "        raise RuntimeError(f\"Net return and value computation failed: {e}\") from e\n"
      ],
      "metadata": {
        "id": "8RW-rL3xgzS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24 — Build evaluation metrics and depth-scaling diagnostics callable (compute_all_metrics)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Build evaluation metrics and depth-scaling diagnostics callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 1: Compute Total Return and Annualized Volatility from monthly net return series and value path.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_return_and_volatility(net_returns: np.ndarray, value_path: np.ndarray, periods_per_year: int) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Computes the Total Return and Annualized Volatility for a given strategy.\n",
        "\n",
        "    This function strictly implements the manuscript's performance equations. It\n",
        "    enforces the use of the sample standard deviation (ddof=1) to provide an unbiased\n",
        "    estimator of volatility, preventing silent drift caused by default library behaviors.\n",
        "\n",
        "    Args:\n",
        "        net_returns (np.ndarray): The 1D array of monthly net returns (length 12).\n",
        "        value_path (np.ndarray): The 1D array of portfolio values (length 13, including V_0).\n",
        "        periods_per_year (int): The annualization scalar (e.g., 12 for monthly).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: The Total Return and Annualized Volatility.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input arrays have incorrect lengths or if V_0 is zero.\n",
        "    \"\"\"\n",
        "    # Assert the expected lengths for a 1-year monthly backtest\n",
        "    if len(net_returns) != 12:\n",
        "        raise ValueError(f\"Metrics computation failed: Expected exactly 12 net returns, got {len(net_returns)}.\")\n",
        "    if len(value_path) != 13:\n",
        "        raise ValueError(f\"Metrics computation failed: Expected exactly 13 value path entries, got {len(value_path)}.\")\n",
        "\n",
        "    v_initial = value_path[0]\n",
        "    v_final = value_path[-1]\n",
        "\n",
        "    # Defensive check against divide-by-zero\n",
        "    if v_initial <= 0.0:\n",
        "        raise ValueError(f\"Metrics computation failed: Initial portfolio value must be strictly positive, got {v_initial}.\")\n",
        "\n",
        "    # Compute Total Return: R_tot = (V_final / V_initial) - 1\n",
        "    total_return = (v_final / v_initial) - 1.0\n",
        "\n",
        "    # Compute Annualized Volatility: sigma_ann = std(returns) * sqrt(12)\n",
        "    # We explicitly pin ddof=1 to use the unbiased sample standard deviation\n",
        "    monthly_volatility = np.std(net_returns, ddof=1)\n",
        "    annualized_volatility = monthly_volatility * np.sqrt(periods_per_year)\n",
        "\n",
        "    return float(total_return), float(annualized_volatility)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 2: Compute Sharpe Ratio (manuscript policy) and Maximum Drawdown from the value path.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_sharpe_and_drawdown(net_returns: np.ndarray, value_path: np.ndarray,\n",
        "                                 annualized_volatility: float, periods_per_year: int) -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Computes the Sharpe Ratio and Maximum Drawdown for a given strategy.\n",
        "\n",
        "    This function strictly implements the pinned Sharpe Ratio formula from the manuscript\n",
        "    and calculates the exact peak-to-trough Maximum Drawdown using an optimized running\n",
        "    maximum algorithm.\n",
        "\n",
        "    Args:\n",
        "        net_returns (np.ndarray): The 1D array of monthly net returns.\n",
        "        value_path (np.ndarray): The 1D array of portfolio values.\n",
        "        annualized_volatility (float): The previously computed annualized volatility.\n",
        "        periods_per_year (int): The annualization scalar.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: The Sharpe Ratio and Maximum Drawdown.\n",
        "    \"\"\"\n",
        "    # Compute Sharpe Ratio: SR = (mean(returns) * 12) / sigma_ann\n",
        "    # Handle the edge case where volatility is zero (e.g., cash portfolio)\n",
        "    if annualized_volatility <= 1e-12:\n",
        "        logger.warning(\"Annualized volatility is near zero. Sharpe Ratio is undefined; returning NaN.\")\n",
        "        sharpe_ratio = float('nan')\n",
        "    else:\n",
        "        mean_monthly_return = np.mean(net_returns)\n",
        "        sharpe_ratio = (mean_monthly_return * periods_per_year) / annualized_volatility\n",
        "\n",
        "    # Compute Maximum Drawdown: MDD = min( (V_t / max_{s<=t} V_s) - 1 )\n",
        "    # np.maximum.accumulate efficiently computes the running maximum of the value path\n",
        "    running_max = np.maximum.accumulate(value_path)\n",
        "\n",
        "    # Compute the pointwise drawdown at each step\n",
        "    # We suppress divide-by-zero warnings here as V_t > 0 is guaranteed by Task 23\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        drawdowns = (value_path / running_max) - 1.0\n",
        "\n",
        "    # The Maximum Drawdown is the most negative value in the drawdown array\n",
        "    max_drawdown = np.min(drawdowns)\n",
        "\n",
        "    return float(sharpe_ratio), float(max_drawdown)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Assemble depth-scaling diagnostics table using a pinned aggregation rule across months.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _aggregate_depth_diagnostics(raw_diagnostics: List[List[Dict[str, Any]]], qaoa_depths: List[int]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Aggregates the monthly QAOA depth diagnostics into a single Table II-grade summary.\n",
        "\n",
        "    This function implements the pinned aggregation rule: for each depth p, it computes\n",
        "    the arithmetic mean of the final expected cost, iterations, and gradient norm across\n",
        "    all months where that depth successfully produced a candidate. It explicitly records\n",
        "    failure counts to prevent survivorship bias in the reporting.\n",
        "\n",
        "    Args:\n",
        "        raw_diagnostics (List[List[Dict[str, Any]]]): The nested list of monthly diagnostics [month][depth].\n",
        "        qaoa_depths (List[int]): The configured list of circuit depths to aggregate.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: The aggregated depth-scaling table (one dict per depth).\n",
        "    \"\"\"\n",
        "    aggregated_table: List[Dict[str, Any]] = []\n",
        "\n",
        "    for p in qaoa_depths:\n",
        "        costs = []\n",
        "        iterations = []\n",
        "        grad_norms = []\n",
        "        failure_count = 0\n",
        "\n",
        "        # Iterate through all 12 months\n",
        "        for month_idx, month_records in enumerate(raw_diagnostics):\n",
        "            # Find the record corresponding to depth p for this month\n",
        "            record_p = next((r for r in month_records if r[\"p\"] == p), None)\n",
        "\n",
        "            if record_p is None or record_p.get(\"best_candidate_x\") is None:\n",
        "                failure_count += 1\n",
        "                continue\n",
        "\n",
        "            costs.append(record_p[\"final_expected_cost\"])\n",
        "            iterations.append(record_p[\"iterations_to_convergence\"])\n",
        "\n",
        "            # Handle potential NaN gradient norms gracefully\n",
        "            gn = record_p[\"final_gradient_norm\"]\n",
        "            if not math.isnan(gn):\n",
        "                grad_norms.append(gn)\n",
        "\n",
        "        # Compute the arithmetic means, handling the case where all months failed\n",
        "        if len(costs) > 0:\n",
        "            mean_cost = float(np.mean(costs))\n",
        "            mean_iters = float(np.mean(iterations))\n",
        "            mean_grad = float(np.mean(grad_norms)) if grad_norms else float('nan')\n",
        "        else:\n",
        "            mean_cost = float('nan')\n",
        "            mean_iters = float('nan')\n",
        "            mean_grad = float('nan')\n",
        "\n",
        "        aggregated_table.append({\n",
        "            \"Depth (p)\": p,\n",
        "            \"Final Cost (Mean)\": mean_cost,\n",
        "            \"Iter. to Conv. (Mean)\": mean_iters,\n",
        "            \"Gradient Norm (Mean)\": mean_grad,\n",
        "            \"Months Failed\": failure_count\n",
        "        })\n",
        "\n",
        "    return aggregated_table\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_all_metrics(strategy_time_series: Dict[str, Dict[str, np.ndarray]],\n",
        "                        raw_qaoa_diagnostics: List[List[Dict[str, Any]]],\n",
        "                        config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of all financial evaluation metrics and quantum diagnostics.\n",
        "\n",
        "    This function processes the accumulated time-series data for all strategies (QAOA_XY, SA, HRP)\n",
        "    at the conclusion of the walk-forward backtest. It computes the canonical performance\n",
        "    metrics (Total Return, Volatility, Sharpe, MDD, Average Turnover) and aggregates the\n",
        "    QAOA depth-scaling telemetry into structured tables ready for persistence and visualization.\n",
        "\n",
        "    Args:\n",
        "        strategy_time_series (Dict[str, Dict[str, np.ndarray]]): A dictionary mapping strategy names\n",
        "            to their respective time-series arrays ('net_returns', 'value_path', 'turnovers').\n",
        "        raw_qaoa_diagnostics (List[List[Dict[str, Any]]]): The nested list of monthly QAOA diagnostics.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive results dictionary containing the metrics table and depth table.\n",
        "    \"\"\"\n",
        "    logger.info(\"Commencing final evaluation metrics and diagnostics computation.\")\n",
        "\n",
        "    periods_per_year = config[\"evaluation_metrics\"][\"periods_per_year\"]\n",
        "    qaoa_depths = config[\"qaoa_architecture\"][\"qaoa_depths_p\"]\n",
        "\n",
        "    final_results: Dict[str, Any] = {\n",
        "        \"financial_metrics\": {},\n",
        "        \"depth_scaling_table\": []\n",
        "    }\n",
        "\n",
        "    # Process financial metrics for each strategy\n",
        "    for strategy_name, series in strategy_time_series.items():\n",
        "        net_returns = series[\"net_returns\"]\n",
        "        value_path = series[\"value_path\"]\n",
        "        turnovers = series[\"turnovers\"]\n",
        "\n",
        "        # Execute Step 1: Compute Return and Volatility\n",
        "        tot_ret, ann_vol = _compute_return_and_volatility(net_returns, value_path, periods_per_year)\n",
        "\n",
        "        # Execute Step 2: Compute Sharpe and Drawdown\n",
        "        sharpe, mdd = _compute_sharpe_and_drawdown(net_returns, value_path, ann_vol, periods_per_year)\n",
        "\n",
        "        # Compute Average Monthly Turnover\n",
        "        avg_turnover = float(np.mean(turnovers))\n",
        "\n",
        "        # Store the metrics in the final results dictionary\n",
        "        final_results[\"financial_metrics\"][strategy_name] = {\n",
        "            \"Total Return\": tot_ret,\n",
        "            \"Annualized Volatility\": ann_vol,\n",
        "            \"Sharpe Ratio\": sharpe,\n",
        "            \"Max Drawdown\": mdd,\n",
        "            \"Average Monthly Turnover\": avg_turnover\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Metrics for {strategy_name}: SR={sharpe:.2f}, Ret={tot_ret:.2%}, MDD={mdd:.2%}, TO={avg_turnover:.2%}\")\n",
        "\n",
        "    # Execute Step 3: Assemble the depth-scaling diagnostics table\n",
        "    if raw_qaoa_diagnostics:\n",
        "        depth_table = _aggregate_depth_diagnostics(raw_qaoa_diagnostics, qaoa_depths)\n",
        "        final_results[\"depth_scaling_table\"] = depth_table\n",
        "        logger.info(\"Depth-scaling diagnostics table aggregated successfully.\")\n",
        "    else:\n",
        "        logger.warning(\"No raw QAOA diagnostics provided; skipping depth-scaling table aggregation.\")\n",
        "\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "Yz6J93_Xhm-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25 — Build artifact persistence callable (persist_artifacts)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Build artifact persistence callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 1: Persist raw and derived data artifacts with drift-resistant formats and deterministic naming.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _persist_data_artifacts(base_dir: Path, cleaned_price_df: pd.DataFrame, rebalance_dates: Tuple[pd.Timestamp, ...],\n",
        "                            mu_history: List[np.ndarray], sigma_history: List[np.ndarray]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Serializes the foundational data artifacts to disk using exact-precision formats.\n",
        "\n",
        "    This function writes the cleansed price matrix to Parquet (preserving float64 and DatetimeIndex),\n",
        "    the rebalance schedule to JSON, and the econometric estimators to binary .npy files.\n",
        "    This guarantees that the exact inputs to the solvers can be reloaded without numerical drift.\n",
        "\n",
        "    Args:\n",
        "        base_dir (Path): The root directory for the artifacts.\n",
        "        cleaned_price_df (pd.DataFrame): The canonical price matrix.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        mu_history (List[np.ndarray]): The chronological list of annualized expected return vectors.\n",
        "        sigma_history (List[np.ndarray]): The chronological list of annualized covariance matrices.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of absolute file paths written to disk.\n",
        "    \"\"\"\n",
        "    written_files: List[str] = []\n",
        "    data_dir = base_dir / \"data\"\n",
        "    data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1. Persist the cleansed price matrix as Parquet\n",
        "    price_path = data_dir / \"cleaned_prices.parquet\"\n",
        "    cleaned_price_df.to_parquet(price_path, engine=\"pyarrow\")\n",
        "    written_files.append(str(price_path))\n",
        "\n",
        "    # 2. Persist the rebalance dates as ISO 8601 strings in JSON\n",
        "    dates_path = data_dir / \"rebalance_dates.json\"\n",
        "    iso_dates = [d.isoformat() for d in rebalance_dates]\n",
        "    with open(dates_path, \"w\") as f:\n",
        "        json.dump(iso_dates, f, indent=4)\n",
        "    written_files.append(str(dates_path))\n",
        "\n",
        "    # 3. Persist the econometric estimators as binary numpy arrays\n",
        "    est_dir = data_dir / \"estimators\"\n",
        "    est_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for i, date in enumerate(rebalance_dates):\n",
        "        date_str = date.date().isoformat()\n",
        "\n",
        "        mu_path = est_dir / f\"mu_{date_str}.npy\"\n",
        "        np.save(mu_path, mu_history[i])\n",
        "        written_files.append(str(mu_path))\n",
        "\n",
        "        sigma_path = est_dir / f\"sigma_{date_str}.npy\"\n",
        "        np.save(sigma_path, sigma_history[i])\n",
        "        written_files.append(str(sigma_path))\n",
        "\n",
        "    logger.debug(f\"Persisted {len(written_files)} data artifacts to {data_dir}.\")\n",
        "    return written_files\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 2: Persist per-strategy decision artifacts: selections, QAOA depth scans, weights, and fallback log.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _persist_decision_artifacts(base_dir: Path, rebalance_dates: Tuple[pd.Timestamp, ...], universe: List[str],\n",
        "                                selections: Dict[str, List[np.ndarray]], weights: Dict[str, List[np.ndarray]],\n",
        "                                qaoa_diagnostics: List[List[Dict[str, Any]]], fallback_log: List[Dict[str, Any]]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Serializes the algorithmic decision outputs and quantum telemetry to disk.\n",
        "\n",
        "    This function writes the discrete selections as binary arrays, the continuous weights\n",
        "    as labeled Parquet DataFrames, and the highly nested QAOA depth-scan diagnostics as JSON.\n",
        "    This provides the complete audit trail required to reconstruct the portfolio evolution.\n",
        "\n",
        "    Args:\n",
        "        base_dir (Path): The root directory for the artifacts.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        universe (List[str]): The canonical list of asset tickers.\n",
        "        selections (Dict[str, List[np.ndarray]]): The binary selection vectors per strategy.\n",
        "        weights (Dict[str, List[np.ndarray]]): The continuous weight vectors per strategy.\n",
        "        qaoa_diagnostics (List[List[Dict[str, Any]]]): The nested QAOA telemetry.\n",
        "        fallback_log (List[Dict[str, Any]]): The record of HRP fallback events.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of absolute file paths written to disk.\n",
        "    \"\"\"\n",
        "    written_files: List[str] = []\n",
        "    decisions_dir = base_dir / \"decisions\"\n",
        "    decisions_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    date_index = pd.DatetimeIndex(rebalance_dates)\n",
        "\n",
        "    # 1. Persist Selections and Weights per strategy\n",
        "    for strategy in [\"QAOA_XY\", \"SA\", \"HRP\"]:\n",
        "        strat_dir = decisions_dir / strategy\n",
        "        strat_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # HRP does not have discrete selections\n",
        "        if strategy in selections and selections[strategy]:\n",
        "            sel_matrix = np.vstack(selections[strategy])\n",
        "            sel_df = pd.DataFrame(sel_matrix, index=date_index, columns=universe)\n",
        "            sel_path = strat_dir / \"selections.parquet\"\n",
        "            sel_df.to_parquet(sel_path, engine=\"pyarrow\")\n",
        "            written_files.append(str(sel_path))\n",
        "\n",
        "        if strategy in weights and weights[strategy]:\n",
        "            w_matrix = np.vstack(weights[strategy])\n",
        "            w_df = pd.DataFrame(w_matrix, index=date_index, columns=universe)\n",
        "            w_path = strat_dir / \"weights.parquet\"\n",
        "            w_df.to_parquet(w_path, engine=\"pyarrow\")\n",
        "            written_files.append(str(w_path))\n",
        "\n",
        "    # 2. Persist QAOA Depth Diagnostics\n",
        "    if qaoa_diagnostics:\n",
        "        qaoa_dir = decisions_dir / \"QAOA_XY\" / \"depth_scans\"\n",
        "        qaoa_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for i, date in enumerate(rebalance_dates):\n",
        "            date_str = date.date().isoformat()\n",
        "            diag_path = qaoa_dir / f\"scan_{date_str}.json\"\n",
        "            with open(diag_path, \"w\") as f:\n",
        "                # The arrays inside the dicts were converted to lists in Task 18\n",
        "                json.dump(qaoa_diagnostics[i], f, indent=4)\n",
        "            written_files.append(str(diag_path))\n",
        "\n",
        "    # 3. Persist Fallback Log\n",
        "    fallback_path = decisions_dir / \"fallback_log.json\"\n",
        "    with open(fallback_path, \"w\") as f:\n",
        "        json.dump(fallback_log, f, indent=4)\n",
        "    written_files.append(str(fallback_path))\n",
        "\n",
        "    logger.debug(f\"Persisted decision artifacts to {decisions_dir}.\")\n",
        "    return written_files\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Step 3: Persist evaluation outputs and software version manifest.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _persist_evaluation_and_manifest(base_dir: Path, final_results: Dict[str, Any],\n",
        "                                     strategy_time_series: Dict[str, Dict[str, np.ndarray]],\n",
        "                                     rebalance_dates: Tuple[pd.Timestamp, ...], exit_date: pd.Timestamp) -> List[str]:\n",
        "    \"\"\"\n",
        "    Serializes the final performance metrics, time-series, and the software environment manifest.\n",
        "\n",
        "    This function writes the human-readable CSV tables for the manuscript (Table II, Table III),\n",
        "    the exact time-series DataFrames for plotting, and a JSON manifest capturing the exact\n",
        "    versions of the critical libraries used, fulfilling the determinism contract.\n",
        "\n",
        "    Args:\n",
        "        base_dir (Path): The root directory for the artifacts.\n",
        "        final_results (Dict[str, Any]): The aggregated metrics and depth tables.\n",
        "        strategy_time_series (Dict[str, Dict[str, np.ndarray]]): The raw performance arrays.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        exit_date (pd.Timestamp): The final date for the value path.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of absolute file paths written to disk.\n",
        "    \"\"\"\n",
        "    written_files: List[str] = []\n",
        "    eval_dir = base_dir / \"evaluation\"\n",
        "    eval_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1. Persist Financial Metrics Table (Table III)\n",
        "    metrics_df = pd.DataFrame.from_dict(final_results[\"financial_metrics\"], orient=\"index\")\n",
        "    metrics_path = eval_dir / \"financial_metrics.csv\"\n",
        "    metrics_df.to_csv(metrics_path)\n",
        "    written_files.append(str(metrics_path))\n",
        "\n",
        "    # 2. Persist Depth Scaling Table (Table II)\n",
        "    if final_results.get(\"depth_scaling_table\"):\n",
        "        depth_df = pd.DataFrame(final_results[\"depth_scaling_table\"])\n",
        "        depth_path = eval_dir / \"depth_scaling_diagnostics.csv\"\n",
        "        depth_df.to_csv(depth_path, index=False)\n",
        "        written_files.append(str(depth_path))\n",
        "\n",
        "    # 3. Persist Time-Series Data\n",
        "    ts_dir = eval_dir / \"time_series\"\n",
        "    ts_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # The value path has 13 entries (including V_0), returns/turnovers have 12\n",
        "    # We align them by padding the first month of returns/turnovers with NaN\n",
        "    value_index = pd.DatetimeIndex(list(rebalance_dates) + [exit_date])\n",
        "\n",
        "    for strategy, series in strategy_time_series.items():\n",
        "        ts_df = pd.DataFrame({\n",
        "            \"Value_Path\": series[\"value_path\"],\n",
        "            \"Net_Return\": np.insert(series[\"net_returns\"], 0, np.nan),\n",
        "            \"Turnover\": np.insert(series[\"turnovers\"], 0, np.nan)\n",
        "        }, index=value_index)\n",
        "\n",
        "        ts_path = ts_dir / f\"{strategy}_timeseries.parquet\"\n",
        "        ts_df.to_parquet(ts_path, engine=\"pyarrow\")\n",
        "        written_files.append(str(ts_path))\n",
        "\n",
        "    # 4. Generate and Persist Software Manifest\n",
        "    manifest = {\n",
        "        \"python_version\": platform.python_version(),\n",
        "        \"os\": platform.system(),\n",
        "        \"libraries\": {}\n",
        "    }\n",
        "\n",
        "    # Create list of critical libraries\n",
        "    critical_libs = [\"numpy\", \"pandas\", \"scipy\", \"pennylane\", \"dimod\", \"dwave-neal\", \"PyPortfolioOpt\"]\n",
        "    for lib in critical_libs:\n",
        "        try:\n",
        "            manifest[\"libraries\"][lib] = importlib.metadata.version(lib)\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            manifest[\"libraries\"][lib] = \"NOT_FOUND\"\n",
        "\n",
        "    manifest_path = base_dir / \"software_manifest.json\"\n",
        "    with open(manifest_path, \"w\") as f:\n",
        "        json.dump(manifest, f, indent=4)\n",
        "    written_files.append(str(manifest_path))\n",
        "\n",
        "    logger.debug(f\"Persisted evaluation artifacts and manifest to {eval_dir}.\")\n",
        "    return written_files\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def persist_artifacts(base_dir_str: str, cleaned_price_df: pd.DataFrame, rebalance_dates: Tuple[pd.Timestamp, ...],\n",
        "                      exit_date: pd.Timestamp, mu_history: List[np.ndarray], sigma_history: List[np.ndarray],\n",
        "                      selections: Dict[str, List[np.ndarray]], weights: Dict[str, List[np.ndarray]],\n",
        "                      qaoa_diagnostics: List[List[Dict[str, Any]]], fallback_log: List[Dict[str, Any]],\n",
        "                      strategy_time_series: Dict[str, Dict[str, np.ndarray]], final_results: Dict[str, Any],\n",
        "                      config: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the comprehensive serialization of all study artifacts to disk.\n",
        "\n",
        "    This function freezes the entire state of the experiment. It writes the raw data,\n",
        "    the intermediate econometric parameters, the algorithmic decisions (selections and weights),\n",
        "    the quantum telemetry, the final performance metrics, and the software environment\n",
        "    manifest into a deterministic, highly structured directory tree. This guarantees\n",
        "    institutional-grade reproducibility.\n",
        "\n",
        "    Args:\n",
        "        base_dir_str (str): The root directory path for the artifacts.\n",
        "        cleaned_price_df (pd.DataFrame): The canonical price matrix.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        exit_date (pd.Timestamp): The final date for the value path.\n",
        "        mu_history (List[np.ndarray]): The chronological list of expected return vectors.\n",
        "        sigma_history (List[np.ndarray]): The chronological list of covariance matrices.\n",
        "        selections (Dict[str, List[np.ndarray]]): The binary selection vectors per strategy.\n",
        "        weights (Dict[str, List[np.ndarray]]): The continuous weight vectors per strategy.\n",
        "        qaoa_diagnostics (List[List[Dict[str, Any]]]): The nested QAOA telemetry.\n",
        "        fallback_log (List[Dict[str, Any]]): The record of HRP fallback events.\n",
        "        strategy_time_series (Dict[str, Dict[str, np.ndarray]]): The raw performance arrays.\n",
        "        final_results (Dict[str, Any]): The aggregated metrics and depth tables.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A comprehensive list of all absolute file paths written to disk.\n",
        "    \"\"\"\n",
        "    logger.info(f\"Commencing artifact persistence to root directory: {base_dir_str}\")\n",
        "    base_dir = Path(base_dir_str)\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    all_written_files: List[str] = []\n",
        "\n",
        "    try:\n",
        "        # Persist the master configuration itself\n",
        "        config_path = base_dir / \"master_configuration.json\"\n",
        "        with open(config_path, \"w\") as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "        all_written_files.append(str(config_path))\n",
        "\n",
        "        # Execute Step 1: Persist Data Artifacts\n",
        "        data_files = _persist_data_artifacts(base_dir, cleaned_price_df, rebalance_dates, mu_history, sigma_history)\n",
        "        all_written_files.extend(data_files)\n",
        "\n",
        "        # Execute Step 2: Persist Decision Artifacts\n",
        "        universe = config[\"global_setup\"][\"universe\"]\n",
        "        decision_files = _persist_decision_artifacts(base_dir, rebalance_dates, universe, selections, weights, qaoa_diagnostics, fallback_log)\n",
        "        all_written_files.extend(decision_files)\n",
        "\n",
        "        # Execute Step 3: Persist Evaluation Outputs and Manifest\n",
        "        eval_files = _persist_evaluation_and_manifest(base_dir, final_results, strategy_time_series, rebalance_dates, exit_date)\n",
        "        all_written_files.extend(eval_files)\n",
        "\n",
        "        logger.info(f\"Artifact persistence completed successfully. Wrote {len(all_written_files)} files.\")\n",
        "        return all_written_files\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Fatal error during artifact persistence: {e}\")\n",
        "        raise RuntimeError(f\"Failed to persist artifacts: {e}\") from e\n"
      ],
      "metadata": {
        "id": "hdHzVBB8iha0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26 — Build end-to-end orchestrator callable (run_full_backtest)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Build end-to-end orchestrator callable\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 1: Initialize state, seeds, and reusable quantum artifacts exactly once.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _initialize_backtest_state(config: Dict[str, Any]) -> Tuple[Dict[str, Any], np.ndarray, Any]:\n",
        "    \"\"\"\n",
        "    Initializes the random seeds, quantum invariants, and tracking state containers.\n",
        "\n",
        "    This function enforces the determinism contract by setting global seeds. It computes\n",
        "    the Dicke state vector and the XY mixer Hamiltonian exactly once, as they are invariant\n",
        "    across the walk-forward loop. It initializes the isolated state tracking dictionaries\n",
        "    for each strategy to prevent cross-contamination.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, Any], np.ndarray, Any]:\n",
        "            The state tracking dictionary, the Dicke statevector, and the XY mixer Hamiltonian.\n",
        "    \"\"\"\n",
        "    # Enforce the determinism contract\n",
        "    global_seed = config[\"randomness\"][\"global_seed\"]\n",
        "    random.seed(global_seed)\n",
        "    np.random.seed(global_seed)\n",
        "    logger.info(f\"Global random seeds set to {global_seed}.\")\n",
        "\n",
        "    N = len(config[\"global_setup\"][\"universe\"])\n",
        "    K = config[\"objective_function\"][\"cardinality_K\"]\n",
        "    initial_capital = config[\"capital_and_bounds\"][\"initial_capital\"]\n",
        "\n",
        "    # Construct the quantum invariants (built once, reused across all months)\n",
        "    dicke_vector = prepare_dicke_state_vector(N, K)\n",
        "    h_xy = build_xy_mixer_hamiltonian(N, config)\n",
        "    logger.info(\"Quantum invariants (Dicke state, XY mixer) initialized.\")\n",
        "\n",
        "    # Initialize isolated state containers for each strategy\n",
        "    strategies = [\"QAOA_XY\", \"SA\", \"HRP\"]\n",
        "    state: Dict[str, Any] = {\n",
        "        \"weights\": {s: [] for s in strategies},\n",
        "        \"selections\": {s: [] for s in [\"QAOA_XY\", \"SA\"]}, # HRP does not have discrete selections\n",
        "        \"net_returns\": {s: [] for s in strategies},\n",
        "        \"turnovers\": {s: [] for s in strategies},\n",
        "        \"value_paths\": {s: [initial_capital] for s in strategies},\n",
        "\n",
        "        # Current state variables (updated iteratively)\n",
        "        \"current_weights\": {s: np.zeros(N, dtype=np.float64) for s in strategies},\n",
        "        \"current_selections\": {s: None for s in [\"QAOA_XY\", \"SA\"]},\n",
        "\n",
        "        # Diagnostic tracking\n",
        "        \"qaoa_diagnostics\": [],\n",
        "        \"fallback_log\": [],\n",
        "        \"mu_history\": [],\n",
        "        \"sigma_history\": []\n",
        "    }\n",
        "\n",
        "    return state, dicke_vector, h_xy\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 2: Execute the monthly loop exactly as Algorithm 1.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_walk_forward_loop(cleaned_price_df: pd.DataFrame, rebalance_dates: Tuple[pd.Timestamp, ...],\n",
        "                               state: Dict[str, Any], dicke_vector: np.ndarray, h_xy: Any,\n",
        "                               config: Dict[str, Any]) -> pd.Timestamp:\n",
        "    \"\"\"\n",
        "    Executes the core monthly walk-forward backtest loop defined in Algorithm 1.\n",
        "\n",
        "    This function orchestrates the sequential execution of data extraction, econometric\n",
        "    estimation, discrete selection, continuous allocation, and performance accounting.\n",
        "    It strictly enforces causality and updates the isolated state containers iteratively.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical price matrix.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        state (Dict[str, Any]): The initialized state tracking dictionary.\n",
        "        dicke_vector (np.ndarray): The canonical initial statevector.\n",
        "        h_xy (Any): The constraint-preserving XY mixer Hamiltonian.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.Timestamp: The final exit date (t^+) used for the last holding period.\n",
        "    \"\"\"\n",
        "    L = config[\"global_setup\"][\"lookback_window_L\"]\n",
        "    universe = config[\"global_setup\"][\"universe\"]\n",
        "    N = len(universe)\n",
        "\n",
        "    final_exit_date = None\n",
        "\n",
        "    for m, t in enumerate(rebalance_dates):\n",
        "        logger.info(f\"=== Executing Rebalance Month {m+1}/12: {t.date()} ===\")\n",
        "\n",
        "        t_plus = rebalance_dates[m+1] if m < len(rebalance_dates) - 1 else None\n",
        "\n",
        "        # 1. Data Extraction & Econometrics\n",
        "        window_df = extract_lookback_window(cleaned_price_df, t, L, universe)\n",
        "        returns_df = compute_daily_log_returns(window_df, config)\n",
        "        mu_ann = estimate_mu_annualized(returns_df, config)\n",
        "        sigma_ann = estimate_sigma_annualized(returns_df, config)\n",
        "\n",
        "        state[\"mu_history\"].append(mu_ann)\n",
        "        state[\"sigma_history\"].append(sigma_ann)\n",
        "\n",
        "        # =========================================================================================\n",
        "        # NOTE ON CONTINUITY STATE AND FALLBACKS:\n",
        "        # The continuity indicator (s_prev) is derived from the prior month's discrete selection.\n",
        "        # If a solver (e.g., QAOA) failed in month t-1 and fell back to HRP, its selection\n",
        "        # state is None. Consequently, s_prev for month t will correctly initialize to a\n",
        "        # zero vector, meaning no continuity bonus is applied. This strictly adheres to the\n",
        "        # manuscript's logic: you cannot receive a bonus for holding an asset if the discrete\n",
        "        # solver failed to select a valid subset in the prior period.\n",
        "        # =========================================================================================\n",
        "\n",
        "        # 2. Continuity State Updates\n",
        "        s_prev_sa = update_continuity_state(state[\"current_selections\"][\"SA\"], config, \"SA\")\n",
        "        s_prev_qaoa = update_continuity_state(state[\"current_selections\"][\"QAOA_XY\"], config, \"QAOA_XY\")\n",
        "\n",
        "        # 3. Simulated Annealing (SA) Selection\n",
        "        qubo = build_qubo_matrix(mu_ann, sigma_ann, config, s_prev_sa)\n",
        "        sa_samples = run_sa_sampling(qubo, N, config)\n",
        "        x_sa = filter_sa_samples(sa_samples, config)\n",
        "        state[\"current_selections\"][\"SA\"] = x_sa\n",
        "        if x_sa is not None:\n",
        "            state[\"selections\"][\"SA\"].append(x_sa)\n",
        "\n",
        "        # 4. QAOA-XY Selection\n",
        "        h_c = build_cost_hamiltonian(mu_ann, sigma_ann, config, s_prev_qaoa)\n",
        "        x_qaoa, qaoa_diags = qaoa_select_best_across_depths(dicke_vector, h_c, h_xy, mu_ann, sigma_ann, config)\n",
        "        state[\"current_selections\"][\"QAOA_XY\"] = x_qaoa\n",
        "        state[\"qaoa_diagnostics\"].append(qaoa_diags)\n",
        "        if x_qaoa is not None:\n",
        "            state[\"selections\"][\"QAOA_XY\"].append(x_qaoa)\n",
        "\n",
        "        # 5. Allocation & Accounting for each strategy\n",
        "        for strategy in [\"QAOA_XY\", \"SA\", \"HRP\"]:\n",
        "            if strategy == \"HRP\":\n",
        "                w_t = compute_hrp_weights(returns_df, config)\n",
        "                fallback_used = False\n",
        "            else:\n",
        "                x_t = state[\"current_selections\"][strategy]\n",
        "                w_t, fallback_used = dispatch_allocation(x_t, mu_ann, sigma_ann, returns_df, config, strategy, t)\n",
        "                if fallback_used:\n",
        "                    state[\"fallback_log\"].append({\"month\": t.date().isoformat(), \"strategy\": strategy})\n",
        "\n",
        "            state[\"weights\"][strategy].append(w_t)\n",
        "\n",
        "            w_t_minus_1 = state[\"current_weights\"][strategy]\n",
        "            gross_ret, asset_rets, turnover = compute_holding_returns_and_turnover(cleaned_price_df, t, t_plus, w_t, w_t_minus_1, config)\n",
        "\n",
        "            v_t_minus_1 = state[\"value_paths\"][strategy][-1]\n",
        "            net_ret, v_t = compute_net_return_and_value(gross_return=gross_ret, turnover=turnover, v_t_minus_1=v_t_minus_1, config=config, strategy_name=strategy)\n",
        "\n",
        "            state[\"current_weights\"][strategy] = w_t\n",
        "            state[\"net_returns\"][strategy].append(net_ret)\n",
        "            state[\"turnovers\"][strategy].append(turnover)\n",
        "            state[\"value_paths\"][strategy].append(v_t)\n",
        "\n",
        "        if t_plus is None:\n",
        "            target_tz = cleaned_price_df.index.tz\n",
        "            next_year_start = pd.Timestamp(year=config[\"global_setup\"][\"backtest_year\"] + 1, month=1, day=1)\n",
        "            next_year_start = _align_timestamp_timezone(next_year_start, target_tz)\n",
        "            valid_exit_dates = cleaned_price_df.index[cleaned_price_df.index >= next_year_start]\n",
        "            final_exit_date = valid_exit_dates[0]\n",
        "\n",
        "    return final_exit_date\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Finalize: compute metrics, persist artifacts, and return full results bundle.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _finalize_and_persist(state: Dict[str, Any], cleaned_price_df: pd.DataFrame, rebalance_dates: Tuple[pd.Timestamp, ...],\n",
        "                          final_exit_date: pd.Timestamp, config: Dict[str, Any], output_dir: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Finalizes the backtest by computing aggregate metrics and persisting all artifacts.\n",
        "\n",
        "    This function packages the raw chronological lists into numpy arrays, invokes the\n",
        "    metrics calculator (Task 24) to generate the final performance tables, and calls\n",
        "    the persistence layer (Task 25) to freeze the experiment's state to disk.\n",
        "\n",
        "    Args:\n",
        "        state (Dict[str, Any]): The populated state tracking dictionary.\n",
        "        cleaned_price_df (pd.DataFrame): The canonical price matrix.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        final_exit_date (pd.Timestamp): The final date for the value path.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        output_dir (str): The target directory for artifact persistence.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The comprehensive results bundle.\n",
        "    \"\"\"\n",
        "    # Package time-series data for the metrics calculator\n",
        "    strategy_time_series = {}\n",
        "    for strategy in [\"QAOA_XY\", \"SA\", \"HRP\"]:\n",
        "        strategy_time_series[strategy] = {\n",
        "            \"net_returns\": np.array(state[\"net_returns\"][strategy]),\n",
        "            \"value_path\": np.array(state[\"value_paths\"][strategy]),\n",
        "            \"turnovers\": np.array(state[\"turnovers\"][strategy])\n",
        "        }\n",
        "\n",
        "    # Compute final metrics and depth-scaling tables\n",
        "    final_results = compute_all_metrics(strategy_time_series, state[\"qaoa_diagnostics\"], config)\n",
        "\n",
        "    # Persist all artifacts to disk\n",
        "    try:\n",
        "        written_files = persist_artifacts(\n",
        "            base_dir_str=output_dir,\n",
        "            cleaned_price_df=cleaned_price_df,\n",
        "            rebalance_dates=rebalance_dates,\n",
        "            exit_date=final_exit_date,\n",
        "            mu_history=state[\"mu_history\"],\n",
        "            sigma_history=state[\"sigma_history\"],\n",
        "            selections=state[\"selections\"],\n",
        "            weights=state[\"weights\"],\n",
        "            qaoa_diagnostics=state[\"qaoa_diagnostics\"],\n",
        "            fallback_log=state[\"fallback_log\"],\n",
        "            strategy_time_series=strategy_time_series,\n",
        "            final_results=final_results,\n",
        "            config=config\n",
        "        )\n",
        "        final_results[\"persisted_files\"] = written_files\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Artifact persistence failed, but returning in-memory results. Error: {e}\")\n",
        "        final_results[\"persisted_files\"] = []\n",
        "\n",
        "    # Attach raw state for downstream programmatic access\n",
        "    final_results[\"raw_state\"] = state\n",
        "\n",
        "    return final_results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 26, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_full_backtest(cleaned_price_df: pd.DataFrame, rebalance_dates: Tuple[pd.Timestamp, ...],\n",
        "                      config: Dict[str, Any], output_dir: str = \"./backtest_results\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end execution of the hybrid quantum-classical backtest.\n",
        "\n",
        "    This is the master execution function. It initializes the simulation environment,\n",
        "    executes the rigorous monthly walk-forward loop defined in Algorithm 1, computes\n",
        "    the final institutional-grade performance metrics, and serializes the entire\n",
        "    experimental state to disk for absolute reproducibility.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical, cleansed price matrix.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen, causally validated rebalance schedule.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "        output_dir (str): The target directory for artifact persistence.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The comprehensive results bundle containing metrics, diagnostics, and raw state.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If any critical failure occurs during the walk-forward loop.\n",
        "    \"\"\"\n",
        "    logger.info(\"=================================================================\")\n",
        "    logger.info(\"COMMENCING END-TO-END HYBRID QUANTUM-CLASSICAL BACKTEST\")\n",
        "    logger.info(\"=================================================================\")\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Initialize state, seeds, and quantum invariants\n",
        "        state, dicke_vector, h_xy = _initialize_backtest_state(config)\n",
        "\n",
        "        # Execute Step 2: Run the monthly walk-forward loop\n",
        "        final_exit_date = _execute_walk_forward_loop(\n",
        "            cleaned_price_df, rebalance_dates, state, dicke_vector, h_xy, config\n",
        "        )\n",
        "\n",
        "        # Execute Step 3: Finalize metrics and persist artifacts\n",
        "        results_bundle = _finalize_and_persist(\n",
        "            state, cleaned_price_df, rebalance_dates, final_exit_date, config, output_dir\n",
        "        )\n",
        "\n",
        "        logger.info(\"=================================================================\")\n",
        "        logger.info(\"BACKTEST COMPLETED SUCCESSFULLY\")\n",
        "        logger.info(\"=================================================================\")\n",
        "\n",
        "        return results_bundle\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"FATAL ERROR: Backtest aborted due to unrecoverable exception: {e}\")\n",
        "        raise RuntimeError(f\"End-to-end backtest failed: {e}\") from e\n"
      ],
      "metadata": {
        "id": "aPhtONVdkWNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27 — Execute final fidelity assertions (run_fidelity_assertions)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Execute final fidelity assertions\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 1: Feasibility assertions: selections are exactly K-hot; allocations satisfy constraints.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _assert_feasibility_invariants(raw_state: Dict[str, Any], config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Mathematically asserts the feasibility invariants for all selections and allocations.\n",
        "\n",
        "    This function iterates through the entire chronological state of the backtest. It\n",
        "    verifies that the discrete solvers strictly obeyed the K-hot cardinality constraint,\n",
        "    and that the continuous allocators strictly obeyed the sum-to-one and box constraints\n",
        "    (within a microscopic floating-point tolerance).\n",
        "\n",
        "    Args:\n",
        "        raw_state (Dict[str, Any]): The raw state tracking dictionary from the orchestrator.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any selection or weight vector violates the mathematical constraints.\n",
        "    \"\"\"\n",
        "    K = config[\"objective_function\"][\"cardinality_K\"]\n",
        "    w_min, w_max = config[\"capital_and_bounds\"][\"allocation_bounds\"]\n",
        "    tol = 1e-8\n",
        "\n",
        "    # 1. Assert QAOA and SA Selections are exactly K-hot\n",
        "    for strategy in [\"QAOA_XY\", \"SA\"]:\n",
        "        selections = raw_state[\"selections\"][strategy]\n",
        "        for m, x_t in enumerate(selections):\n",
        "            if x_t is None:\n",
        "                continue # Handled by fallback logic, but if present, must be valid\n",
        "\n",
        "            actual_k = np.sum(x_t)\n",
        "            if actual_k != K:\n",
        "                raise AssertionError(f\"Feasibility Violation: {strategy} selection at month {m+1} \"\n",
        "                                     f\"has Hamming weight {actual_k}, expected {K}.\")\n",
        "\n",
        "    # 2. Assert Allocation Weights satisfy constraints\n",
        "    for strategy in [\"QAOA_XY\", \"SA\", \"HRP\"]:\n",
        "        weights = raw_state[\"weights\"][strategy]\n",
        "        for m, w_t in enumerate(weights):\n",
        "            # Assert Sum-to-One\n",
        "            weight_sum = np.sum(w_t)\n",
        "            if not np.isclose(weight_sum, 1.0, atol=tol):\n",
        "                raise AssertionError(f\"Feasibility Violation: {strategy} weights at month {m+1} \"\n",
        "                                     f\"sum to {weight_sum:.8f}, expected 1.0.\")\n",
        "\n",
        "            # Assert Non-negativity\n",
        "            if np.any(w_t < -tol):\n",
        "                raise AssertionError(f\"Feasibility Violation: {strategy} weights at month {m+1} \"\n",
        "                                     f\"contain negative values.\")\n",
        "\n",
        "            # Strategy-specific constraints (QAOA and SA only)\n",
        "            if strategy in [\"QAOA_XY\", \"SA\"]:\n",
        "                # Assert exactly K non-zero weights\n",
        "                non_zeros = np.count_nonzero(w_t > tol)\n",
        "                if non_zeros != K:\n",
        "                    raise AssertionError(f\"Feasibility Violation: {strategy} weights at month {m+1} \"\n",
        "                                         f\"have {non_zeros} active assets, expected {K}.\")\n",
        "\n",
        "                # Assert Box Constraints for active assets\n",
        "                active_weights = w_t[w_t > tol]\n",
        "                if np.any(active_weights < w_min - tol) or np.any(active_weights > w_max + tol):\n",
        "                    raise AssertionError(f\"Feasibility Violation: {strategy} weights at month {m+1} \"\n",
        "                                         f\"violate box bounds [{w_min}, {w_max}].\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 2: Causality assertions: estimation windows end strictly before rebalance timestamps.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _assert_causality_invariants(cleaned_price_df: pd.DataFrame, rebalance_dates: Tuple[pd.Timestamp, ...],\n",
        "                                 config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Mathematically asserts the strict causality of the walk-forward methodology.\n",
        "\n",
        "    This function verifies that for every rebalance date, the required historical\n",
        "    lookback window exists entirely and strictly prior to the rebalance timestamp,\n",
        "    guaranteeing zero look-ahead bias in the econometric estimation phase.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical price matrix.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If causality is violated or the schedule is not monotonic.\n",
        "    \"\"\"\n",
        "    L = config[\"global_setup\"][\"lookback_window_L\"]\n",
        "    price_index = cleaned_price_df.index\n",
        "    target_tz = price_index.tz\n",
        "\n",
        "    # Assert monotonicity of the rebalance schedule\n",
        "    for i in range(1, len(rebalance_dates)):\n",
        "        if rebalance_dates[i] <= rebalance_dates[i-1]:\n",
        "            raise AssertionError(\"Causality Violation: Rebalance dates are not strictly monotonically increasing.\")\n",
        "\n",
        "    # Assert strict causal boundary for each window\n",
        "    for m, t in enumerate(rebalance_dates):\n",
        "        t_aligned = t.tz_convert(target_tz) if t.tz is not None and target_tz is not None else (t.tz_localize(target_tz) if target_tz is not None else t.tz_localize(None))\n",
        "\n",
        "        # The mask used in Task 5: tau < t\n",
        "        causal_mask = price_index < t_aligned\n",
        "        causal_history = price_index[causal_mask]\n",
        "\n",
        "        if len(causal_history) < L:\n",
        "            raise AssertionError(f\"Causality Violation: Month {m+1} ({t.date()}) has only {len(causal_history)} \"\n",
        "                                 f\"prior trading days, expected at least {L}.\")\n",
        "\n",
        "        # The last timestamp in the causal history MUST be strictly less than t\n",
        "        window_end = causal_history[-1]\n",
        "        if window_end >= t_aligned:\n",
        "            raise AssertionError(f\"FATAL Causality Violation: Window end {window_end} is >= rebalance date {t_aligned}.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Step 3: Determinism and completeness assertions: exactly 12 records, correct initial capital, complete metrics.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _assert_completeness_invariants(results_bundle: Dict[str, Any], config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Asserts the structural completeness and determinism of the final results bundle.\n",
        "\n",
        "    This function verifies that the backtest produced exactly the expected number of\n",
        "    monthly records, that the value paths originated from the correct initial capital,\n",
        "    and that the final metrics and depth-scaling tables conform exactly to the schemas\n",
        "    required for manuscript reproduction.\n",
        "\n",
        "    Args:\n",
        "        results_bundle (Dict[str, Any]): The comprehensive results dictionary from Task 26.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any structural or completeness invariant is violated.\n",
        "    \"\"\"\n",
        "    raw_state = results_bundle[\"raw_state\"]\n",
        "    metrics = results_bundle[\"financial_metrics\"]\n",
        "    depth_table = results_bundle.get(\"depth_scaling_table\", [])\n",
        "\n",
        "    initial_capital = config[\"capital_and_bounds\"][\"initial_capital\"]\n",
        "    expected_months = 12\n",
        "\n",
        "    # 1. Assert Time-Series Completeness and Initial Capital\n",
        "    for strategy in [\"QAOA_XY\", \"SA\", \"HRP\"]:\n",
        "        net_returns = raw_state[\"net_returns\"][strategy]\n",
        "        value_path = raw_state[\"value_paths\"][strategy]\n",
        "\n",
        "        if len(net_returns) != expected_months:\n",
        "            raise AssertionError(f\"Completeness Violation: {strategy} has {len(net_returns)} return records, expected {expected_months}.\")\n",
        "\n",
        "        if len(value_path) != expected_months + 1:\n",
        "            raise AssertionError(f\"Completeness Violation: {strategy} has {len(value_path)} value path records, expected {expected_months + 1}.\")\n",
        "\n",
        "        if not math.isclose(value_path[0], initial_capital, abs_tol=1e-4):\n",
        "            raise AssertionError(f\"Completeness Violation: {strategy} initial capital is {value_path[0]}, expected {initial_capital}.\")\n",
        "\n",
        "    # 2. Assert Metrics Table Schema\n",
        "    expected_metrics = [\"Total Return\", \"Annualized Volatility\", \"Sharpe Ratio\", \"Max Drawdown\", \"Average Monthly Turnover\"]\n",
        "    for strategy in [\"QAOA_XY\", \"SA\", \"HRP\"]:\n",
        "        if strategy not in metrics:\n",
        "            raise AssertionError(f\"Completeness Violation: Metrics missing for strategy {strategy}.\")\n",
        "        for metric in expected_metrics:\n",
        "            if metric not in metrics[strategy]:\n",
        "                raise AssertionError(f\"Completeness Violation: Metric '{metric}' missing for strategy {strategy}.\")\n",
        "\n",
        "    # 3. Assert Depth-Scaling Table Schema\n",
        "    if len(depth_table) != 6:\n",
        "        raise AssertionError(f\"Completeness Violation: Depth scaling table has {len(depth_table)} rows, expected exactly 6.\")\n",
        "\n",
        "    expected_depth_cols = [\"Depth (p)\", \"Final Cost (Mean)\", \"Iter. to Conv. (Mean)\", \"Gradient Norm (Mean)\", \"Months Failed\"]\n",
        "    for row in depth_table:\n",
        "        for col in expected_depth_cols:\n",
        "            if col not in row:\n",
        "                raise AssertionError(f\"Completeness Violation: Depth table missing column '{col}'.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_fidelity_assertions(results_bundle: Dict[str, Any], cleaned_price_df: pd.DataFrame,\n",
        "                            rebalance_dates: Tuple[pd.Timestamp, ...], config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the final, rigorous fidelity assertions on the completed backtest.\n",
        "\n",
        "    This function acts as the ultimate gatekeeper for institutional research validity.\n",
        "    It mathematically proves that the entire walk-forward execution strictly obeyed\n",
        "    the K-hot feasibility constraints, maintained absolute causality (zero look-ahead bias),\n",
        "    and produced a structurally complete set of artifacts ready for publication.\n",
        "\n",
        "    Args:\n",
        "        results_bundle (Dict[str, Any]): The comprehensive results dictionary from Task 26.\n",
        "        cleaned_price_df (pd.DataFrame): The canonical price matrix.\n",
        "        rebalance_dates (Tuple[pd.Timestamp, ...]): The frozen rebalance schedule.\n",
        "        config (Dict[str, Any]): The validated master study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If any fidelity assertion fails, invalidating the study.\n",
        "    \"\"\"\n",
        "    logger.info(\"Commencing final institutional fidelity assertions.\")\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Assert Feasibility Invariants (K-hot, Bounds, Sum-to-One)\n",
        "        _assert_feasibility_invariants(results_bundle[\"raw_state\"], config)\n",
        "        logger.info(\"Feasibility assertions PASSED.\")\n",
        "\n",
        "        # Execute Step 2: Assert Causality Invariants (Zero Look-Ahead Bias)\n",
        "        _assert_causality_invariants(cleaned_price_df, rebalance_dates, config)\n",
        "        logger.info(\"Causality assertions PASSED.\")\n",
        "\n",
        "        # Execute Step 3: Assert Completeness and Determinism Invariants\n",
        "        _assert_completeness_invariants(results_bundle, config)\n",
        "        logger.info(\"Completeness assertions PASSED.\")\n",
        "\n",
        "        logger.info(\"=================================================================\")\n",
        "        logger.info(\"ALL FIDELITY VERIFICATIONS PASSED. RESULTS ARE PUBLICATION-GRADE.\")\n",
        "        logger.info(\"=================================================================\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        logger.error(f\"FIDELITY VERIFICATION FAILED: {e}\")\n",
        "        raise RuntimeError(f\"The backtest results are invalid due to a fidelity violation: {e}\") from e\n"
      ],
      "metadata": {
        "id": "gxGb6m9mkXIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator Callables\n",
        "\n",
        "# ==============================================================================\n",
        "# Top-Level Orchestrator: Execute Quantum-Hybrid Portfolio Optimization\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Top-Level Step 1: Execute Phase 1 (Data Engineering & Validation - Tasks 1 to 4)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_data_engineering_phase(raw_price_df: pd.DataFrame, raw_config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the rigorous data engineering and validation phase (Tasks 1-4).\n",
        "\n",
        "    This function acts as the first major checkpoint in the pipeline. It strictly\n",
        "    validates the configuration schema, verifies the integrity of the raw price data,\n",
        "    applies the deterministic missing-data policy, and constructs the causal rebalance\n",
        "    calendar. It guarantees that downstream solvers only operate on pristine data.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw, unvalidated time-series price matrix.\n",
        "        raw_config (Dict[str, Any]): The raw, unvalidated master study configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the validated configuration, the data\n",
        "                        validation report, the cleansed price matrix, and the frozen schedule.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any configuration, structural, or numerical invariant is violated.\n",
        "        TypeError: If data types do not match the required schema.\n",
        "    \"\"\"\n",
        "    logger.info(\"--- PHASE 1: DATA ENGINEERING & VALIDATION ---\")\n",
        "\n",
        "    # Task 1: Validate the master configuration dictionary\n",
        "    validated_config = validate_master_config(raw_config)\n",
        "    logger.info(\"Task 1 Complete: Master configuration validated.\")\n",
        "\n",
        "    # Task 2: Validate the raw price DataFrame schema and integrity\n",
        "    data_validation_report = validate_raw_price_df(raw_price_df, validated_config)\n",
        "    logger.info(\"Task 2 Complete: Raw price data integrity verified.\")\n",
        "\n",
        "    # Task 3: Cleanse and normalize the price data\n",
        "    cleaned_price_df = cleanse_price_data(raw_price_df, validated_config)\n",
        "    logger.info(\"Task 3 Complete: Price data cleansed and normalized.\")\n",
        "\n",
        "    # Task 4: Construct the deterministic, causal rebalance calendar\n",
        "    rebalance_dates = build_rebalance_calendar(cleaned_price_df, validated_config)\n",
        "    logger.info(\"Task 4 Complete: Causal rebalance calendar frozen.\")\n",
        "\n",
        "    return {\n",
        "        \"validated_config\": validated_config,\n",
        "        \"data_validation_report\": data_validation_report,\n",
        "        \"cleaned_price_df\": cleaned_price_df,\n",
        "        \"rebalance_dates\": rebalance_dates\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Top-Level Step 2: Execute Phase 2 (Backtest Execution & Fidelity Verification - Tasks 26 & 27)\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def _execute_optimization_and_verification_phase(cleaned_price_df: pd.DataFrame,\n",
        "                                                 rebalance_dates: tuple,\n",
        "                                                 validated_config: Dict[str, Any],\n",
        "                                                 output_dir: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the hybrid quantum-classical backtest and mathematically verifies the results (Tasks 26-27).\n",
        "\n",
        "    This function drives the core computational engine. It invokes the end-to-end walk-forward\n",
        "    loop (Algorithm 1), which handles the QAOA-XY and SA optimizations, continuous weight\n",
        "    allocations, and performance accounting. Immediately upon completion, it subjects the\n",
        "    results to strict institutional fidelity assertions to prove constraint adherence and causality.\n",
        "\n",
        "    Args:\n",
        "        cleaned_price_df (pd.DataFrame): The canonical, cleansed price matrix.\n",
        "        rebalance_dates (tuple): The frozen, causally validated rebalance schedule.\n",
        "        validated_config (Dict[str, Any]): The validated master study configuration.\n",
        "        output_dir (str): The target directory for artifact persistence.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the comprehensive results bundle and fidelity status.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the backtest fails or if any fidelity assertion is violated.\n",
        "    \"\"\"\n",
        "    logger.info(\"--- PHASE 2: HYBRID OPTIMIZATION & FIDELITY VERIFICATION ---\")\n",
        "\n",
        "    # Task 26: Execute the end-to-end walk-forward backtest and persist artifacts\n",
        "    results_bundle = run_full_backtest(\n",
        "        cleaned_price_df=cleaned_price_df,\n",
        "        rebalance_dates=rebalance_dates,\n",
        "        config=validated_config,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "    logger.info(\"Task 26 Complete: End-to-end backtest executed and artifacts persisted.\")\n",
        "\n",
        "    # Task 27: Execute final institutional fidelity assertions\n",
        "    run_fidelity_assertions(\n",
        "        results_bundle=results_bundle,\n",
        "        cleaned_price_df=cleaned_price_df,\n",
        "        rebalance_dates=rebalance_dates,\n",
        "        config=validated_config\n",
        "    )\n",
        "    logger.info(\"Task 27 Complete: All fidelity assertions passed. Results are mathematically sound.\")\n",
        "\n",
        "    return {\n",
        "        \"results_bundle\": results_bundle,\n",
        "        \"fidelity_verified\": True\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Top-Level Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_quantum_hybrid_portfolio_optimization(raw_price_df: pd.DataFrame,\n",
        "                                                  raw_config: Dict[str, Any],\n",
        "                                                  output_dir: str = \"./quantum_portfolio_artifacts\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    The master entry point for the \"Constrained Portfolio Optimization via QAOA-XY\" study.\n",
        "\n",
        "    This elite, implementation-grade orchestrator encapsulates the entire 27-task pipeline.\n",
        "    It sequentially drives the data engineering, quantum circuit simulation, classical convex\n",
        "    optimization, performance accounting, artifact persistence, and mathematical fidelity\n",
        "    verification. It guarantees that the final output is a publication-ready, institutionally\n",
        "    auditable representation of the hybrid quantum-classical methodology.\n",
        "\n",
        "    Args:\n",
        "        raw_price_df (pd.DataFrame): The raw, unvalidated time-series price matrix fetched from the data provider.\n",
        "        raw_config (Dict[str, Any]): The raw, unvalidated master study configuration dictionary.\n",
        "        output_dir (str, optional): The root directory where all serialized artifacts will be saved.\n",
        "                                    Defaults to \"./quantum_portfolio_artifacts\".\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all generated artifacts, including:\n",
        "            - 'validated_config': The strictly typed configuration.\n",
        "            - 'data_validation_report': Telemetry from the data cleansing phase.\n",
        "            - 'cleaned_price_df': The canonical price matrix used for the study.\n",
        "            - 'rebalance_dates': The frozen temporal schedule.\n",
        "            - 'results_bundle': The final metrics, time-series, and quantum diagnostics.\n",
        "            - 'fidelity_verified': Boolean confirming all mathematical constraints were upheld.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If data or configuration invariants are violated during Phase 1.\n",
        "        TypeError: If data types are incorrect during Phase 1.\n",
        "        RuntimeError: If the optimization loop fails or fidelity assertions are violated in Phase 2.\n",
        "    \"\"\"\n",
        "    logger.info(\"===============================================================================\")\n",
        "    logger.info(\"INITIATING QUANTUM-ACCELERATED CONSTRAINED PORTFOLIO OPTIMIZATION PIPELINE\")\n",
        "    logger.info(\"===============================================================================\")\n",
        "\n",
        "    try:\n",
        "        # Execute Top-Level Step 1: Data Engineering & Validation (Tasks 1-4)\n",
        "        phase_1_artifacts = _execute_data_engineering_phase(raw_price_df, raw_config)\n",
        "\n",
        "        # Extract the canonical artifacts required for Phase 2\n",
        "        cleaned_price_df = phase_1_artifacts[\"cleaned_price_df\"]\n",
        "        rebalance_dates = phase_1_artifacts[\"rebalance_dates\"]\n",
        "        validated_config = phase_1_artifacts[\"validated_config\"]\n",
        "\n",
        "        # Execute Top-Level Step 2: Backtest Execution & Fidelity Verification (Tasks 26-27)\n",
        "        phase_2_artifacts = _execute_optimization_and_verification_phase(\n",
        "            cleaned_price_df=cleaned_price_df,\n",
        "            rebalance_dates=rebalance_dates,\n",
        "            validated_config=validated_config,\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        # Compile the final comprehensive output dictionary\n",
        "        final_output = {\n",
        "            **phase_1_artifacts,\n",
        "            **phase_2_artifacts\n",
        "        }\n",
        "\n",
        "        logger.info(\"===============================================================================\")\n",
        "        logger.info(\"PIPELINE EXECUTION COMPLETE. ALL ARTIFACTS GENERATED AND VERIFIED.\")\n",
        "        logger.info(\"===============================================================================\")\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception across the entire pipeline, log it as a critical failure, and re-raise\n",
        "        logger.critical(f\"PIPELINE ABORTED DUE TO FATAL ERROR: {e}\")\n",
        "        raise RuntimeError(f\"Quantum-Hybrid Portfolio Optimization Pipeline failed: {e}\") from e\n"
      ],
      "metadata": {
        "id": "OuETWJbYoaVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}